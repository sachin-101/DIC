{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Machine Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import random  \n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import spacy\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "from spacy.lang.zh import Chinese  # chinese tokenizer\n",
    "import gensim.downloader as api\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\")))\n",
    "from models.encoder import Encoder\n",
    "from models.decoder import Decoder\n",
    "from models.seq2seq import Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:4\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "DEVICE = torch.device('cuda:4') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Fields, which form a pipeline of converting sentence to vectors. We will create a field for each langauge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_field = Field(\n",
    "    tokenize='spacy', \n",
    "    tokenizer_language='en', \n",
    "    lower=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>', \n",
    "    batch_first=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = Chinese()\n",
    "def tokenize_zh(sentence, tokenizer=tokenizer):\n",
    "    return [tok.text for tok in tokenizer(sentence)]\n",
    "\n",
    "#Chinese Field\n",
    "zh_field = Field(\n",
    "    tokenize=tokenize_zh,\n",
    "    tokenizer_language='zh',\n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lxml import etree\n",
    "\n",
    "# tree = etree.iterparse('../../../Dataset/MT/Chinese/globalvoices.zht-en.xliff')\n",
    "# i = 0\n",
    "# for a in tree:\n",
    "#     #print(( element.tag))\n",
    "#     action, element = a\n",
    "#     try:\n",
    "#         print(element.text)\n",
    "#     except:\n",
    "#         pass\n",
    "#     i += 1\n",
    "#     if i == 1000:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 1.124 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../../../Dataset/MT/Chinese/old_dataset/' \n",
    "\n",
    "train_set, val_set = TabularDataset.splits(\n",
    "    path=dataset_dir, \n",
    "    train='zh_en.csv', \n",
    "    validation='zh_en_validate.csv',\n",
    "    format='CSV', \n",
    "    fields=[('Chinese', zh_field), ('English', en_field)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example from Chinese vocabulary:\n",
      " ['zh', '1929', '年', '还是', '1989', '?', '巴黎', '-', '随着', '经济危机', '不断', '加深', '和', '蔓延', '，', '整个', '世界', '一直', '在', '寻找', '历史', '上', '的', '类似', '事件', '希望', '有助于', '我们', '了解', '目前']\n",
      "Examples from English vocabulary:\n",
      " ['en', '1929', 'or', '1989', '?', 'paris', '–', 'as', 'the', 'economic', 'crisis', 'deepens', 'and', 'widens', ',', 'world', 'has', 'been', 'searching', 'for', 'historical', 'analogies', 'to', 'help', 'us', 'understand', 'what', 'happening', '.', 'at']\n"
     ]
    }
   ],
   "source": [
    "zh_field.build_vocab(train_set, val_set, min_freq=5)\n",
    "en_field.build_vocab(train_set, val_set, min_freq=5)\n",
    "\n",
    "print(\"Example from Chinese vocabulary:\\n\", list(zh_field.vocab.freqs.keys())[:30])\n",
    "print(\"Examples from English vocabulary:\\n\", list(en_field.vocab.freqs.keys())[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocabulary size: 24628\n",
      "Chinese vocabulary size: 30896\n"
     ]
    }
   ],
   "source": [
    "# some statistics\n",
    "print('English vocabulary size:',len(en_field.vocab.stoi))\n",
    "print('Chinese vocabulary size:',len(zh_field.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "### English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loading pre-trained glove-embeddings for English\n",
    "# model_gigaword = api.load(\"glove-wiki-gigaword-100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check word embeddings\n",
    "# word = 'look'\n",
    "# print(model_gigaword.wv[word])\n",
    "# model_gigaword.wv.most_similar(positive=[word], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create english word embedding matrix\n",
    "# eng_vocab_sz = len(en_field.vocab.stoi)\n",
    "# eng_embed_dim = 100\n",
    "# eng_embed_matrix = torch.zeros((eng_vocab_sz, eng_embed_dim))\n",
    "\n",
    "# for i, word in enumerate(en_field.vocab.stoi.keys()):\n",
    "#     try:\n",
    "#         eng_embed_matrix[i] = torch.from_numpy(model_gigaword.wv[word])\n",
    "#     except KeyError:\n",
    "#         if word in ['<unk>', '<sos>', '<eos>', '<pad>']:\n",
    "#             eng_embed_matrix[i] = torch.ones((eng_embed_dim,))*i\n",
    "#         else:\n",
    "#             print(\"No embedding vector for\", word)\n",
    "\n",
    "# print(\"English embedding vector created.\")\n",
    "# print(eng_embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinese"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import io\n",
    "\n",
    "# def load_vectors(fname):\n",
    "#     fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "#     n, d = map(int, fin.readline().split())\n",
    "#     data = {}\n",
    "#     for line in fin:\n",
    "#         tokens = line.rstrip().split(' ')\n",
    "#         data[tokens[0]] = map(float, tokens[1:])\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fname = '../../../Dataset/MT/Chinese/embeddings/zhwiki_20180420_100d.txt'\n",
    "# data = load_vectors(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create english word embedding matrix\n",
    "# ch_vocab_sz = len(zh_field.vocab.stoi)\n",
    "# ch_embed_dim = 100\n",
    "# ch_embed_matrix = torch.zeros((ch_vocab_sz, ch_embed_dim))\n",
    "\n",
    "# for i, word in enumerate(zh_field.vocab.stoi.keys()):\n",
    "#     try:\n",
    "#         ch_embed_matrix[i] = torch.tensor(list(data[word]))\n",
    "#     except KeyError:\n",
    "#         if word in ['<unk>', '<sos>', '<eos>', '<pad>']:\n",
    "#             ch_embed_matrix[i] = torch.ones((ch_embed_dim,))*i\n",
    "#         else:\n",
    "#             print(\"No embedding vector for\", word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Chinese embedding vector created.\")\n",
    "# print(ch_embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the embeddings\n",
    "# np.save(os.path.join(dataset_dir, 'eng_embed_matrix'), eng_embed_matrix.cpu().numpy())\n",
    "# np.save(os.path.join(dataset_dir, 'ch_embed_matrix'), ch_embed_matrix.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.Size([24628, 100])\n",
      "torch.Size([30896, 100])\n"
     ]
    }
   ],
   "source": [
    "# load the embeddings\n",
    "eng_embed_matrix = torch.from_numpy(np.load(os.path.join(dataset_dir, 'eng_embed_matrix.npy'))).to(DEVICE)\n",
    "ch_embed_matrix = torch.from_numpy(np.load(os.path.join(dataset_dir, 'ch_embed_matrix.npy'))).to(DEVICE)\n",
    "\n",
    "print(eng_embed_matrix.dtype)\n",
    "print(eng_embed_matrix.shape)\n",
    "print(ch_embed_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embed_layer): Embedding(30896, 100)\n",
       "    (lstm): LSTM(100, 512, num_layers=4, dropout=0.1, bidirectional=True)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embed_layer): Embedding(24628, 100, padding_idx=0)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=1536, out_features=768, bias=True)\n",
       "      (linear2): Linear(in_features=768, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(1124, 512)\n",
       "    (post_lstm_cell): LSTMCell(1536, 512)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=24628, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(24628, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_dim = 512  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "# layer_norm = True   \n",
    "encoder = Encoder(ch_embed_matrix, hidden_dim, num_layers, dropout=dropout, bidirectional=True)\n",
    "\n",
    "hid_sz = 512\n",
    "vocab_size_ch = len(en_field.vocab.stoi)\n",
    "decoder = Decoder(eng_embed_matrix, hid_sz, encoder.output_size, vocab_size_ch)\n",
    "\n",
    "hyperparams = { 'hidden_dim':hidden_dim, 'num_layers':num_layers,\n",
    "               'dropout':dropout, 'hid_sz':hid_sz, \n",
    "               'vocab_size_ch':vocab_size_ch}\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 57])\n",
      "torch.Size([32, 66])\n",
      "Num training example 7587\n",
      "Num validation example 157\n"
     ]
    }
   ],
   "source": [
    "# Sequence bucketing based on size of English sentences\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "    (train_set, val_set), \n",
    "    batch_size=BATCH_SIZE, \n",
    "    sort_key=lambda x: len(x.English), \n",
    "    shuffle=True, \n",
    ")\n",
    "\n",
    "batch = next(iter(train_iterator))\n",
    "print(batch.Chinese.shape)\n",
    "print(batch.English.shape)\n",
    "\n",
    "print(\"Num training example\", len(train_iterator))\n",
    "print(\"Num validation example\", len(val_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()\n",
    "\n",
    "# load = False\n",
    "# if load:\n",
    "#     saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "#     model.load_state_dict(torch.load(saved_afile))\n",
    "#     start_epoch = int(saved_file[-1]) + 1\n",
    "#     time = os.listdir(tensorboard_dir)[-1]  # use the last one \n",
    "\n",
    "NAME = 'Long_training'\n",
    "time = str(datetime.datetime.now())\n",
    "save_dir = os.path.join('trained_models', f'{NAME}_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, \n",
    "          print_interval, writer=None, log_interval=-1, scheduler=None, train_dataset=None):\n",
    "    \n",
    "    model.train()\n",
    "    print(f'Training, Logging: Mean loss of previous {print_interval} batches \\n')\n",
    "    \n",
    "    running_loss = []\n",
    "    date1 = datetime.datetime.now()\n",
    "    \n",
    "\n",
    "    \n",
    "    for batch_idx, batch in enumerate(train_loader):\n",
    "        data, target = batch.Chinese.to(DEVICE), batch.English.to(DEVICE)\n",
    "        loss, _ = model(data, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.detach().item())    # update running loss\n",
    "\n",
    "        # writing to console after print_interval batches\n",
    "        if (batch_idx+1) % print_interval == 0:\n",
    "            date2 = datetime.datetime.now()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMean Loss : {:.6f}\\t lr {}\\t time {}:'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                np.mean(running_loss[-print_interval:]), \n",
    "                optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "                date2 - date1))\n",
    "            date1 = date2\n",
    "\n",
    "        # Writing to tensorboard\n",
    "        if (batch_idx+1) % log_interval == 0:\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_loader) + batch_idx\n",
    "                writer.add_scalar('Loss', np.mean(running_loss[-log_interval:]), global_step)\n",
    "\n",
    "        if batch_idx == len(train_loader)//2:\n",
    "            # save, # check, \n",
    "            torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_half_{epoch}'))\n",
    "            torch.save(optimizer.state_dict(), os.path.join(save_dir, f'optim_half_{epoch}'))\n",
    "            validate_personal(model, train_iterator)\n",
    "            \n",
    "    \n",
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    out = out.squeeze(0)\n",
    "    for t in out:\n",
    "        s = t.max(dim=0)[1].item()\n",
    "        pred_sent.append(en_field.vocab.itos[s])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    y = y.squeeze(0)\n",
    "    for t in y:\n",
    "        sent.append(en_field.vocab.itos[t.item()])\n",
    "    return ''.join(sent)\n",
    "\n",
    "\n",
    "def validate_personal(model, test_loader):\n",
    "    model.eval()\n",
    "    for i in range(2):\n",
    "        batch = next(iter(test_loader))\n",
    "        \n",
    "        ch_sent = batch.Chinese[i].unsqueeze(dim=0).to(DEVICE)\n",
    "        eng_sent = batch.English[i].unsqueeze(dim=0).to(DEVICE)\n",
    "        \n",
    "        loss, output = model(ch_sent, eng_sent)\n",
    "        \n",
    "        print(\"\\n\")\n",
    "        print(\"True sent : \", decode_true_sent(eng_sent))\n",
    "        print(\"Pred sent : \", decode_pred_sent(output))\n",
    "        print(\"Loss :\", loss.item())  \n",
    "    model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models/Long_training_2020-01-01 14:04:32.058587\n",
      "Training, Logging: Mean loss of previous 50 batches \n",
      "\n",
      "Train Epoch: 0 [1568/242778 (1%)]\tMean Loss : 10.111634\t lr 0.001\t time 0:00:14.259819:\n",
      "Train Epoch: 0 [3168/242778 (1%)]\tMean Loss : 10.111629\t lr 0.001\t time 0:00:13.303233:\n",
      "Train Epoch: 0 [4768/242778 (2%)]\tMean Loss : 10.111625\t lr 0.001\t time 0:00:13.168609:\n",
      "Train Epoch: 0 [6368/242778 (3%)]\tMean Loss : 10.111627\t lr 0.001\t time 0:00:13.448727:\n",
      "Train Epoch: 0 [7968/242778 (3%)]\tMean Loss : 10.111616\t lr 0.001\t time 0:00:12.659831:\n",
      "Train Epoch: 0 [9568/242778 (4%)]\tMean Loss : 10.111596\t lr 0.001\t time 0:00:13.203081:\n",
      "Train Epoch: 0 [11168/242778 (5%)]\tMean Loss : 10.111537\t lr 0.001\t time 0:00:13.057947:\n",
      "Train Epoch: 0 [12768/242778 (5%)]\tMean Loss : 10.111420\t lr 0.001\t time 0:00:13.592822:\n",
      "Train Epoch: 0 [14368/242778 (6%)]\tMean Loss : 10.111292\t lr 0.001\t time 0:00:13.242830:\n",
      "Train Epoch: 0 [15968/242778 (7%)]\tMean Loss : 10.111287\t lr 0.001\t time 0:00:13.079717:\n",
      "Train Epoch: 0 [17568/242778 (7%)]\tMean Loss : 10.111028\t lr 0.001\t time 0:00:13.694283:\n",
      "Train Epoch: 0 [19168/242778 (8%)]\tMean Loss : 10.111026\t lr 0.001\t time 0:00:13.235503:\n",
      "Train Epoch: 0 [20768/242778 (9%)]\tMean Loss : 10.110987\t lr 0.001\t time 0:00:13.764251:\n",
      "Train Epoch: 0 [22368/242778 (9%)]\tMean Loss : 10.110731\t lr 0.001\t time 0:00:13.140795:\n",
      "Train Epoch: 0 [23968/242778 (10%)]\tMean Loss : 10.110592\t lr 0.001\t time 0:00:12.758247:\n",
      "Train Epoch: 0 [25568/242778 (11%)]\tMean Loss : 10.110573\t lr 0.001\t time 0:00:13.006158:\n",
      "Train Epoch: 0 [27168/242778 (11%)]\tMean Loss : 10.110732\t lr 0.001\t time 0:00:13.142013:\n",
      "Train Epoch: 0 [28768/242778 (12%)]\tMean Loss : 10.110706\t lr 0.001\t time 0:00:13.386160:\n",
      "Train Epoch: 0 [30368/242778 (13%)]\tMean Loss : 10.110523\t lr 0.001\t time 0:00:13.275199:\n",
      "Train Epoch: 0 [31968/242778 (13%)]\tMean Loss : 10.110436\t lr 0.001\t time 0:00:13.680421:\n",
      "Train Epoch: 0 [33568/242778 (14%)]\tMean Loss : 10.110305\t lr 0.001\t time 0:00:13.301122:\n",
      "Train Epoch: 0 [35168/242778 (14%)]\tMean Loss : 10.110503\t lr 0.001\t time 0:00:13.368124:\n",
      "Train Epoch: 0 [36768/242778 (15%)]\tMean Loss : 10.110259\t lr 0.001\t time 0:00:13.888489:\n",
      "Train Epoch: 0 [38368/242778 (16%)]\tMean Loss : 10.110041\t lr 0.001\t time 0:00:13.047193:\n",
      "Train Epoch: 0 [39968/242778 (16%)]\tMean Loss : 10.110018\t lr 0.001\t time 0:00:12.831834:\n",
      "Train Epoch: 0 [41568/242778 (17%)]\tMean Loss : 10.110106\t lr 0.001\t time 0:00:13.401288:\n",
      "Train Epoch: 0 [43168/242778 (18%)]\tMean Loss : 10.109895\t lr 0.001\t time 0:00:13.702735:\n",
      "Train Epoch: 0 [44768/242778 (18%)]\tMean Loss : 10.109670\t lr 0.001\t time 0:00:13.290536:\n",
      "Train Epoch: 0 [46368/242778 (19%)]\tMean Loss : 10.109538\t lr 0.001\t time 0:00:13.852616:\n",
      "Train Epoch: 0 [47968/242778 (20%)]\tMean Loss : 10.109481\t lr 0.001\t time 0:00:13.449143:\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters(), amsgrad=True)\n",
    "\n",
    "log_interval = 5\n",
    "print_interval = 50\n",
    "\n",
    "epochs = 40\n",
    "load = False\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "\n",
    "\n",
    "# load_dict = 'trained_models/Adadelta_NC_step_1_2019-12-31 04:30:30.395730'\n",
    "# model.load_state_dict(torch.load(os.path.join(load_dict, 'las_model_half_0')))\n",
    "# optimizer.load_state_dict(torch.load(os.path.join(load_dict, 'optim_half_0')))\n",
    "\n",
    "\n",
    "for epoch in range(0,epochs): \n",
    "    train(model, DEVICE, train_iterator, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    \n",
    "    #save model\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_dir, f'optim_{epoch}'))\n",
    "    \n",
    "    validate_personal(model, train_iterator)\n",
    "    \n",
    "    # Decrease tf_ratio\n",
    "    if (epoch+1)%10 == 0:\n",
    "        model.tf_ratio = model.tf_ratio - 0.1\n",
    "        print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    \n",
    "#     if scheduler:\n",
    "#         validate_personal(model, 2, train_dataset)\n",
    "#         for param_group in optimizer.param_groups:\n",
    "#             param_group['lr'] = max(param_group['lr']*0.1, 0.001)\n",
    "#         print(\"-\"*10, \"LR decreased\", '-'*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_personal(model, val_iterator)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
