{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n",
      "                 id                        sent\n",
      "0  BAC009S0002W0122     而 对 楼市 成交 抑制 作用 最 大 的 限\n",
      "1  BAC009S0002W0123             也 成为 地方 政府 的 眼中\n",
      "2  BAC009S0002W0124  自 六月 底 呼和浩特 市 率先 宣布 取消 限 购\n",
      "3  BAC009S0002W0125                  各地 政府 便 纷纷\n",
      "4  BAC009S0002W0126              仅 一 个 多 月 的 时间\n"
     ]
    }
   ],
   "source": [
    "# Used for ai_shell dataset\n",
    "def make_train_df(dataset_dir):\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text_file:\n",
    "                data_list = text_file.readlines()\n",
    "            for example in data_list:\n",
    "                id_, sent = str(example.split(' ')[0]), str(' '.join(example.split(' ')[1:])) # -1 to remove '\\n'\n",
    "                data.append((id_, sent))\n",
    "\n",
    "    train_df = pd.DataFrame(data, columns=['id', 'sent'])\n",
    "    train_df.to_csv(os.path.join(dataset_dir, 'train_df.csv'), header=None)#save\n",
    "\n",
    "\n",
    "dataset_dir = '../../../Dataset/data_aishell/'\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dataset_dir, 'train_df.csv'), names=['id', 'sent'])\n",
    "train_df = train_df.dropna(how='any')\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Analysis and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mandarin data contains relatively small sentences and does not need removal of long sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample rate: 16000\n",
      "x.shape: torch.Size([1, 128, 433])\n",
      "sent len: 26\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchaudio.transforms import MelSpectrogram\n",
    "\n",
    "specgram = MelSpectrogram()\n",
    "audio, sent = train_df.iloc[2]\n",
    "waveform, sample_rate = torchaudio.load(os.path.join(dataset_dir, audio+'.wav'))\n",
    "x = specgram(waveform)\n",
    "\n",
    "print(\"sample rate:\", sample_rate)\n",
    "print(\"x.shape:\", x.shape)\n",
    "print(\"sent len:\", len(sent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars 4256\n"
     ]
    }
   ],
   "source": [
    "def get_chars(save_file, train_df=None):\n",
    "    try:\n",
    "        with open(save_file, 'rb') as f:\n",
    "            chars = pickle.load(f) # load file\n",
    "    except FileNotFoundError:\n",
    "        chars = [' ', '<sos>']\n",
    "        for idx in range(train_df.shape[0]):\n",
    "            _, sent = train_df.iloc[idx]\n",
    "            for c in sent:\n",
    "                if c not in chars:\n",
    "                    chars.append(c)\n",
    "        chars = chars + ['<eos>', '<pad>', '<unk>']\n",
    "        with open(save_file, 'wb') as f:\n",
    "            pickle.dump(chars, f) # save file\n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "save_file = os.path.join(dataset_dir, 'chars')\n",
    "chars = get_chars(save_file, train_df)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']\n",
    "\n",
    "\n",
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, \n",
    "                              eos_token, device=DEVICE, file_extension='.wav')\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=32, \n",
    "                               shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 64  # 256*2 nodes in each LSTM\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 64\n",
    "embed_dim = 30\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, 'num_layers':num_layers,\n",
    "                'dropout':dropout, 'layer_norm':layer_norm, 'hid_sz':hid_sz, 'embed_dim':embed_dim}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Seq2Seq(encoder, decoder, criterion, tf_ratio = 1.0, device=DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [1248/104014 (1%)]\tMean Loss : 27.969870\t time 0:01:18.568658:\n",
      "Train Epoch: 0 [2528/104014 (2%)]\tMean Loss : 27.677372\t time 0:01:14.936303:\n",
      "Train Epoch: 0 [3808/104014 (4%)]\tMean Loss : 27.855455\t time 0:01:16.635070:\n",
      "Train Epoch: 0 [5088/104014 (5%)]\tMean Loss : 27.662129\t time 0:01:14.736184:\n",
      "Train Epoch: 0 [6368/104014 (6%)]\tMean Loss : 28.179919\t time 0:01:16.618391:\n",
      "Train Epoch: 0 [7648/104014 (7%)]\tMean Loss : 27.585653\t time 0:01:16.532999:\n",
      "Train Epoch: 0 [8928/104014 (9%)]\tMean Loss : 28.274216\t time 0:01:16.849946:\n",
      "Train Epoch: 0 [10208/104014 (10%)]\tMean Loss : 27.828933\t time 0:01:15.798461:\n",
      "Train Epoch: 0 [11488/104014 (11%)]\tMean Loss : 27.300104\t time 0:01:14.339550:\n",
      "Train Epoch: 0 [12768/104014 (12%)]\tMean Loss : 27.592005\t time 0:01:14.699517:\n",
      "Train Epoch: 0 [14048/104014 (14%)]\tMean Loss : 27.593386\t time 0:01:15.094730:\n",
      "Train Epoch: 0 [15328/104014 (15%)]\tMean Loss : 27.398483\t time 0:01:14.915766:\n",
      "Train Epoch: 0 [16608/104014 (16%)]\tMean Loss : 28.475156\t time 0:01:17.203130:\n",
      "Train Epoch: 0 [17888/104014 (17%)]\tMean Loss : 27.502279\t time 0:01:14.647354:\n",
      "Train Epoch: 0 [19168/104014 (18%)]\tMean Loss : 27.924102\t time 0:01:16.043018:\n",
      "Train Epoch: 0 [20448/104014 (20%)]\tMean Loss : 27.781466\t time 0:01:15.161572:\n",
      "Train Epoch: 0 [21728/104014 (21%)]\tMean Loss : 28.001386\t time 0:01:16.840592:\n",
      "Train Epoch: 0 [23008/104014 (22%)]\tMean Loss : 27.564183\t time 0:01:15.904800:\n",
      "Train Epoch: 0 [24288/104014 (23%)]\tMean Loss : 27.677655\t time 0:01:15.473518:\n",
      "Train Epoch: 0 [25568/104014 (25%)]\tMean Loss : 28.004224\t time 0:01:17.165206:\n",
      "Train Epoch: 0 [26848/104014 (26%)]\tMean Loss : 27.800191\t time 0:01:16.203682:\n",
      "Train Epoch: 0 [28128/104014 (27%)]\tMean Loss : 27.739308\t time 0:01:16.364432:\n",
      "Train Epoch: 0 [29408/104014 (28%)]\tMean Loss : 27.487585\t time 0:01:16.134267:\n",
      "Train Epoch: 0 [30688/104014 (30%)]\tMean Loss : 27.965328\t time 0:01:17.758240:\n",
      "Train Epoch: 0 [31968/104014 (31%)]\tMean Loss : 27.602090\t time 0:01:16.472598:\n",
      "Train Epoch: 0 [33248/104014 (32%)]\tMean Loss : 27.781099\t time 0:01:17.401122:\n",
      "Train Epoch: 0 [34528/104014 (33%)]\tMean Loss : 27.624255\t time 0:01:15.689703:\n",
      "Train Epoch: 0 [35808/104014 (34%)]\tMean Loss : 28.299574\t time 0:01:18.799818:\n",
      "Train Epoch: 0 [37088/104014 (36%)]\tMean Loss : 27.478684\t time 0:01:16.113928:\n",
      "Train Epoch: 0 [38368/104014 (37%)]\tMean Loss : 28.192928\t time 0:01:17.227141:\n",
      "Train Epoch: 0 [39648/104014 (38%)]\tMean Loss : 27.873476\t time 0:01:16.717342:\n",
      "Train Epoch: 0 [40928/104014 (39%)]\tMean Loss : 27.834585\t time 0:01:16.575882:\n",
      "Train Epoch: 0 [42208/104014 (41%)]\tMean Loss : 27.936484\t time 0:01:18.343128:\n",
      "Train Epoch: 0 [43488/104014 (42%)]\tMean Loss : 27.971248\t time 0:01:17.066146:\n",
      "Train Epoch: 0 [44768/104014 (43%)]\tMean Loss : 27.609139\t time 0:01:16.371720:\n",
      "Train Epoch: 0 [46048/104014 (44%)]\tMean Loss : 27.532891\t time 0:01:16.267942:\n",
      "Train Epoch: 0 [47328/104014 (46%)]\tMean Loss : 27.732894\t time 0:01:17.028778:\n",
      "Train Epoch: 0 [48608/104014 (47%)]\tMean Loss : 27.512579\t time 0:01:16.412937:\n",
      "Train Epoch: 0 [49888/104014 (48%)]\tMean Loss : 27.713680\t time 0:01:17.124899:\n",
      "Train Epoch: 0 [51168/104014 (49%)]\tMean Loss : 27.316286\t time 0:01:16.022053:\n",
      "Train Epoch: 0 [52448/104014 (50%)]\tMean Loss : 27.756408\t time 0:01:17.108654:\n",
      "Train Epoch: 0 [53728/104014 (52%)]\tMean Loss : 27.679260\t time 0:01:16.726059:\n",
      "Train Epoch: 0 [55008/104014 (53%)]\tMean Loss : 27.447417\t time 0:01:16.506890:\n",
      "Train Epoch: 0 [56288/104014 (54%)]\tMean Loss : 27.210941\t time 0:01:14.646962:\n",
      "Train Epoch: 0 [57568/104014 (55%)]\tMean Loss : 27.989262\t time 0:01:17.109031:\n",
      "Train Epoch: 0 [58848/104014 (57%)]\tMean Loss : 27.939662\t time 0:01:16.976241:\n",
      "Train Epoch: 0 [60128/104014 (58%)]\tMean Loss : 28.038089\t time 0:01:17.091207:\n",
      "Train Epoch: 0 [61408/104014 (59%)]\tMean Loss : 27.605113\t time 0:01:16.492188:\n",
      "Train Epoch: 0 [62688/104014 (60%)]\tMean Loss : 27.153803\t time 0:01:15.302817:\n",
      "Train Epoch: 0 [63968/104014 (62%)]\tMean Loss : 27.586827\t time 0:01:32.685061:\n",
      "Train Epoch: 0 [65248/104014 (63%)]\tMean Loss : 27.090779\t time 0:01:32.501409:\n",
      "Train Epoch: 0 [66528/104014 (64%)]\tMean Loss : 27.738260\t time 0:01:26.845492:\n",
      "Train Epoch: 0 [67808/104014 (65%)]\tMean Loss : 27.338060\t time 0:01:17.212512:\n",
      "Train Epoch: 0 [69088/104014 (66%)]\tMean Loss : 27.360969\t time 0:01:15.496717:\n",
      "Train Epoch: 0 [70368/104014 (68%)]\tMean Loss : 27.595906\t time 0:01:15.982243:\n",
      "Train Epoch: 0 [71648/104014 (69%)]\tMean Loss : 27.781377\t time 0:01:16.387743:\n",
      "Train Epoch: 0 [72928/104014 (70%)]\tMean Loss : 27.861124\t time 0:01:16.776228:\n",
      "Train Epoch: 0 [74208/104014 (71%)]\tMean Loss : 27.778614\t time 0:01:16.795835:\n",
      "Train Epoch: 0 [75488/104014 (73%)]\tMean Loss : 27.907875\t time 0:01:17.050784:\n",
      "Train Epoch: 0 [76768/104014 (74%)]\tMean Loss : 27.379940\t time 0:01:16.558814:\n",
      "Train Epoch: 0 [78048/104014 (75%)]\tMean Loss : 27.719247\t time 0:01:16.389854:\n",
      "Train Epoch: 0 [79328/104014 (76%)]\tMean Loss : 28.010677\t time 0:01:16.541553:\n",
      "Train Epoch: 0 [80608/104014 (78%)]\tMean Loss : 27.502972\t time 0:01:15.537726:\n",
      "Train Epoch: 0 [81888/104014 (79%)]\tMean Loss : 27.948384\t time 0:01:16.705029:\n",
      "Train Epoch: 0 [83168/104014 (80%)]\tMean Loss : 27.826155\t time 0:01:17.146720:\n",
      "Train Epoch: 0 [84448/104014 (81%)]\tMean Loss : 27.564527\t time 0:01:16.130904:\n",
      "Train Epoch: 0 [85728/104014 (82%)]\tMean Loss : 27.399518\t time 0:01:16.407189:\n",
      "Train Epoch: 0 [87008/104014 (84%)]\tMean Loss : 27.699366\t time 0:01:17.188359:\n",
      "Train Epoch: 0 [88288/104014 (85%)]\tMean Loss : 27.785950\t time 0:01:16.772001:\n",
      "Train Epoch: 0 [89568/104014 (86%)]\tMean Loss : 27.740428\t time 0:01:16.523323:\n",
      "Train Epoch: 0 [90848/104014 (87%)]\tMean Loss : 28.249261\t time 0:01:17.464952:\n",
      "Train Epoch: 0 [92128/104014 (89%)]\tMean Loss : 27.284439\t time 0:01:15.235144:\n",
      "Train Epoch: 0 [93408/104014 (90%)]\tMean Loss : 27.515145\t time 0:01:15.936058:\n",
      "Train Epoch: 0 [94688/104014 (91%)]\tMean Loss : 27.710384\t time 0:01:15.944314:\n",
      "Train Epoch: 0 [95968/104014 (92%)]\tMean Loss : 27.287739\t time 0:01:15.846850:\n",
      "Train Epoch: 0 [97248/104014 (94%)]\tMean Loss : 27.500679\t time 0:01:16.663227:\n",
      "Train Epoch: 0 [98528/104014 (95%)]\tMean Loss : 27.776673\t time 0:01:16.026583:\n",
      "Train Epoch: 0 [99808/104014 (96%)]\tMean Loss : 28.154229\t time 0:01:18.449298:\n",
      "Train Epoch: 0 [101088/104014 (97%)]\tMean Loss : 27.861744\t time 0:01:17.676716:\n",
      "Train Epoch: 0 [102368/104014 (98%)]\tMean Loss : 27.819515\t time 0:01:16.318593:\n",
      "Train Epoch: 0 [103648/104014 (100%)]\tMean Loss : 27.772098\t time 0:01:15.923179:\n",
      "\n",
      "Teacher forcing ratio: 0.95\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 1 [1248/104014 (1%)]\tMean Loss : 27.767685\t time 0:01:16.347438:\n",
      "Train Epoch: 1 [2528/104014 (2%)]\tMean Loss : 28.023618\t time 0:01:17.392432:\n",
      "Train Epoch: 1 [5088/104014 (5%)]\tMean Loss : 27.285828\t time 0:01:15.487744:\n",
      "Train Epoch: 1 [6368/104014 (6%)]\tMean Loss : 27.901771\t time 0:01:16.359161:\n",
      "Train Epoch: 1 [7648/104014 (7%)]\tMean Loss : 27.726626\t time 0:01:16.563547:\n",
      "Train Epoch: 1 [8928/104014 (9%)]\tMean Loss : 27.745884\t time 0:01:16.394635:\n",
      "Train Epoch: 1 [10208/104014 (10%)]\tMean Loss : 27.758536\t time 0:01:16.672326:\n",
      "Train Epoch: 1 [11488/104014 (11%)]\tMean Loss : 27.544562\t time 0:01:16.360508:\n",
      "Train Epoch: 1 [12768/104014 (12%)]\tMean Loss : 27.537840\t time 0:01:16.517147:\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.ASGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "\n",
    "load = False\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "save_dir = os.path.join('trained_models_mandarin', f'Training_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.txt'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "    \n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    scheduler.step()                                    # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.05, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sent :  <sos>who was now approaching womanhood he would sometimes talk with her differently from the manner in which he would speak to a mere girl but on her part she seemed not to notice the difference and for their daily amusement either go<eos>\n",
      "\n",
      "Pred sent :   uholshs sornsnpropsh ng shmpndsun sorshuld shmp hnpl shll shlh hor shgfordds f soom shv sord r sn sholl sorshmld shrrssh nsnd r dsorlpsurosf sor srrshsho shnnud sor shvsor sorshvososfordd krsnd sor shv r sonnh snosh ond sngh<eos>r soo\n",
      "Loss : 722.2744140625\n",
      "\n",
      "\n",
      "True sent :  <sos>now sworn to the service of his most christian majesty<eos>\n",
      "\n",
      "Pred sent :   uor shordssh shj shnoongdshosos sors soooshgnn soruss  \n",
      "Loss : 178.7652587890625\n",
      "\n",
      "\n",
      "True sent :  <sos>and a paper cap on his head has the strong conscience and the strong sense the blended susceptibility and self command of our friend adam he was not an average man yet such men as he are reared here and there in every generation of our peasant artisans<eos>\n",
      "\n",
      "Pred sent :   und shsroprssorpsf sos sorrdshmpshv shoong sonsoordsossnd shoushoong shndonshv suoss soshrposp nlffnu snd shndosonsond sf sfr soognd snondsor orhsor sndsnor lonson souhshrhoupd shosousno soslsd sor  snd sho r sn sndso sonfrslunn sf sf shrrrsnd snohnhnd \n",
      "Loss : 809.318603515625\n",
      "\n",
      "\n",
      "True sent :  <sos>uncommon patience in planning a revenge that is worth while m is for moses who slew the egyptian as sweet as a rose is the meekness of moses no monument shows his post mortem inscription<eos>\n",
      "\n",
      "Pred sent :   undopponssrrhor hssn sroogor ssnsouor lrshvn sn shoooou hnfhsosn sor sors  sholshossshvosnpordnn sn shordhsndsnsous sn shv sonoross shosors  sorsorsspn  shoulosos srrsosornhrpsn ooosprnn \n",
      "Loss : 608.6458129882812\n",
      "\n",
      "\n",
      "True sent :  <sos>i'm not a coward as a general rule went on the promoter but i always said that if i ever met the sucker that bought that lot i'd run like a turkey now you see that old babe in the wood over there well he's the boy that drew the prize<eos>\n",
      "\n",
      "Pred sent :   ursosor snsonsrd sn sndorddsllsosh shooosf sho sroppnhd sur hnshoor  sholsshvn sn snsnduosonhshv shrhrdsshodlulush  shvn sonhsns sosgsogfosnshrn d sornsour  lfshvn shooshmlrsn shv shud sho rshv ousholosors shv suu shvn soossshv sroppd\n",
      "Loss : 762.9634399414062\n",
      "\n",
      "\n",
      "True sent :  <sos>if we discover that they have the least desire to get the better of us<eos>\n",
      "\n",
      "Pred sent :   uroprrsossooorsshvn shv  sodf shv sosrs sossrsdsh son shv surhor sf sn \n",
      "Loss : 230.7517547607422\n",
      "\n",
      "\n",
      "True sent :  <sos>the promise long with the fulfilment short will make thee triumph in thy lofty seat francis came afterward when i was dead for me but one of the black cherubim said to him take him not do me no wrong<eos>\n",
      "\n",
      "Pred sent :   uhp sroppnhdsong shlh hoolsollorlord shous shll sons shv orh<eos><sos>rplosn shv song  shndrsoonghnhsorp snfor ord shon shshs sosrssor sonsur hf  sf shv suonk shousslnpshmd sh sos shlfosos sor shmsorsorsholdh\n",
      "Loss : 639.7150268554688\n",
      "\n",
      "\n",
      "True sent :  <sos>it was plain to see that mister sandford esteemed her less and less every day and as he was the person who most influenced the opinion of her guardian he became to her very soon an object not merely of dislike but of abhorrence<eos>\n",
      "\n",
      "Pred sent :   ud shs sronn sh sho shvn sors r shmd urd snshrsod sor sonsosnd soss snors son snd sndsorshs shv srrsonssholsors sn oosddk  shvouklngnn sf sor sorrd nn sorsurolpdsh sor sors shmn sndsflurkusor sonosf sf shghonf sur hf snlousoddks\n",
      "Loss : 720.8135986328125\n",
      "\n",
      "\n",
      "True sent :  <sos>there came a letter addressed to doctor livesey with this addition to be opened in the case of his absence by tom redruth or young hawkins<eos>\n",
      "\n",
      "Pred sent :   uhppprsorp snsoss r snoooss soshrsonkor sogo  l shgo hhvs snoonunn sh sursfpn d sn shv sors sf sos snlosdh su sh ssosuosh hf sourd sodn ng \n",
      "Loss : 451.2442626953125\n",
      "\n",
      "\n",
      "True sent :  <sos>what is he that every cheek turns pale at the mention of his name asked capitola black donald oh my child may you never know more of black donald than i can tell you black donald is the chief of a band of ruthless desperadoes that infest these mountain roads<eos>\n",
      "\n",
      "Pred sent :   uhor sn soushjn snors soorr shrn  srrsdsn shv sonoonn sf shghsomplsn od sorroorlgsuookosongll sfrsorsoosloshm soursosor snoo sorn sf shork songoo shvn snsonrshllosoursuonk songll sn shv sooososf shsurd sf sosh f shsosslrslons shvn sn ors shv orsornd sn sour  \n",
      "Loss : 821.7781372070312\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "model.device = DEVICE\n",
    "model.tf_ratio = 0.9\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, eos_token, file_extension='.flac')\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
