{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Model\n",
    "\n",
    "Language models form an important part of any ASR or MT systems. Language models assist the model to output sentences which make more sense. They are trained with huge corporas of monolingual text, and are coupled with decoder to output sentences. \n",
    "\n",
    "There are two types of Language models integration techniques, namely Soft fusion and hard fusion.\n",
    "We are going to integrate soft fusion into our decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn       # neural Networks module of pytorch for extending\n",
    "import torch.optim as optim     # Optimizers\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy    # for English tokenization\n",
    "import dill      # for saving field of the datasets\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating train and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fields take in text and output tensors\n",
    "# can add preprocessing pipelines\n",
    "DEVICE = torch.device('cuda:0') if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "en_field = Field(\n",
    "    tokenize='spacy', \n",
    "    tokenizer_language='en', \n",
    "    lower=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    "    include_lengths=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_csv(max):\n",
    "    data = []\n",
    "    with open('librispeech-lm-norm.txt', 'rb') as text_file:\n",
    "        data = text_file.readlines()\n",
    "\n",
    "    data_df = pd.DataFrame(data[:max])\n",
    "    data_df.to_csv('lm_corpus.csv', header=None)\n",
    "    print(data_df.head())\n",
    "\n",
    "  \n",
    "NUM_SENTENCES = 100000\n",
    "\n",
    "if not os.path.exists('lm_corpus.csv'):\n",
    "    print('Saving as csv.. may take few minutes')\n",
    "    save_as_csv(max=NUM_SENTENCES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 22226\n",
      "torch.Size([32, 58])\n",
      "Num training examples 95008\n"
     ]
    }
   ],
   "source": [
    "dataset = TabularDataset(\n",
    "    path='lm_corpus.csv',\n",
    "    format='CSV',\n",
    "    fields=[('id', None), ('sent', en_field)]\n",
    ")\n",
    "\n",
    "val_data, train_data = dataset.split(split_ratio=0.05)\n",
    "\n",
    "# build vocabularies\n",
    "en_field.build_vocab(dataset, min_freq=5)\n",
    "print('vocab size:', len(en_field.vocab.stoi))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_iterator, val_iterator = BucketIterator.splits(\n",
    "        (train_data, val_data),\n",
    "        batch_size = BATCH_SIZE,\n",
    "        sort_key = lambda x: len(x.sent),\n",
    "        shuffle=True,\n",
    "        device=DEVICE\n",
    ")\n",
    "\n",
    "data = next(iter(train_iterator))\n",
    "print(data.sent[0].shape)\n",
    "print('Num training examples', len(train_iterator)*train_iterator.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the datasets for later use\n",
    "with open('en_field.Field', 'wb') as f:\n",
    "    dill.dump(en_field, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNNLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    ''' RNN Language Model '''\n",
    "\n",
    "    def __init__(self, vocab_size, emb_dim, dim, n_layers, pad_token, dropout):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.n_layers = n_layers\n",
    "        self.pad_token  = pad_token \n",
    "        \n",
    "        self.dp1 = nn.Dropout(dropout)\n",
    "        self.dp2 = nn.Dropout(dropout)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
    "        self.linear = nn.Linear(dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, lengths, hidden=None):\n",
    "        \n",
    "        if self.training:\n",
    "            emb_x = self.dp1(self.embed(x))\n",
    "            packed = pack_padded_sequence(emb_x, lengths, batch_first=True, enforce_sorted=False)\n",
    "            out, hidden = self.rnn(packed, hidden)\n",
    "            padded, _ = pad_packed_sequence(out, batch_first=True, padding_value=self.pad_token)\n",
    "            out = self.linear(self.dp2(padded))\n",
    "            return out, hidden\n",
    "        else:\n",
    "            self.rnn.flatten_parameters()\n",
    "            \n",
    "            # producing a single example\n",
    "            h = torch.zeros((1*self.n_layers, 1,  self.dim)).to(DEVICE)\n",
    "            c = torch.zeros((1*self.n_layers, 1, self.dim)).to(DEVICE)\n",
    "            hidden = (h, c)\n",
    "            outputs = []\n",
    "            \n",
    "            while len(outputs)<10:\n",
    "                emb_x = self.dp1(self.embed(x))\n",
    "                out, hidden = self.rnn(emb_x, hidden)\n",
    "                out = self.linear(self.dp2(out))\n",
    "                y_t = out.argmax(dim=2)\n",
    "                outputs.append(y_t.item())\n",
    "                x = y_t    # input for next time step\n",
    "            return outputs\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Langauage Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_iterator, optimizer, vocab_size, epoch, pad_token,\n",
    "          print_interval, writer=None, log_interval=-1):\n",
    "    \n",
    "    running_loss = []\n",
    "    date1 = datetime.datetime.now()\n",
    "    for iter_n, batch in enumerate(train_iterator):\n",
    "                \n",
    "        X, lengths = batch.sent    # sentence\n",
    "        y_out, _ = model(X, lengths)\n",
    "        \n",
    "        # A simple hack\n",
    "        lengths = lengths - 1  # Since <eos> should not be fed to the model\n",
    "        Y = F.pad(X[:,1:], (0,1), mode='constant', value=pad_token) \n",
    "     \n",
    "        optimizer.zero_grad()\n",
    "        loss = F.cross_entropy(y_out.view(-1, vocab_size), Y.reshape(-1))\n",
    "        loss.backward()        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss.append(loss.detach().item())    # update running loss\n",
    "        \n",
    "        # writing to console after print_interval batches\n",
    "        if (iter_n+1) % print_interval == 0:\n",
    "            date2 = datetime.datetime.now()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMean Loss : {:.6f}\\t time {}:'.format(\n",
    "                epoch, iter_n * len(data), len(train_iterator)*train_iterator.batch_size,\n",
    "                100. * iter_n / len(train_iterator), \n",
    "                np.mean(running_loss[-print_interval:]), \n",
    "                date2 - date1))\n",
    "            date1 = date2\n",
    "\n",
    "        # Logging in tensorboard\n",
    "        if (iter_n) % log_interval == 0:\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_iterator) + iter_n\n",
    "                writer.add_scalar('Loss', np.mean(running_loss[-log_interval:]), global_step)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6368/95008 (7%)]\tMean Loss : 2.508074\t time 0:00:17.375048:\n",
      "Train Epoch: 0 [12768/95008 (13%)]\tMean Loss : 2.304891\t time 0:00:17.508558:\n",
      "Train Epoch: 0 [19168/95008 (20%)]\tMean Loss : 2.269401\t time 0:00:17.352124:\n",
      "Train Epoch: 0 [25568/95008 (27%)]\tMean Loss : 2.304137\t time 0:00:17.407171:\n",
      "Train Epoch: 0 [31968/95008 (34%)]\tMean Loss : 2.266123\t time 0:00:18.169832:\n",
      "Train Epoch: 0 [38368/95008 (40%)]\tMean Loss : 2.181649\t time 0:00:19.631043:\n",
      "Train Epoch: 0 [44768/95008 (47%)]\tMean Loss : 2.007011\t time 0:00:20.097710:\n",
      "Train Epoch: 0 [51168/95008 (54%)]\tMean Loss : 1.979948\t time 0:00:19.619043:\n",
      "Train Epoch: 0 [57568/95008 (61%)]\tMean Loss : 1.937282\t time 0:00:21.110461:\n",
      "Train Epoch: 0 [63968/95008 (67%)]\tMean Loss : 1.894662\t time 0:00:20.509657:\n",
      "Train Epoch: 0 [70368/95008 (74%)]\tMean Loss : 1.933059\t time 0:00:19.306097:\n",
      "Train Epoch: 0 [76768/95008 (81%)]\tMean Loss : 1.900327\t time 0:00:20.185312:\n",
      "Train Epoch: 0 [83168/95008 (88%)]\tMean Loss : 1.922454\t time 0:00:19.697773:\n",
      "Train Epoch: 0 [89568/95008 (94%)]\tMean Loss : 1.855179\t time 0:00:19.661397:\n",
      "----------\n",
      "Train Epoch: 1 [6368/95008 (7%)]\tMean Loss : 1.831671\t time 0:00:20.812003:\n",
      "Train Epoch: 1 [12768/95008 (13%)]\tMean Loss : 1.855399\t time 0:00:19.807269:\n",
      "Train Epoch: 1 [19168/95008 (20%)]\tMean Loss : 1.866630\t time 0:00:19.859819:\n",
      "Train Epoch: 1 [25568/95008 (27%)]\tMean Loss : 1.800714\t time 0:00:20.073922:\n",
      "Train Epoch: 1 [31968/95008 (34%)]\tMean Loss : 1.815798\t time 0:00:20.229828:\n",
      "Train Epoch: 1 [38368/95008 (40%)]\tMean Loss : 1.846627\t time 0:00:20.126310:\n",
      "Train Epoch: 1 [44768/95008 (47%)]\tMean Loss : 1.837128\t time 0:00:19.281592:\n",
      "Train Epoch: 1 [51168/95008 (54%)]\tMean Loss : 1.793577\t time 0:00:20.532388:\n",
      "Train Epoch: 1 [57568/95008 (61%)]\tMean Loss : 1.807484\t time 0:00:20.447222:\n",
      "Train Epoch: 1 [63968/95008 (67%)]\tMean Loss : 1.746971\t time 0:00:21.273819:\n",
      "Train Epoch: 1 [70368/95008 (74%)]\tMean Loss : 1.753981\t time 0:00:20.542599:\n",
      "Train Epoch: 1 [76768/95008 (81%)]\tMean Loss : 1.799487\t time 0:00:20.097219:\n",
      "Train Epoch: 1 [83168/95008 (88%)]\tMean Loss : 1.752516\t time 0:00:19.975805:\n",
      "Train Epoch: 1 [89568/95008 (94%)]\tMean Loss : 1.775094\t time 0:00:20.061606:\n",
      "----------\n",
      "Train Epoch: 2 [6368/95008 (7%)]\tMean Loss : 1.769910\t time 0:00:19.948410:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-4b3a88b43a68>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-88-5425647d26b4>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_iterator, optimizer, vocab_size, epoch, pad_token, print_interval, writer, log_interval)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msent\u001b[0m    \u001b[0;31m# sentence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# Since <eos> should not be fed to the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-72-2f62d1bd7b95>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, lengths, hidden)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_padded_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menforce_sorted\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpacked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mpadded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_packed_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdp2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/utils/rnn.py\u001b[0m in \u001b[0;36mpad_packed_sequence\u001b[0;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[1;32m    331\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0munsorted_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mbatch_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch_first\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_select\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munsorted_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpadded_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vocab_size = len(en_field.vocab.stoi)\n",
    "emb_dim = 100\n",
    "dim = 1024  # lstm cell dimension\n",
    "num_layers = 3\n",
    "\n",
    "pad_token = en_field.vocab.stoi['<pad>']\n",
    "\n",
    "model = RNNLM(vocab_size, emb_dim, dim, num_layers, pad_token, dropout=0.1)\n",
    "\n",
    "model.to(DEVICE)   # move model to GPU\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "print_interval = 200\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(model, train_iterator, optimizer, vocab_size, epoch, pad_token, print_interval)\n",
    "    print('-'*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Novel sentences\n",
    "\n",
    "a. Starting with sos token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7, 68, 0, 6, 3, 1, 0, 6, 3, 1]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_in = torch.tensor(en_field.vocab.stoi['<sos>']).view((1,1)).to(DEVICE)\n",
    "model.eval()\n",
    "out = model(x_in, lengths=-1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'a certain <unk> ' <eos> <pad> <unk> ' <eos> <pad> "
     ]
    }
   ],
   "source": [
    "for i in out:\n",
    "    print(en_field.vocab.itos[i], end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b. Starting with a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' <eos> <pad> <unk> ' <eos> <pad> <unk> ' <eos> "
     ]
    }
   ],
   "source": [
    "sent_in = en_field.tokenize('What')\n",
    "tokens = [en_field.vocab.stoi[s] for s in sent_in]\n",
    "out = model(torch.tensor(tokens).view(1, -1).to(DEVICE), lengths=-1)\n",
    "for i in out:\n",
    "    print(en_field.vocab.itos[i], end=' ')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
