{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower().replace('\\n', '')\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(root_dir, dataset, file_ext, csv_file):\n",
    "    dataset_dir = os.path.join(root_dir, dataset)\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text_file:\n",
    "                data_list = text_file.readlines()\n",
    "            for example in data_list:\n",
    "                path = os.path.join(dataset, str(example.split(' ')[0])) + file_ext   \n",
    "                sent = preprocess(str(' '.join(example.split(' ')[1:])))\n",
    "                data.append((path, sent))\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['path', 'sent'])\n",
    "    data_df.to_csv(os.path.join(root_dir, csv_file), header=None)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "root_dir = '../../../Dataset/LibriSpeech'\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 281241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0001.flac</td>\n",
       "      <td>that had its source away back in the woods of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0002.flac</td>\n",
       "      <td>for not even a brook could run past missus rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_100/103-1240-0003.flac</td>\n",
       "      <td>and that if she noticed anything odd or out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_100/103-1240-0004.flac</td>\n",
       "      <td>but missus rachel lynde was one of those capab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0001.flac   \n",
       "2  dataset_100/103-1240-0002.flac   \n",
       "3  dataset_100/103-1240-0003.flac   \n",
       "4  dataset_100/103-1240-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  that had its source away back in the woods of ...  \n",
       "2  for not even a brook could run past missus rac...  \n",
       "3  and that if she noticed anything odd or out of...  \n",
       "4  but missus rachel lynde was one of those capab...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100 = pd.read_csv(os.path.join(root_dir, 'train_100.csv'), header=None, names=['path', 'sent'])\n",
    "train_360 = pd.read_csv(os.path.join(root_dir, 'train_360.csv'), header=None, names=['path', 'sent'])\n",
    "train_500 = pd.read_csv(os.path.join(root_dir, 'train_500.csv'), header=None, names=['path', 'sent'])\n",
    "\n",
    "# combine all of them\n",
    "train_df = pd.concat([train_100, train_360, train_500])\n",
    "print(\"Number of training examples:\", train_df.shape[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing very large sentences\n",
    "# def remove_long_sent(train_df, max_len):\n",
    "#     data = []\n",
    "#     for idx in range(train_df.shape[0]):\n",
    "#         path, sent = train_df.iloc[idx]\n",
    "#         if len(sent) > max_len:\n",
    "#             continue\n",
    "#         data.append((path, sent))\n",
    "#     return pd.DataFrame(data, columns=['path', 'sent'])\n",
    "\n",
    "# max_len = 225\n",
    "# train_df = remove_long_sent(train_df, max_len)\n",
    "# print(\"Number of training examples:\",  train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 219709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0006.flac</td>\n",
       "      <td>as avonlea housekeepers were wont to tell in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0009.flac</td>\n",
       "      <td>missus rachel knew that he ought because she h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0006.flac   \n",
       "2  dataset_100/103-1240-0009.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  as avonlea housekeepers were wont to tell in a...  \n",
       "2  missus rachel knew that he ought because she h...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save train_df\n",
    "#train_df.to_csv(os.path.join(root_dir, 'total_train.csv'), header=None)\n",
    "# load train_df\n",
    "train_df = pd.read_csv(os.path.join(root_dir, 'total_train.csv'), header=None, names=['path', 'sent'])\n",
    "print(\"Number of training examples:\",  train_df.shape[0])\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars 32\n"
     ]
    }
   ],
   "source": [
    "def get_chars(include_digits=True):\n",
    "    if include_digits:\n",
    "        chars = ['<sos>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \\\n",
    "                 'x', 'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    else:\n",
    "        chars = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \\\n",
    "                 \n",
    "                'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "chars = get_chars(include_digits=False)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=64, \n",
    "                               shuffle=True, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "name = f'amsgrad_ln_640_{time}'\n",
    "save_dir = os.path.join('trained_models_librispeech', name)\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (embedding): Embedding(32, 50)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=3200, out_features=1600, bias=True)\n",
       "      (linear2): Linear(in_features=1600, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(2610, 640)\n",
       "    (post_lstm_cell): LSTMCell(3200, 640)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=640, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 640  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "layer_norm = True   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 640\n",
    "embed_dim = 50\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size}\n",
    "\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models_librispeech/amsgrad_ln_640_2019-12-28 14:41:29.019788\n",
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [2496/219709 (1%)]\tMean Loss : 3.416665\t time 0:01:24.765318:\n",
      "Train Epoch: 0 [5056/219709 (2%)]\tMean Loss : 3.385057\t time 0:01:26.849841:\n",
      "Train Epoch: 0 [7616/219709 (3%)]\tMean Loss : 3.364435\t time 0:01:30.013839:\n",
      "Train Epoch: 0 [10176/219709 (5%)]\tMean Loss : 3.356212\t time 0:01:26.869090:\n",
      "Train Epoch: 0 [12736/219709 (6%)]\tMean Loss : 3.329969\t time 0:01:27.393574:\n",
      "Train Epoch: 0 [15296/219709 (7%)]\tMean Loss : 3.315616\t time 0:01:28.597292:\n",
      "Train Epoch: 0 [17856/219709 (8%)]\tMean Loss : 3.301885\t time 0:01:28.817658:\n",
      "Train Epoch: 0 [20416/219709 (9%)]\tMean Loss : 3.287353\t time 0:01:27.081143:\n",
      "Train Epoch: 0 [22976/219709 (10%)]\tMean Loss : 3.274144\t time 0:01:31.028958:\n",
      "Train Epoch: 0 [25536/219709 (12%)]\tMean Loss : 3.263107\t time 0:01:29.013424:\n",
      "Train Epoch: 0 [28096/219709 (13%)]\tMean Loss : 3.247313\t time 0:01:27.546809:\n",
      "Train Epoch: 0 [30656/219709 (14%)]\tMean Loss : 3.236685\t time 0:01:28.607049:\n",
      "Train Epoch: 0 [33216/219709 (15%)]\tMean Loss : 3.225769\t time 0:01:29.089773:\n",
      "Train Epoch: 0 [35776/219709 (16%)]\tMean Loss : 3.213664\t time 0:01:28.876818:\n",
      "Train Epoch: 0 [38336/219709 (17%)]\tMean Loss : 3.199965\t time 0:01:27.030434:\n",
      "Train Epoch: 0 [40896/219709 (19%)]\tMean Loss : 3.190592\t time 0:01:26.787758:\n",
      "Train Epoch: 0 [43456/219709 (20%)]\tMean Loss : 3.177385\t time 0:01:27.412668:\n",
      "Train Epoch: 0 [46016/219709 (21%)]\tMean Loss : 3.162212\t time 0:01:28.168510:\n",
      "Train Epoch: 0 [48576/219709 (22%)]\tMean Loss : 3.154757\t time 0:01:30.325262:\n",
      "Train Epoch: 0 [51136/219709 (23%)]\tMean Loss : 3.146666\t time 0:01:29.647521:\n",
      "Train Epoch: 0 [53696/219709 (24%)]\tMean Loss : 3.133517\t time 0:01:28.639169:\n",
      "Train Epoch: 0 [56256/219709 (26%)]\tMean Loss : 3.127458\t time 0:01:29.351008:\n",
      "Train Epoch: 0 [58816/219709 (27%)]\tMean Loss : 3.117419\t time 0:01:30.236316:\n",
      "Train Epoch: 0 [61376/219709 (28%)]\tMean Loss : 3.113598\t time 0:01:29.981914:\n",
      "Train Epoch: 0 [63936/219709 (29%)]\tMean Loss : 3.106664\t time 0:01:29.008205:\n",
      "Train Epoch: 0 [66496/219709 (30%)]\tMean Loss : 3.101495\t time 0:01:28.612540:\n",
      "Train Epoch: 0 [69056/219709 (31%)]\tMean Loss : 3.089877\t time 0:01:28.379565:\n",
      "Train Epoch: 0 [71616/219709 (33%)]\tMean Loss : 3.087072\t time 0:01:30.072824:\n",
      "Train Epoch: 0 [74176/219709 (34%)]\tMean Loss : 3.079684\t time 0:01:27.302360:\n",
      "Train Epoch: 0 [76736/219709 (35%)]\tMean Loss : 3.079844\t time 0:01:28.975795:\n",
      "Train Epoch: 0 [79296/219709 (36%)]\tMean Loss : 3.068845\t time 0:01:29.855019:\n",
      "Train Epoch: 0 [81856/219709 (37%)]\tMean Loss : 3.065361\t time 0:01:28.840135:\n",
      "Train Epoch: 0 [84416/219709 (38%)]\tMean Loss : 3.060413\t time 0:01:27.916884:\n",
      "Train Epoch: 0 [86976/219709 (40%)]\tMean Loss : 3.057779\t time 0:01:29.398314:\n",
      "Train Epoch: 0 [89536/219709 (41%)]\tMean Loss : 3.053782\t time 0:01:31.025201:\n",
      "Train Epoch: 0 [92096/219709 (42%)]\tMean Loss : 3.049929\t time 0:01:29.714588:\n",
      "Train Epoch: 0 [94656/219709 (43%)]\tMean Loss : 3.049163\t time 0:01:31.041032:\n",
      "Train Epoch: 0 [97216/219709 (44%)]\tMean Loss : 3.047074\t time 0:01:29.982735:\n",
      "Train Epoch: 0 [99776/219709 (45%)]\tMean Loss : 3.036901\t time 0:01:32.298417:\n",
      "Train Epoch: 0 [102336/219709 (47%)]\tMean Loss : 3.037365\t time 0:01:28.897398:\n",
      "Train Epoch: 0 [104896/219709 (48%)]\tMean Loss : 3.035890\t time 0:01:26.840870:\n",
      "Train Epoch: 0 [107456/219709 (49%)]\tMean Loss : 3.033022\t time 0:01:28.659176:\n",
      "Train Epoch: 0 [110016/219709 (50%)]\tMean Loss : 3.028841\t time 0:01:32.207650:\n",
      "Train Epoch: 0 [112576/219709 (51%)]\tMean Loss : 3.026084\t time 0:01:29.153161:\n",
      "Train Epoch: 0 [115136/219709 (52%)]\tMean Loss : 3.019616\t time 0:01:28.055933:\n",
      "Train Epoch: 0 [117696/219709 (54%)]\tMean Loss : 3.023910\t time 0:01:28.841921:\n",
      "Train Epoch: 0 [120256/219709 (55%)]\tMean Loss : 3.018341\t time 0:01:30.062144:\n",
      "Train Epoch: 0 [122816/219709 (56%)]\tMean Loss : 3.017647\t time 0:01:29.273151:\n",
      "Train Epoch: 0 [125376/219709 (57%)]\tMean Loss : 3.014017\t time 0:01:28.758486:\n",
      "Train Epoch: 0 [127936/219709 (58%)]\tMean Loss : 3.013616\t time 0:01:28.572053:\n",
      "Train Epoch: 0 [130496/219709 (59%)]\tMean Loss : 3.003277\t time 0:01:30.206457:\n",
      "Train Epoch: 0 [133056/219709 (61%)]\tMean Loss : 3.014845\t time 0:01:29.309844:\n",
      "Train Epoch: 0 [135616/219709 (62%)]\tMean Loss : 3.010179\t time 0:01:30.145475:\n",
      "Train Epoch: 0 [138176/219709 (63%)]\tMean Loss : 3.008720\t time 0:01:30.877261:\n",
      "Train Epoch: 0 [140736/219709 (64%)]\tMean Loss : 3.004524\t time 0:01:27.718845:\n",
      "Train Epoch: 0 [143296/219709 (65%)]\tMean Loss : 3.010714\t time 0:01:29.919636:\n",
      "Train Epoch: 0 [145856/219709 (66%)]\tMean Loss : 3.007178\t time 0:01:29.457408:\n",
      "Train Epoch: 0 [148416/219709 (68%)]\tMean Loss : 3.003487\t time 0:01:27.771755:\n",
      "Train Epoch: 0 [150976/219709 (69%)]\tMean Loss : 3.000512\t time 0:01:27.553798:\n",
      "Train Epoch: 0 [153536/219709 (70%)]\tMean Loss : 3.003327\t time 0:01:28.143556:\n",
      "Train Epoch: 0 [156096/219709 (71%)]\tMean Loss : 3.000982\t time 0:01:30.568013:\n",
      "Train Epoch: 0 [158656/219709 (72%)]\tMean Loss : 2.996059\t time 0:01:28.011172:\n",
      "Train Epoch: 0 [161216/219709 (73%)]\tMean Loss : 2.997705\t time 0:01:29.481024:\n",
      "Train Epoch: 0 [163776/219709 (75%)]\tMean Loss : 2.999750\t time 0:01:29.118261:\n",
      "Train Epoch: 0 [166336/219709 (76%)]\tMean Loss : 2.995276\t time 0:01:28.886396:\n",
      "Train Epoch: 0 [168896/219709 (77%)]\tMean Loss : 2.994750\t time 0:01:27.762962:\n",
      "Train Epoch: 0 [171456/219709 (78%)]\tMean Loss : 2.989434\t time 0:01:29.486492:\n",
      "Train Epoch: 0 [174016/219709 (79%)]\tMean Loss : 2.986372\t time 0:01:27.166458:\n",
      "Train Epoch: 0 [176576/219709 (80%)]\tMean Loss : 2.987055\t time 0:01:29.060779:\n",
      "Train Epoch: 0 [179136/219709 (82%)]\tMean Loss : 2.990700\t time 0:01:28.614663:\n",
      "Train Epoch: 0 [181696/219709 (83%)]\tMean Loss : 2.985979\t time 0:01:27.820008:\n",
      "Train Epoch: 0 [184256/219709 (84%)]\tMean Loss : 2.990629\t time 0:01:29.015324:\n",
      "Train Epoch: 0 [186816/219709 (85%)]\tMean Loss : 2.985277\t time 0:01:29.462053:\n",
      "Train Epoch: 0 [189376/219709 (86%)]\tMean Loss : 2.982576\t time 0:01:27.535538:\n",
      "Train Epoch: 0 [191936/219709 (87%)]\tMean Loss : 2.983420\t time 0:01:31.439439:\n",
      "Train Epoch: 0 [194496/219709 (89%)]\tMean Loss : 2.984560\t time 0:01:29.672094:\n",
      "Train Epoch: 0 [197056/219709 (90%)]\tMean Loss : 2.980313\t time 0:01:30.505607:\n",
      "Train Epoch: 0 [199616/219709 (91%)]\tMean Loss : 2.982871\t time 0:01:31.273815:\n",
      "Train Epoch: 0 [202176/219709 (92%)]\tMean Loss : 2.978470\t time 0:01:30.070184:\n",
      "Train Epoch: 0 [204736/219709 (93%)]\tMean Loss : 2.979060\t time 0:01:29.379408:\n",
      "Train Epoch: 0 [207296/219709 (94%)]\tMean Loss : 2.982802\t time 0:01:29.799570:\n",
      "Train Epoch: 0 [209856/219709 (96%)]\tMean Loss : 2.981841\t time 0:01:29.578209:\n",
      "Train Epoch: 0 [212416/219709 (97%)]\tMean Loss : 2.980532\t time 0:01:28.670811:\n",
      "Train Epoch: 0 [214976/219709 (98%)]\tMean Loss : 2.982501\t time 0:01:28.390788:\n",
      "Train Epoch: 0 [217536/219709 (99%)]\tMean Loss : 2.974547\t time 0:01:27.957141:\n",
      "\n",
      "Teacher forcing ratio: 0.975\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 1 [2496/219709 (1%)]\tMean Loss : 2.977483\t time 0:01:29.225526:\n",
      "Train Epoch: 1 [5056/219709 (2%)]\tMean Loss : 2.982768\t time 0:01:29.128694:\n",
      "Train Epoch: 1 [7616/219709 (3%)]\tMean Loss : 2.979308\t time 0:01:28.914007:\n",
      "Train Epoch: 1 [10176/219709 (5%)]\tMean Loss : 2.986346\t time 0:01:29.393194:\n",
      "Train Epoch: 1 [12736/219709 (6%)]\tMean Loss : 2.980767\t time 0:01:28.402808:\n",
      "Train Epoch: 1 [15296/219709 (7%)]\tMean Loss : 2.980757\t time 0:01:27.554568:\n",
      "Train Epoch: 1 [17856/219709 (8%)]\tMean Loss : 2.982996\t time 0:01:27.968559:\n",
      "Train Epoch: 1 [20416/219709 (9%)]\tMean Loss : 2.985115\t time 0:01:29.397670:\n",
      "Train Epoch: 1 [22976/219709 (10%)]\tMean Loss : 2.976620\t time 0:01:30.935548:\n",
      "Train Epoch: 1 [25536/219709 (12%)]\tMean Loss : 2.977395\t time 0:01:28.488469:\n",
      "Train Epoch: 1 [28096/219709 (13%)]\tMean Loss : 2.975863\t time 0:01:28.328344:\n",
      "Train Epoch: 1 [30656/219709 (14%)]\tMean Loss : 2.974213\t time 0:01:29.144902:\n",
      "Train Epoch: 1 [33216/219709 (15%)]\tMean Loss : 2.975272\t time 0:01:28.346697:\n",
      "Train Epoch: 1 [35776/219709 (16%)]\tMean Loss : 2.976915\t time 0:01:28.659250:\n",
      "Train Epoch: 1 [38336/219709 (17%)]\tMean Loss : 2.979804\t time 0:01:30.559226:\n",
      "Train Epoch: 1 [40896/219709 (19%)]\tMean Loss : 2.980863\t time 0:01:28.069436:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [43456/219709 (20%)]\tMean Loss : 2.982091\t time 0:01:27.567987:\n",
      "Train Epoch: 1 [46016/219709 (21%)]\tMean Loss : 2.973303\t time 0:01:29.971138:\n",
      "Train Epoch: 1 [48576/219709 (22%)]\tMean Loss : 2.975635\t time 0:01:29.334412:\n",
      "Train Epoch: 1 [51136/219709 (23%)]\tMean Loss : 2.977903\t time 0:01:28.760793:\n",
      "Train Epoch: 1 [53696/219709 (24%)]\tMean Loss : 2.974466\t time 0:01:28.902228:\n",
      "Train Epoch: 1 [56256/219709 (26%)]\tMean Loss : 2.974586\t time 0:01:29.347609:\n",
      "Train Epoch: 1 [58816/219709 (27%)]\tMean Loss : 2.979770\t time 0:01:29.355026:\n",
      "Train Epoch: 1 [61376/219709 (28%)]\tMean Loss : 2.973696\t time 0:01:29.021265:\n",
      "Train Epoch: 1 [63936/219709 (29%)]\tMean Loss : 2.971554\t time 0:01:29.351546:\n",
      "Train Epoch: 1 [66496/219709 (30%)]\tMean Loss : 2.979290\t time 0:01:29.835348:\n",
      "Train Epoch: 1 [69056/219709 (31%)]\tMean Loss : 2.968335\t time 0:01:28.667136:\n",
      "Train Epoch: 1 [71616/219709 (33%)]\tMean Loss : 2.974703\t time 0:01:28.939343:\n",
      "Train Epoch: 1 [74176/219709 (34%)]\tMean Loss : 2.973435\t time 0:01:29.635895:\n",
      "Train Epoch: 1 [76736/219709 (35%)]\tMean Loss : 2.971265\t time 0:01:29.529631:\n",
      "Train Epoch: 1 [79296/219709 (36%)]\tMean Loss : 2.968049\t time 0:01:28.643870:\n",
      "Train Epoch: 1 [81856/219709 (37%)]\tMean Loss : 2.973505\t time 0:01:29.184977:\n",
      "Train Epoch: 1 [84416/219709 (38%)]\tMean Loss : 2.975837\t time 0:01:27.749032:\n",
      "Train Epoch: 1 [86976/219709 (40%)]\tMean Loss : 2.969013\t time 0:01:27.054545:\n",
      "Train Epoch: 1 [89536/219709 (41%)]\tMean Loss : 2.971226\t time 0:01:28.546370:\n",
      "Train Epoch: 1 [92096/219709 (42%)]\tMean Loss : 2.974226\t time 0:01:29.138002:\n",
      "Train Epoch: 1 [94656/219709 (43%)]\tMean Loss : 2.975742\t time 0:01:27.299079:\n",
      "Train Epoch: 1 [97216/219709 (44%)]\tMean Loss : 2.971753\t time 0:01:27.504072:\n",
      "Train Epoch: 1 [99776/219709 (45%)]\tMean Loss : 2.972691\t time 0:01:30.983469:\n",
      "Train Epoch: 1 [102336/219709 (47%)]\tMean Loss : 2.965375\t time 0:01:28.238424:\n",
      "Train Epoch: 1 [104896/219709 (48%)]\tMean Loss : 2.967948\t time 0:01:28.998129:\n",
      "Train Epoch: 1 [107456/219709 (49%)]\tMean Loss : 2.965035\t time 0:01:29.096830:\n",
      "Train Epoch: 1 [110016/219709 (50%)]\tMean Loss : 2.970407\t time 0:01:27.466603:\n",
      "Train Epoch: 1 [112576/219709 (51%)]\tMean Loss : 2.968305\t time 0:01:28.827029:\n",
      "Train Epoch: 1 [115136/219709 (52%)]\tMean Loss : 2.965050\t time 0:01:28.344444:\n",
      "Train Epoch: 1 [117696/219709 (54%)]\tMean Loss : 2.969991\t time 0:01:30.758903:\n",
      "Train Epoch: 1 [120256/219709 (55%)]\tMean Loss : 2.966008\t time 0:01:26.445403:\n",
      "Train Epoch: 1 [122816/219709 (56%)]\tMean Loss : 2.967469\t time 0:01:29.266904:\n",
      "Train Epoch: 1 [125376/219709 (57%)]\tMean Loss : 2.965244\t time 0:01:28.747748:\n",
      "Train Epoch: 1 [127936/219709 (58%)]\tMean Loss : 2.966297\t time 0:01:26.989856:\n",
      "Train Epoch: 1 [130496/219709 (59%)]\tMean Loss : 2.966740\t time 0:01:29.056016:\n",
      "Train Epoch: 1 [133056/219709 (61%)]\tMean Loss : 2.969925\t time 0:01:29.693776:\n",
      "Train Epoch: 1 [135616/219709 (62%)]\tMean Loss : 2.963498\t time 0:01:29.399259:\n",
      "Train Epoch: 1 [138176/219709 (63%)]\tMean Loss : 2.964766\t time 0:01:29.305087:\n",
      "Train Epoch: 1 [140736/219709 (64%)]\tMean Loss : 2.968184\t time 0:01:30.230377:\n",
      "Train Epoch: 1 [143296/219709 (65%)]\tMean Loss : 2.962765\t time 0:01:28.524260:\n",
      "Train Epoch: 1 [145856/219709 (66%)]\tMean Loss : 2.966084\t time 0:01:28.738364:\n",
      "Train Epoch: 1 [148416/219709 (68%)]\tMean Loss : 2.965207\t time 0:01:27.610641:\n",
      "Train Epoch: 1 [150976/219709 (69%)]\tMean Loss : 2.966011\t time 0:01:29.969625:\n",
      "Train Epoch: 1 [153536/219709 (70%)]\tMean Loss : 2.969880\t time 0:01:28.614308:\n",
      "Train Epoch: 1 [156096/219709 (71%)]\tMean Loss : 2.965412\t time 0:01:29.592552:\n",
      "Train Epoch: 1 [158656/219709 (72%)]\tMean Loss : 2.968090\t time 0:01:29.602067:\n",
      "Train Epoch: 1 [161216/219709 (73%)]\tMean Loss : 2.962081\t time 0:01:28.718082:\n",
      "Train Epoch: 1 [163776/219709 (75%)]\tMean Loss : 2.964037\t time 0:01:28.641040:\n",
      "Train Epoch: 1 [166336/219709 (76%)]\tMean Loss : 2.956490\t time 0:01:29.552496:\n",
      "Train Epoch: 1 [168896/219709 (77%)]\tMean Loss : 2.961673\t time 0:01:29.227807:\n",
      "Train Epoch: 1 [171456/219709 (78%)]\tMean Loss : 2.970645\t time 0:01:29.430159:\n",
      "Train Epoch: 1 [174016/219709 (79%)]\tMean Loss : 2.965536\t time 0:01:28.797876:\n",
      "Train Epoch: 1 [176576/219709 (80%)]\tMean Loss : 2.967633\t time 0:01:29.034422:\n",
      "Train Epoch: 1 [179136/219709 (82%)]\tMean Loss : 2.959399\t time 0:01:31.799601:\n",
      "Train Epoch: 1 [181696/219709 (83%)]\tMean Loss : 2.958872\t time 0:01:28.493627:\n",
      "Train Epoch: 1 [184256/219709 (84%)]\tMean Loss : 2.959394\t time 0:01:28.753178:\n",
      "Train Epoch: 1 [186816/219709 (85%)]\tMean Loss : 2.958791\t time 0:01:29.926137:\n",
      "Train Epoch: 1 [189376/219709 (86%)]\tMean Loss : 2.964601\t time 0:01:28.150179:\n",
      "Train Epoch: 1 [191936/219709 (87%)]\tMean Loss : 2.962220\t time 0:01:29.565500:\n",
      "Train Epoch: 1 [194496/219709 (89%)]\tMean Loss : 2.961654\t time 0:01:30.394900:\n",
      "Train Epoch: 1 [197056/219709 (90%)]\tMean Loss : 2.961334\t time 0:01:27.356192:\n",
      "Train Epoch: 1 [199616/219709 (91%)]\tMean Loss : 2.962714\t time 0:01:29.102600:\n",
      "Train Epoch: 1 [202176/219709 (92%)]\tMean Loss : 2.958530\t time 0:01:29.217804:\n",
      "Train Epoch: 1 [204736/219709 (93%)]\tMean Loss : 2.965372\t time 0:01:28.412121:\n",
      "Train Epoch: 1 [207296/219709 (94%)]\tMean Loss : 2.960582\t time 0:01:29.555110:\n",
      "Train Epoch: 1 [209856/219709 (96%)]\tMean Loss : 2.960785\t time 0:01:27.914148:\n",
      "Train Epoch: 1 [212416/219709 (97%)]\tMean Loss : 2.956889\t time 0:01:29.812163:\n",
      "Train Epoch: 1 [214976/219709 (98%)]\tMean Loss : 2.957807\t time 0:01:28.983007:\n",
      "Train Epoch: 1 [217536/219709 (99%)]\tMean Loss : 2.956637\t time 0:01:28.134813:\n",
      "\n",
      "Teacher forcing ratio: 0.95\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 2 [2496/219709 (1%)]\tMean Loss : 2.965278\t time 0:01:29.744056:\n",
      "Train Epoch: 2 [5056/219709 (2%)]\tMean Loss : 2.967136\t time 0:01:29.052725:\n",
      "Train Epoch: 2 [7616/219709 (3%)]\tMean Loss : 2.963242\t time 0:01:29.788690:\n",
      "Train Epoch: 2 [10176/219709 (5%)]\tMean Loss : 2.965800\t time 0:01:29.507551:\n",
      "Train Epoch: 2 [12736/219709 (6%)]\tMean Loss : 2.966475\t time 0:01:27.479827:\n",
      "Train Epoch: 2 [15296/219709 (7%)]\tMean Loss : 2.965865\t time 0:01:27.686981:\n",
      "Train Epoch: 2 [17856/219709 (8%)]\tMean Loss : 2.964528\t time 0:01:29.902166:\n",
      "Train Epoch: 2 [20416/219709 (9%)]\tMean Loss : 2.964819\t time 0:01:28.283562:\n",
      "Train Epoch: 2 [22976/219709 (10%)]\tMean Loss : 2.966027\t time 0:01:29.970883:\n",
      "Train Epoch: 2 [25536/219709 (12%)]\tMean Loss : 2.968355\t time 0:01:27.342015:\n",
      "Train Epoch: 2 [28096/219709 (13%)]\tMean Loss : 2.967165\t time 0:01:30.607251:\n",
      "Train Epoch: 2 [30656/219709 (14%)]\tMean Loss : 2.964927\t time 0:01:28.763996:\n",
      "Train Epoch: 2 [33216/219709 (15%)]\tMean Loss : 2.965095\t time 0:01:28.838898:\n",
      "Train Epoch: 2 [35776/219709 (16%)]\tMean Loss : 2.960780\t time 0:01:28.587661:\n",
      "Train Epoch: 2 [38336/219709 (17%)]\tMean Loss : 2.962465\t time 0:01:27.872051:\n",
      "Train Epoch: 2 [40896/219709 (19%)]\tMean Loss : 2.960500\t time 0:01:27.361716:\n",
      "Train Epoch: 2 [43456/219709 (20%)]\tMean Loss : 2.959277\t time 0:01:27.592751:\n",
      "Train Epoch: 2 [46016/219709 (21%)]\tMean Loss : 2.962284\t time 0:01:29.673316:\n",
      "Train Epoch: 2 [48576/219709 (22%)]\tMean Loss : 2.959264\t time 0:01:27.905332:\n",
      "Train Epoch: 2 [51136/219709 (23%)]\tMean Loss : 2.960146\t time 0:01:30.350369:\n",
      "Train Epoch: 2 [53696/219709 (24%)]\tMean Loss : 2.957092\t time 0:01:29.353468:\n",
      "Train Epoch: 2 [56256/219709 (26%)]\tMean Loss : 2.961780\t time 0:01:28.914718:\n",
      "Train Epoch: 2 [58816/219709 (27%)]\tMean Loss : 2.964823\t time 0:01:29.360044:\n",
      "Train Epoch: 2 [61376/219709 (28%)]\tMean Loss : 2.968691\t time 0:01:29.263650:\n",
      "Train Epoch: 2 [63936/219709 (29%)]\tMean Loss : 2.965920\t time 0:01:27.839452:\n",
      "Train Epoch: 2 [66496/219709 (30%)]\tMean Loss : 2.966248\t time 0:01:29.384667:\n",
      "Train Epoch: 2 [69056/219709 (31%)]\tMean Loss : 2.968433\t time 0:01:30.121378:\n",
      "Train Epoch: 2 [71616/219709 (33%)]\tMean Loss : 2.964401\t time 0:01:27.770134:\n",
      "Train Epoch: 2 [74176/219709 (34%)]\tMean Loss : 2.959841\t time 0:01:28.864710:\n",
      "Train Epoch: 2 [76736/219709 (35%)]\tMean Loss : 2.960959\t time 0:01:29.888000:\n",
      "Train Epoch: 2 [79296/219709 (36%)]\tMean Loss : 2.957661\t time 0:01:29.189655:\n",
      "Train Epoch: 2 [81856/219709 (37%)]\tMean Loss : 2.959636\t time 0:01:31.134177:\n",
      "Train Epoch: 2 [84416/219709 (38%)]\tMean Loss : 2.961777\t time 0:01:28.285757:\n",
      "Train Epoch: 2 [86976/219709 (40%)]\tMean Loss : 2.956334\t time 0:01:28.619465:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [89536/219709 (41%)]\tMean Loss : 2.961492\t time 0:01:26.468724:\n",
      "Train Epoch: 2 [92096/219709 (42%)]\tMean Loss : 2.962712\t time 0:01:30.588428:\n",
      "Train Epoch: 2 [94656/219709 (43%)]\tMean Loss : 2.962444\t time 0:01:28.991024:\n",
      "Train Epoch: 2 [97216/219709 (44%)]\tMean Loss : 2.961264\t time 0:01:28.560877:\n",
      "Train Epoch: 2 [99776/219709 (45%)]\tMean Loss : 2.963455\t time 0:01:29.113570:\n",
      "Train Epoch: 2 [102336/219709 (47%)]\tMean Loss : 2.958875\t time 0:01:29.015964:\n",
      "Train Epoch: 2 [104896/219709 (48%)]\tMean Loss : 2.953585\t time 0:01:29.305850:\n",
      "Train Epoch: 2 [107456/219709 (49%)]\tMean Loss : 2.959219\t time 0:01:29.930353:\n",
      "Train Epoch: 2 [110016/219709 (50%)]\tMean Loss : 2.961991\t time 0:01:27.828625:\n",
      "Train Epoch: 2 [112576/219709 (51%)]\tMean Loss : 2.956921\t time 0:01:28.327233:\n",
      "Train Epoch: 2 [115136/219709 (52%)]\tMean Loss : 2.963129\t time 0:01:29.253111:\n",
      "Train Epoch: 2 [117696/219709 (54%)]\tMean Loss : 2.956582\t time 0:01:29.912117:\n",
      "Train Epoch: 2 [120256/219709 (55%)]\tMean Loss : 2.958860\t time 0:01:26.723389:\n",
      "Train Epoch: 2 [122816/219709 (56%)]\tMean Loss : 2.962321\t time 0:01:28.439311:\n",
      "Train Epoch: 2 [125376/219709 (57%)]\tMean Loss : 2.960982\t time 0:01:29.863664:\n",
      "Train Epoch: 2 [127936/219709 (58%)]\tMean Loss : 2.958233\t time 0:01:29.806070:\n",
      "Train Epoch: 2 [130496/219709 (59%)]\tMean Loss : 2.961627\t time 0:01:28.121421:\n",
      "Train Epoch: 2 [133056/219709 (61%)]\tMean Loss : 2.963047\t time 0:01:29.160175:\n",
      "Train Epoch: 2 [135616/219709 (62%)]\tMean Loss : 2.958336\t time 0:01:29.156010:\n",
      "Train Epoch: 2 [138176/219709 (63%)]\tMean Loss : 2.956731\t time 0:01:29.442977:\n",
      "Train Epoch: 2 [140736/219709 (64%)]\tMean Loss : 2.959568\t time 0:01:28.087937:\n",
      "Train Epoch: 2 [143296/219709 (65%)]\tMean Loss : 2.956212\t time 0:01:28.390827:\n",
      "Train Epoch: 2 [145856/219709 (66%)]\tMean Loss : 2.958062\t time 0:01:30.015879:\n",
      "Train Epoch: 2 [148416/219709 (68%)]\tMean Loss : 2.958768\t time 0:01:27.749284:\n",
      "Train Epoch: 2 [150976/219709 (69%)]\tMean Loss : 2.956913\t time 0:01:29.642213:\n",
      "Train Epoch: 2 [153536/219709 (70%)]\tMean Loss : 2.958007\t time 0:01:28.558780:\n",
      "Train Epoch: 2 [156096/219709 (71%)]\tMean Loss : 2.958622\t time 0:01:28.860936:\n",
      "Train Epoch: 2 [158656/219709 (72%)]\tMean Loss : 2.956978\t time 0:01:29.723143:\n",
      "Train Epoch: 2 [161216/219709 (73%)]\tMean Loss : 2.956155\t time 0:01:28.438398:\n",
      "Train Epoch: 2 [163776/219709 (75%)]\tMean Loss : 2.954671\t time 0:01:29.573974:\n",
      "Train Epoch: 2 [166336/219709 (76%)]\tMean Loss : 2.956112\t time 0:01:30.664452:\n",
      "Train Epoch: 2 [168896/219709 (77%)]\tMean Loss : 2.955765\t time 0:01:28.924924:\n",
      "Train Epoch: 2 [171456/219709 (78%)]\tMean Loss : 2.955416\t time 0:01:29.754116:\n",
      "Train Epoch: 2 [174016/219709 (79%)]\tMean Loss : 2.956561\t time 0:01:30.091117:\n",
      "Train Epoch: 2 [176576/219709 (80%)]\tMean Loss : 2.954996\t time 0:01:29.470153:\n",
      "Train Epoch: 2 [179136/219709 (82%)]\tMean Loss : 2.958038\t time 0:01:29.377128:\n",
      "Train Epoch: 2 [181696/219709 (83%)]\tMean Loss : 2.957014\t time 0:01:28.706812:\n",
      "Train Epoch: 2 [184256/219709 (84%)]\tMean Loss : 2.951414\t time 0:01:30.139137:\n",
      "Train Epoch: 2 [186816/219709 (85%)]\tMean Loss : 2.955979\t time 0:01:29.548340:\n",
      "Train Epoch: 2 [189376/219709 (86%)]\tMean Loss : 2.953160\t time 0:01:30.031771:\n",
      "Train Epoch: 2 [191936/219709 (87%)]\tMean Loss : 2.954401\t time 0:01:28.248802:\n",
      "Train Epoch: 2 [194496/219709 (89%)]\tMean Loss : 2.955266\t time 0:01:29.273016:\n",
      "Train Epoch: 2 [197056/219709 (90%)]\tMean Loss : 2.955014\t time 0:01:28.434817:\n",
      "Train Epoch: 2 [199616/219709 (91%)]\tMean Loss : 2.950500\t time 0:01:28.105620:\n",
      "Train Epoch: 2 [202176/219709 (92%)]\tMean Loss : 2.951524\t time 0:01:30.170701:\n",
      "Train Epoch: 2 [204736/219709 (93%)]\tMean Loss : 2.954088\t time 0:01:28.089548:\n",
      "Train Epoch: 2 [207296/219709 (94%)]\tMean Loss : 2.957030\t time 0:01:28.449376:\n",
      "Train Epoch: 2 [209856/219709 (96%)]\tMean Loss : 2.953632\t time 0:01:30.137186:\n",
      "Train Epoch: 2 [212416/219709 (97%)]\tMean Loss : 2.959100\t time 0:01:28.850211:\n",
      "Train Epoch: 2 [214976/219709 (98%)]\tMean Loss : 2.952767\t time 0:01:28.240628:\n",
      "Train Epoch: 2 [217536/219709 (99%)]\tMean Loss : 2.952676\t time 0:01:28.131573:\n",
      "\n",
      "Teacher forcing ratio: 0.9249999999999999\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 3 [2496/219709 (1%)]\tMean Loss : 2.960450\t time 0:01:31.408360:\n",
      "Train Epoch: 3 [5056/219709 (2%)]\tMean Loss : 2.968286\t time 0:01:30.799935:\n",
      "Train Epoch: 3 [7616/219709 (3%)]\tMean Loss : 2.955891\t time 0:01:28.555116:\n",
      "Train Epoch: 3 [10176/219709 (5%)]\tMean Loss : 2.963614\t time 0:01:28.143045:\n",
      "Train Epoch: 3 [12736/219709 (6%)]\tMean Loss : 2.958317\t time 0:01:28.603149:\n",
      "Train Epoch: 3 [15296/219709 (7%)]\tMean Loss : 2.960851\t time 0:01:28.508408:\n",
      "Train Epoch: 3 [17856/219709 (8%)]\tMean Loss : 2.959223\t time 0:01:29.824136:\n",
      "Train Epoch: 3 [20416/219709 (9%)]\tMean Loss : 2.967995\t time 0:01:29.702534:\n",
      "Train Epoch: 3 [22976/219709 (10%)]\tMean Loss : 2.962913\t time 0:01:28.556855:\n",
      "Train Epoch: 3 [25536/219709 (12%)]\tMean Loss : 2.961334\t time 0:01:28.626561:\n",
      "Train Epoch: 3 [28096/219709 (13%)]\tMean Loss : 2.960020\t time 0:01:28.283631:\n",
      "Train Epoch: 3 [30656/219709 (14%)]\tMean Loss : 2.959772\t time 0:01:29.907647:\n",
      "Train Epoch: 3 [33216/219709 (15%)]\tMean Loss : 2.959975\t time 0:01:28.265416:\n",
      "Train Epoch: 3 [35776/219709 (16%)]\tMean Loss : 2.960647\t time 0:01:28.271552:\n",
      "Train Epoch: 3 [38336/219709 (17%)]\tMean Loss : 2.960737\t time 0:01:29.601639:\n",
      "Train Epoch: 3 [40896/219709 (19%)]\tMean Loss : 2.957510\t time 0:01:30.519142:\n",
      "Train Epoch: 3 [43456/219709 (20%)]\tMean Loss : 2.962355\t time 0:01:27.864771:\n",
      "Train Epoch: 3 [46016/219709 (21%)]\tMean Loss : 2.957407\t time 0:01:27.472759:\n",
      "Train Epoch: 3 [48576/219709 (22%)]\tMean Loss : 2.958260\t time 0:01:28.237414:\n",
      "Train Epoch: 3 [51136/219709 (23%)]\tMean Loss : 2.963776\t time 0:01:29.061421:\n",
      "Train Epoch: 3 [53696/219709 (24%)]\tMean Loss : 2.961205\t time 0:01:28.460063:\n",
      "Train Epoch: 3 [56256/219709 (26%)]\tMean Loss : 2.959027\t time 0:01:26.671206:\n",
      "Train Epoch: 3 [58816/219709 (27%)]\tMean Loss : 2.961327\t time 0:01:30.332546:\n",
      "Train Epoch: 3 [61376/219709 (28%)]\tMean Loss : 2.951884\t time 0:01:28.159530:\n",
      "Train Epoch: 3 [63936/219709 (29%)]\tMean Loss : 2.959757\t time 0:01:29.468253:\n",
      "Train Epoch: 3 [66496/219709 (30%)]\tMean Loss : 2.953431\t time 0:01:28.627323:\n",
      "Train Epoch: 3 [69056/219709 (31%)]\tMean Loss : 2.957714\t time 0:01:29.209817:\n",
      "Train Epoch: 3 [71616/219709 (33%)]\tMean Loss : 2.958810\t time 0:01:29.503506:\n",
      "Train Epoch: 3 [74176/219709 (34%)]\tMean Loss : 2.965805\t time 0:01:27.747360:\n",
      "Train Epoch: 3 [76736/219709 (35%)]\tMean Loss : 2.960173\t time 0:01:29.036657:\n",
      "Train Epoch: 3 [79296/219709 (36%)]\tMean Loss : 2.957575\t time 0:01:27.497392:\n",
      "Train Epoch: 3 [81856/219709 (37%)]\tMean Loss : 2.954015\t time 0:01:28.591680:\n",
      "Train Epoch: 3 [84416/219709 (38%)]\tMean Loss : 2.956810\t time 0:01:30.676431:\n",
      "Train Epoch: 3 [86976/219709 (40%)]\tMean Loss : 2.957517\t time 0:01:28.474926:\n",
      "Train Epoch: 3 [89536/219709 (41%)]\tMean Loss : 2.953739\t time 0:01:28.996505:\n",
      "Train Epoch: 3 [92096/219709 (42%)]\tMean Loss : 2.961836\t time 0:01:28.884955:\n",
      "Train Epoch: 3 [94656/219709 (43%)]\tMean Loss : 2.958060\t time 0:01:27.201987:\n",
      "Train Epoch: 3 [97216/219709 (44%)]\tMean Loss : 2.954542\t time 0:01:30.229103:\n",
      "Train Epoch: 3 [99776/219709 (45%)]\tMean Loss : 2.955699\t time 0:01:29.599601:\n",
      "Train Epoch: 3 [102336/219709 (47%)]\tMean Loss : 2.960004\t time 0:01:27.647505:\n",
      "Train Epoch: 3 [104896/219709 (48%)]\tMean Loss : 2.957512\t time 0:01:28.835131:\n",
      "Train Epoch: 3 [107456/219709 (49%)]\tMean Loss : 2.964244\t time 0:01:27.750565:\n",
      "Train Epoch: 3 [110016/219709 (50%)]\tMean Loss : 2.957152\t time 0:01:29.500392:\n",
      "Train Epoch: 3 [112576/219709 (51%)]\tMean Loss : 2.957635\t time 0:01:29.171419:\n",
      "Train Epoch: 3 [115136/219709 (52%)]\tMean Loss : 2.966217\t time 0:01:28.763673:\n",
      "Train Epoch: 3 [117696/219709 (54%)]\tMean Loss : 2.961086\t time 0:01:29.739959:\n",
      "Train Epoch: 3 [120256/219709 (55%)]\tMean Loss : 2.956592\t time 0:01:28.576035:\n",
      "Train Epoch: 3 [122816/219709 (56%)]\tMean Loss : 2.950101\t time 0:01:28.134540:\n",
      "Train Epoch: 3 [125376/219709 (57%)]\tMean Loss : 2.955736\t time 0:01:29.852937:\n",
      "Train Epoch: 3 [127936/219709 (58%)]\tMean Loss : 2.962086\t time 0:01:29.414347:\n",
      "Train Epoch: 3 [130496/219709 (59%)]\tMean Loss : 2.949539\t time 0:01:28.864282:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [133056/219709 (61%)]\tMean Loss : 2.958021\t time 0:01:26.855363:\n",
      "Train Epoch: 3 [135616/219709 (62%)]\tMean Loss : 2.957306\t time 0:01:27.413099:\n",
      "Train Epoch: 3 [138176/219709 (63%)]\tMean Loss : 2.955300\t time 0:01:28.213190:\n",
      "Train Epoch: 3 [140736/219709 (64%)]\tMean Loss : 2.959444\t time 0:01:30.501114:\n",
      "Train Epoch: 3 [143296/219709 (65%)]\tMean Loss : 2.956383\t time 0:01:27.105799:\n",
      "Train Epoch: 3 [145856/219709 (66%)]\tMean Loss : 2.958987\t time 0:01:27.810386:\n",
      "Train Epoch: 3 [148416/219709 (68%)]\tMean Loss : 2.956971\t time 0:01:28.149143:\n",
      "Train Epoch: 3 [150976/219709 (69%)]\tMean Loss : 2.957462\t time 0:01:30.499819:\n",
      "Train Epoch: 3 [153536/219709 (70%)]\tMean Loss : 2.951393\t time 0:01:28.524077:\n",
      "Train Epoch: 3 [156096/219709 (71%)]\tMean Loss : 2.950846\t time 0:01:31.010778:\n",
      "Train Epoch: 3 [158656/219709 (72%)]\tMean Loss : 2.956939\t time 0:01:27.331991:\n",
      "Train Epoch: 3 [161216/219709 (73%)]\tMean Loss : 2.957613\t time 0:01:29.259798:\n",
      "Train Epoch: 3 [163776/219709 (75%)]\tMean Loss : 2.952177\t time 0:01:29.504712:\n",
      "Train Epoch: 3 [166336/219709 (76%)]\tMean Loss : 2.953374\t time 0:01:27.643685:\n",
      "Train Epoch: 3 [168896/219709 (77%)]\tMean Loss : 2.958430\t time 0:01:28.734907:\n",
      "Train Epoch: 3 [171456/219709 (78%)]\tMean Loss : 2.953400\t time 0:01:28.764458:\n",
      "Train Epoch: 3 [174016/219709 (79%)]\tMean Loss : 2.945608\t time 0:01:29.076399:\n",
      "Train Epoch: 3 [176576/219709 (80%)]\tMean Loss : 2.955007\t time 0:01:29.038892:\n",
      "Train Epoch: 3 [179136/219709 (82%)]\tMean Loss : 2.953577\t time 0:01:29.274969:\n",
      "Train Epoch: 3 [181696/219709 (83%)]\tMean Loss : 2.951809\t time 0:01:27.942062:\n",
      "Train Epoch: 3 [184256/219709 (84%)]\tMean Loss : 2.955939\t time 0:01:28.559659:\n",
      "Train Epoch: 3 [186816/219709 (85%)]\tMean Loss : 2.957615\t time 0:01:30.323356:\n",
      "Train Epoch: 3 [189376/219709 (86%)]\tMean Loss : 2.953649\t time 0:01:29.645448:\n",
      "Train Epoch: 3 [191936/219709 (87%)]\tMean Loss : 2.956013\t time 0:01:26.735152:\n",
      "Train Epoch: 3 [194496/219709 (89%)]\tMean Loss : 2.951329\t time 0:01:29.244901:\n",
      "Train Epoch: 3 [197056/219709 (90%)]\tMean Loss : 2.949431\t time 0:01:28.953668:\n",
      "Train Epoch: 3 [199616/219709 (91%)]\tMean Loss : 2.956426\t time 0:01:29.210845:\n",
      "Train Epoch: 3 [202176/219709 (92%)]\tMean Loss : 2.953897\t time 0:01:28.986880:\n",
      "Train Epoch: 3 [204736/219709 (93%)]\tMean Loss : 2.951457\t time 0:01:27.175985:\n",
      "Train Epoch: 3 [207296/219709 (94%)]\tMean Loss : 2.953117\t time 0:01:30.580868:\n",
      "Train Epoch: 3 [209856/219709 (96%)]\tMean Loss : 2.949647\t time 0:01:29.590155:\n",
      "Train Epoch: 3 [212416/219709 (97%)]\tMean Loss : 2.953660\t time 0:01:26.613337:\n",
      "Train Epoch: 3 [214976/219709 (98%)]\tMean Loss : 2.950438\t time 0:01:29.545077:\n",
      "Train Epoch: 3 [217536/219709 (99%)]\tMean Loss : 2.953541\t time 0:01:29.235449:\n",
      "\n",
      "Teacher forcing ratio: 0.8999999999999999\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 4 [2496/219709 (1%)]\tMean Loss : 2.952345\t time 0:01:31.221672:\n",
      "Train Epoch: 4 [5056/219709 (2%)]\tMean Loss : 2.963092\t time 0:01:29.719407:\n",
      "Train Epoch: 4 [7616/219709 (3%)]\tMean Loss : 2.961078\t time 0:01:28.641842:\n",
      "Train Epoch: 4 [10176/219709 (5%)]\tMean Loss : 2.956000\t time 0:01:30.709130:\n",
      "Train Epoch: 4 [12736/219709 (6%)]\tMean Loss : 2.962869\t time 0:01:30.139317:\n",
      "Train Epoch: 4 [15296/219709 (7%)]\tMean Loss : 2.957477\t time 0:01:29.504893:\n",
      "Train Epoch: 4 [17856/219709 (8%)]\tMean Loss : 2.960235\t time 0:01:29.055418:\n",
      "Train Epoch: 4 [20416/219709 (9%)]\tMean Loss : 2.961708\t time 0:01:28.259404:\n",
      "Train Epoch: 4 [22976/219709 (10%)]\tMean Loss : 2.952401\t time 0:01:28.393025:\n",
      "Train Epoch: 4 [25536/219709 (12%)]\tMean Loss : 2.959644\t time 0:01:30.212008:\n",
      "Train Epoch: 4 [28096/219709 (13%)]\tMean Loss : 2.955157\t time 0:01:27.302556:\n",
      "Train Epoch: 4 [30656/219709 (14%)]\tMean Loss : 2.964635\t time 0:01:28.150290:\n",
      "Train Epoch: 4 [33216/219709 (15%)]\tMean Loss : 2.957740\t time 0:01:29.716447:\n",
      "Train Epoch: 4 [35776/219709 (16%)]\tMean Loss : 2.958181\t time 0:01:27.858856:\n",
      "Train Epoch: 4 [38336/219709 (17%)]\tMean Loss : 2.962344\t time 0:01:26.759181:\n",
      "Train Epoch: 4 [40896/219709 (19%)]\tMean Loss : 2.961304\t time 0:01:29.671857:\n",
      "Train Epoch: 4 [43456/219709 (20%)]\tMean Loss : 2.958278\t time 0:01:28.906962:\n",
      "Train Epoch: 4 [46016/219709 (21%)]\tMean Loss : 2.960952\t time 0:01:29.463199:\n",
      "Train Epoch: 4 [48576/219709 (22%)]\tMean Loss : 2.955472\t time 0:01:27.917910:\n",
      "Train Epoch: 4 [51136/219709 (23%)]\tMean Loss : 2.955035\t time 0:01:28.683529:\n",
      "Train Epoch: 4 [53696/219709 (24%)]\tMean Loss : 2.957667\t time 0:01:30.401235:\n",
      "Train Epoch: 4 [56256/219709 (26%)]\tMean Loss : 2.963361\t time 0:01:29.119547:\n",
      "Train Epoch: 4 [58816/219709 (27%)]\tMean Loss : 2.956534\t time 0:01:28.634611:\n",
      "Train Epoch: 4 [61376/219709 (28%)]\tMean Loss : 2.954888\t time 0:01:28.115900:\n",
      "Train Epoch: 4 [63936/219709 (29%)]\tMean Loss : 2.957860\t time 0:01:27.446125:\n",
      "Train Epoch: 4 [66496/219709 (30%)]\tMean Loss : 2.964868\t time 0:01:28.446508:\n",
      "Train Epoch: 4 [69056/219709 (31%)]\tMean Loss : 2.962885\t time 0:01:30.929157:\n",
      "Train Epoch: 4 [71616/219709 (33%)]\tMean Loss : 2.952897\t time 0:01:26.760029:\n",
      "Train Epoch: 4 [74176/219709 (34%)]\tMean Loss : 2.956196\t time 0:01:28.798440:\n",
      "Train Epoch: 4 [76736/219709 (35%)]\tMean Loss : 2.953501\t time 0:01:28.861753:\n",
      "Train Epoch: 4 [79296/219709 (36%)]\tMean Loss : 2.958299\t time 0:01:27.990843:\n",
      "Train Epoch: 4 [81856/219709 (37%)]\tMean Loss : 2.948960\t time 0:01:29.076563:\n",
      "Train Epoch: 4 [84416/219709 (38%)]\tMean Loss : 2.951940\t time 0:01:31.828284:\n",
      "Train Epoch: 4 [86976/219709 (40%)]\tMean Loss : 2.955948\t time 0:01:29.305672:\n",
      "Train Epoch: 4 [89536/219709 (41%)]\tMean Loss : 2.956925\t time 0:01:29.495276:\n",
      "Train Epoch: 4 [92096/219709 (42%)]\tMean Loss : 2.958408\t time 0:01:29.309654:\n",
      "Train Epoch: 4 [94656/219709 (43%)]\tMean Loss : 2.952520\t time 0:01:27.719461:\n",
      "Train Epoch: 4 [97216/219709 (44%)]\tMean Loss : 2.950382\t time 0:01:30.422473:\n",
      "Train Epoch: 4 [99776/219709 (45%)]\tMean Loss : 2.961416\t time 0:01:29.444435:\n",
      "Train Epoch: 4 [102336/219709 (47%)]\tMean Loss : 2.953910\t time 0:01:28.702519:\n",
      "Train Epoch: 4 [104896/219709 (48%)]\tMean Loss : 2.953059\t time 0:01:28.267668:\n",
      "Train Epoch: 4 [107456/219709 (49%)]\tMean Loss : 2.958215\t time 0:01:28.463686:\n",
      "Train Epoch: 4 [110016/219709 (50%)]\tMean Loss : 2.953494\t time 0:01:31.762291:\n",
      "Train Epoch: 4 [112576/219709 (51%)]\tMean Loss : 2.959470\t time 0:01:29.137902:\n",
      "Train Epoch: 4 [115136/219709 (52%)]\tMean Loss : 2.954225\t time 0:01:29.867103:\n",
      "Train Epoch: 4 [117696/219709 (54%)]\tMean Loss : 2.956999\t time 0:01:28.337629:\n",
      "Train Epoch: 4 [120256/219709 (55%)]\tMean Loss : 2.959234\t time 0:01:28.740795:\n",
      "Train Epoch: 4 [122816/219709 (56%)]\tMean Loss : 2.956503\t time 0:01:28.915351:\n",
      "Train Epoch: 4 [125376/219709 (57%)]\tMean Loss : 2.953930\t time 0:01:30.692647:\n",
      "Train Epoch: 4 [127936/219709 (58%)]\tMean Loss : 2.955188\t time 0:01:27.807749:\n",
      "Train Epoch: 4 [130496/219709 (59%)]\tMean Loss : 2.956283\t time 0:01:28.721577:\n",
      "Train Epoch: 4 [133056/219709 (61%)]\tMean Loss : 2.958578\t time 0:01:30.308447:\n",
      "Train Epoch: 4 [135616/219709 (62%)]\tMean Loss : 2.954923\t time 0:01:30.739644:\n",
      "Train Epoch: 4 [138176/219709 (63%)]\tMean Loss : 2.955363\t time 0:01:31.079484:\n",
      "Train Epoch: 4 [140736/219709 (64%)]\tMean Loss : 2.957721\t time 0:01:29.442088:\n",
      "Train Epoch: 4 [143296/219709 (65%)]\tMean Loss : 2.956427\t time 0:01:30.073223:\n",
      "Train Epoch: 4 [145856/219709 (66%)]\tMean Loss : 2.957964\t time 0:01:31.259756:\n",
      "Train Epoch: 4 [148416/219709 (68%)]\tMean Loss : 2.952520\t time 0:01:29.389080:\n",
      "Train Epoch: 4 [150976/219709 (69%)]\tMean Loss : 2.955300\t time 0:01:28.290639:\n",
      "Train Epoch: 4 [153536/219709 (70%)]\tMean Loss : 2.959406\t time 0:01:30.200101:\n",
      "Train Epoch: 4 [156096/219709 (71%)]\tMean Loss : 2.949675\t time 0:01:27.209261:\n",
      "Train Epoch: 4 [158656/219709 (72%)]\tMean Loss : 2.948088\t time 0:01:30.367426:\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.ASGD(model.parameters(), lr=0.05)  # lr = 0.2 used in paper\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    # scheduler.step()                                 # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.025, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOES DEEPER NETWORK HELP ?\n",
    "\n",
    "### DOES AMSGRAD HELP ?\n",
    "\n",
    "### DOES LAYER NORMALIZATION HELP ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models_librispeech/amsgrad_layer_norm_2019-12-28 14:37:25.274924\n",
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [2496/219709 (1%)]\tMean Loss : 3.399897\t time 0:01:53.667303:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1762927a8fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTeacher forcing ratio:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m# scheduler.step()                                    # Decrease learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'las_model_{epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra/ASR/DIC/Code/Speech_To_Text/LAS Model/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, print_interval, writer, log_interval)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra/ASR/DIC/Code/Speech_To_Text/LAS Model/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, target)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra/ASR/DIC/Code/Speech_To_Text/LAS Model/attend_and_spell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, yt_prev, hidden_prev, encoder_output, c_prev)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m# context vector: c_i = AttentionContext(encoder_out, s_i)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_i\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;31m#--------------Spell----------------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra/ASR/DIC/Code/Speech_To_Text/LAS Model/attend_and_spell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, h, s_prev)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mconcat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_prev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# (N, Tx)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# sum(alphas) = 1, over Tx axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1370\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1371\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1372\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1373\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # optimizer = optim.ASGD(model.parameters(), lr=0.05)  # lr = 0.2 used in paper\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "# # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "# log_interval = 5\n",
    "# print_interval = 40\n",
    "\n",
    "# epochs = 20\n",
    "# load = False\n",
    "\n",
    "# summary_dir = os.path.join(tensorboard_dir, name)\n",
    "# writer = SummaryWriter(summary_dir)\n",
    "# print('save_dir', save_dir)\n",
    "\n",
    "# for epoch in range(0, epochs):\n",
    "#     print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "#     train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "#     # scheduler.step()                                    # Decrease learning rate\n",
    "#     torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "#     model.tf_ratio = max(model.tf_ratio - 0.05, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
