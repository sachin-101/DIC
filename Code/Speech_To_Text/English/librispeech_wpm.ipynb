{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import string\n",
    "import IPython\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower().replace('\\n', '')\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(root_dir, dataset, file_ext, csv_file):\n",
    "    dataset_dir = os.path.join(root_dir, dataset)\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text_file:\n",
    "                data_list = text_file.readlines()\n",
    "            for example in data_list:\n",
    "                path = os.path.join(dataset, str(example.split(' ')[0])) + file_ext   \n",
    "                sent = preprocess(str(' '.join(example.split(' ')[1:])))\n",
    "                data.append((path, sent))\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['path', 'sent'])\n",
    "    data_df.to_csv(os.path.join(root_dir, csv_file), header=None)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "root_dir = '../../../Dataset/LibriSpeech'\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0001.flac</td>\n",
       "      <td>that had its source away back in the woods of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0002.flac</td>\n",
       "      <td>for not even a brook could run past missus rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_100/103-1240-0003.flac</td>\n",
       "      <td>and that if she noticed anything odd or out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_100/103-1240-0004.flac</td>\n",
       "      <td>but missus rachel lynde was one of those capab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0001.flac   \n",
       "2  dataset_100/103-1240-0002.flac   \n",
       "3  dataset_100/103-1240-0003.flac   \n",
       "4  dataset_100/103-1240-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  that had its source away back in the woods of ...  \n",
       "2  for not even a brook could run past missus rac...  \n",
       "3  and that if she noticed anything odd or out of...  \n",
       "4  but missus rachel lynde was one of those capab...  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a train_df for each 100h, 360h, 500h datsets\n",
    "make_train_df(root_dir, 'dataset_100', '.flac', 'train_100.csv') # for 100h\n",
    "# read csv file\n",
    "train_100 = pd.read_csv(os.path.join(root_dir, 'train_100.csv'), header=None, names=['path', 'sent'])\n",
    "train_100.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_360/100-121669-0000.flac</td>\n",
       "      <td>tom the pipers son</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_360/100-121669-0001.flac</td>\n",
       "      <td>the pig was eat and tom was beat and tom ran c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_360/100-121669-0002.flac</td>\n",
       "      <td>he never did any work except to play the pipes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_360/100-121669-0003.flac</td>\n",
       "      <td>but he was so sly and cautious that no one had...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_360/100-121669-0004.flac</td>\n",
       "      <td>and they lived all alone in a little hut away ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path  \\\n",
       "0  dataset_360/100-121669-0000.flac   \n",
       "1  dataset_360/100-121669-0001.flac   \n",
       "2  dataset_360/100-121669-0002.flac   \n",
       "3  dataset_360/100-121669-0003.flac   \n",
       "4  dataset_360/100-121669-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0                                 tom the pipers son  \n",
       "1  the pig was eat and tom was beat and tom ran c...  \n",
       "2  he never did any work except to play the pipes...  \n",
       "3  but he was so sly and cautious that no one had...  \n",
       "4  and they lived all alone in a little hut away ...  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_train_df(root_dir, 'dataset_360', '.flac', 'train_360.csv') # for 360h\n",
    "# Read csv file\n",
    "train_360 = pd.read_csv(os.path.join(root_dir, 'train_360.csv'), header=None, names=['path', 'sent'])\n",
    "train_360.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_500/1006-135212-0000.flac</td>\n",
       "      <td>coming as it did at a period of exceptional du...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_500/1006-135212-0001.flac</td>\n",
       "      <td>interest drooped however when after weeks of f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_500/1006-135212-0002.flac</td>\n",
       "      <td>it would be as well perhaps that i should refr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_500/1006-135212-0003.flac</td>\n",
       "      <td>in the year already mentioned a train left eus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_500/1006-135212-0004.flac</td>\n",
       "      <td>the train however is a favourite one among man...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                path  \\\n",
       "0  dataset_500/1006-135212-0000.flac   \n",
       "1  dataset_500/1006-135212-0001.flac   \n",
       "2  dataset_500/1006-135212-0002.flac   \n",
       "3  dataset_500/1006-135212-0003.flac   \n",
       "4  dataset_500/1006-135212-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  coming as it did at a period of exceptional du...  \n",
       "1  interest drooped however when after weeks of f...  \n",
       "2  it would be as well perhaps that i should refr...  \n",
       "3  in the year already mentioned a train left eus...  \n",
       "4  the train however is a favourite one among man...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_train_df(root_dir, 'dataset_500', '.flac', 'train_500.csv') # for 500h\n",
    "# read csv\n",
    "train_500 = pd.read_csv(os.path.join(root_dir, 'train_500.csv'), header=None, names=['path', 'sent'])\n",
    "train_500.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 281241\n"
     ]
    }
   ],
   "source": [
    "# Combine all the datasets\n",
    "train_df = pd.concat([train_100, train_360, train_500])\n",
    "print(\"Number of training examples:\", train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num words in vocab: 20167\n"
     ]
    }
   ],
   "source": [
    "def create_vocab(train_df):\n",
    "    vocab = []\n",
    "    for idx in range(train_df.shape[0]):\n",
    "        _, sent = train_df.iloc[idx]\n",
    "        words = sent.split(\" \")\n",
    "        for w in words:\n",
    "            if w not in vocab:\n",
    "                vocab.append(w)\n",
    "        if idx==10000:\n",
    "            break\n",
    "    return vocab\n",
    "\n",
    "vocab = create_vocab(train_df)\n",
    "print(\"num words in vocab:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chapter',\n",
       " 'one',\n",
       " 'missus',\n",
       " 'rachel',\n",
       " 'lynde',\n",
       " 'is',\n",
       " 'surprised',\n",
       " 'lived',\n",
       " 'just',\n",
       " 'where',\n",
       " 'the',\n",
       " 'avonlea',\n",
       " 'main',\n",
       " 'road',\n",
       " 'dipped',\n",
       " 'down',\n",
       " 'into',\n",
       " 'a',\n",
       " 'little',\n",
       " 'hollow',\n",
       " 'fringed',\n",
       " 'with',\n",
       " 'alders',\n",
       " 'and',\n",
       " 'ladies',\n",
       " 'eardrops',\n",
       " 'traversed',\n",
       " 'by',\n",
       " 'brook',\n",
       " 'that',\n",
       " 'had',\n",
       " 'its',\n",
       " 'source',\n",
       " 'away',\n",
       " 'back',\n",
       " 'in',\n",
       " 'woods',\n",
       " 'of',\n",
       " 'old',\n",
       " 'cuthbert',\n",
       " 'place',\n",
       " 'it',\n",
       " 'was',\n",
       " 'reputed',\n",
       " 'to',\n",
       " 'be',\n",
       " 'an',\n",
       " 'intricate',\n",
       " 'headlong',\n",
       " 'earlier',\n",
       " 'course',\n",
       " 'through',\n",
       " 'those',\n",
       " 'dark',\n",
       " 'secrets',\n",
       " 'pool',\n",
       " 'cascade',\n",
       " 'but',\n",
       " 'time',\n",
       " 'reached',\n",
       " 'lyndes',\n",
       " 'quiet',\n",
       " 'well',\n",
       " 'conducted',\n",
       " 'stream',\n",
       " 'for',\n",
       " 'not',\n",
       " 'even',\n",
       " 'could',\n",
       " 'run',\n",
       " 'past',\n",
       " 'door',\n",
       " 'without',\n",
       " 'due',\n",
       " 'regard',\n",
       " 'decency',\n",
       " 'decorum',\n",
       " 'probably',\n",
       " 'conscious',\n",
       " 'sitting',\n",
       " 'at',\n",
       " 'her',\n",
       " 'window',\n",
       " 'keeping',\n",
       " 'sharp',\n",
       " 'eye',\n",
       " 'on',\n",
       " 'everything',\n",
       " 'passed',\n",
       " 'from',\n",
       " 'brooks',\n",
       " 'children',\n",
       " 'up',\n",
       " 'if',\n",
       " 'she',\n",
       " 'noticed',\n",
       " 'anything',\n",
       " 'odd',\n",
       " 'or',\n",
       " 'out',\n",
       " 'would',\n",
       " 'never',\n",
       " 'rest',\n",
       " 'until',\n",
       " 'ferreted',\n",
       " 'whys',\n",
       " 'wherefores',\n",
       " 'thereof',\n",
       " 'there',\n",
       " 'are',\n",
       " 'plenty',\n",
       " 'people',\n",
       " 'who',\n",
       " 'can',\n",
       " 'attend',\n",
       " 'closely',\n",
       " 'their',\n",
       " 'neighbors',\n",
       " 'business',\n",
       " 'dint',\n",
       " 'neglecting',\n",
       " 'own',\n",
       " 'capable',\n",
       " 'creatures',\n",
       " 'manage',\n",
       " 'concerns',\n",
       " 'other',\n",
       " 'folks',\n",
       " 'bargain',\n",
       " 'notable',\n",
       " 'housewife',\n",
       " 'work',\n",
       " 'always',\n",
       " 'done',\n",
       " 'ran',\n",
       " 'sewing',\n",
       " 'circle',\n",
       " 'helped',\n",
       " 'sunday',\n",
       " 'school',\n",
       " 'strongest',\n",
       " 'prop',\n",
       " 'church',\n",
       " 'aid',\n",
       " 'society',\n",
       " 'foreign',\n",
       " 'missions',\n",
       " 'auxiliary',\n",
       " 'yet',\n",
       " 'all',\n",
       " 'this',\n",
       " 'found',\n",
       " 'abundant',\n",
       " 'sit',\n",
       " 'hours',\n",
       " 'kitchen',\n",
       " 'knitting',\n",
       " 'cotton',\n",
       " 'warp',\n",
       " 'quilts',\n",
       " 'knitted',\n",
       " 'sixteen',\n",
       " 'them',\n",
       " 'as',\n",
       " 'housekeepers',\n",
       " 'were',\n",
       " 'wont',\n",
       " 'tell',\n",
       " 'awed',\n",
       " 'voices',\n",
       " 'crossed',\n",
       " 'wound',\n",
       " 'steep',\n",
       " 'red',\n",
       " 'hill',\n",
       " 'beyond',\n",
       " 'anybody',\n",
       " 'went',\n",
       " 'pass',\n",
       " 'over',\n",
       " 'so',\n",
       " 'unseen',\n",
       " 'gauntlet',\n",
       " 'rachels',\n",
       " 'seeing',\n",
       " 'afternoon',\n",
       " 'early',\n",
       " 'june',\n",
       " 'sun',\n",
       " 'coming',\n",
       " 'warm',\n",
       " 'bright',\n",
       " 'orchard',\n",
       " 'slope',\n",
       " 'below',\n",
       " 'house',\n",
       " 'bridal',\n",
       " 'flush',\n",
       " 'pinky',\n",
       " 'white',\n",
       " 'bloom',\n",
       " 'hummed',\n",
       " 'myriad',\n",
       " 'bees',\n",
       " 'thomas',\n",
       " 'meek',\n",
       " 'man',\n",
       " 'whom',\n",
       " 'called',\n",
       " 'husband',\n",
       " 'sowing',\n",
       " 'his',\n",
       " 'late',\n",
       " 'turnip',\n",
       " 'seed',\n",
       " 'field',\n",
       " 'barn',\n",
       " 'knew',\n",
       " 'he',\n",
       " 'ought',\n",
       " 'because',\n",
       " 'heard',\n",
       " 'him',\n",
       " 'peter',\n",
       " 'morrison',\n",
       " 'evening',\n",
       " 'before',\n",
       " 'william',\n",
       " 'j',\n",
       " 'blairs',\n",
       " 'store',\n",
       " 'carmody',\n",
       " 'meant',\n",
       " 'sow',\n",
       " 'next',\n",
       " 'asked',\n",
       " 'matthew',\n",
       " 'been',\n",
       " 'known',\n",
       " 'volunteer',\n",
       " 'information',\n",
       " 'about',\n",
       " 'whole',\n",
       " 'life',\n",
       " 'here',\n",
       " 'half',\n",
       " 'three',\n",
       " 'busy',\n",
       " 'day',\n",
       " 'placidly',\n",
       " 'driving',\n",
       " 'best',\n",
       " 'suit',\n",
       " 'clothes',\n",
       " 'which',\n",
       " 'plain',\n",
       " 'proof',\n",
       " 'going',\n",
       " 'buggy',\n",
       " 'sorrel',\n",
       " 'mare',\n",
       " 'betokened',\n",
       " 'considerable',\n",
       " 'distance',\n",
       " 'now',\n",
       " 'why',\n",
       " 'any',\n",
       " 'deftly',\n",
       " 'putting',\n",
       " 'together',\n",
       " 'might',\n",
       " 'have',\n",
       " 'given',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'guess',\n",
       " 'both',\n",
       " 'questions',\n",
       " 'rarely',\n",
       " 'home',\n",
       " 'must',\n",
       " 'something',\n",
       " 'pressing',\n",
       " 'unusual',\n",
       " 'taking',\n",
       " 'shyest',\n",
       " 'alive',\n",
       " 'hated',\n",
       " 'go',\n",
       " 'among',\n",
       " 'strangers',\n",
       " 'talk',\n",
       " 'dressed',\n",
       " 'collar',\n",
       " 'didnt',\n",
       " 'happen',\n",
       " 'often',\n",
       " 'ponder',\n",
       " 'make',\n",
       " 'nothing',\n",
       " 'afternoons',\n",
       " 'enjoyment',\n",
       " 'spoiled',\n",
       " 'ill',\n",
       " 'step',\n",
       " 'green',\n",
       " 'gables',\n",
       " 'after',\n",
       " 'tea',\n",
       " 'find',\n",
       " 'marilla',\n",
       " 'hes',\n",
       " 'gone',\n",
       " 'worthy',\n",
       " 'woman',\n",
       " 'finally',\n",
       " 'concluded',\n",
       " 'doesnt',\n",
       " 'generally',\n",
       " 'town',\n",
       " 'year',\n",
       " 'visits',\n",
       " 'hed',\n",
       " 'wouldnt',\n",
       " 'dress',\n",
       " 'take',\n",
       " 'more',\n",
       " 'happened',\n",
       " 'since',\n",
       " 'last',\n",
       " 'night',\n",
       " 'start',\n",
       " 'off',\n",
       " 'im',\n",
       " 'clean',\n",
       " 'puzzled',\n",
       " 'thats',\n",
       " 'what',\n",
       " 'i',\n",
       " 'know',\n",
       " 'minutes',\n",
       " 'peace',\n",
       " 'mind',\n",
       " 'conscience',\n",
       " 'has',\n",
       " 'taken',\n",
       " 'today',\n",
       " 'accordingly',\n",
       " 'set',\n",
       " 'far',\n",
       " 'big',\n",
       " 'rambling',\n",
       " 'embowered',\n",
       " 'cuthberts',\n",
       " 'scant',\n",
       " 'quarter',\n",
       " 'mile',\n",
       " 'sure',\n",
       " 'long',\n",
       " 'lane',\n",
       " 'made',\n",
       " 'deal',\n",
       " 'further',\n",
       " 'father',\n",
       " 'shy',\n",
       " 'silent',\n",
       " 'son',\n",
       " 'got',\n",
       " 'possibly',\n",
       " 'fellow',\n",
       " 'men',\n",
       " 'actually',\n",
       " 'retreating',\n",
       " 'when',\n",
       " 'founded',\n",
       " 'homestead',\n",
       " 'built',\n",
       " 'furthest',\n",
       " 'edge',\n",
       " 'cleared',\n",
       " 'land',\n",
       " 'barely',\n",
       " 'visible',\n",
       " 'along',\n",
       " 'houses',\n",
       " 'sociably',\n",
       " 'situated',\n",
       " 'did',\n",
       " 'call',\n",
       " 'living',\n",
       " 'such',\n",
       " 'staying',\n",
       " 'said',\n",
       " 'stepped',\n",
       " 'deep',\n",
       " 'rutted',\n",
       " 'grassy',\n",
       " 'bordered',\n",
       " 'wild',\n",
       " 'rose',\n",
       " 'bushes',\n",
       " 'no',\n",
       " 'wonder',\n",
       " 'themselves',\n",
       " 'trees',\n",
       " 'arent',\n",
       " 'much',\n",
       " 'company',\n",
       " 'though',\n",
       " 'dear',\n",
       " 'knows',\n",
       " 'they',\n",
       " 'thered',\n",
       " 'enough',\n",
       " 'id',\n",
       " 'ruther',\n",
       " 'look',\n",
       " 'seem',\n",
       " 'contented',\n",
       " 'then',\n",
       " 'suppose',\n",
       " 'theyre',\n",
       " 'used',\n",
       " 'body',\n",
       " 'get',\n",
       " 'being',\n",
       " 'hanged',\n",
       " 'irishman',\n",
       " 'backyard',\n",
       " 'very',\n",
       " 'neat',\n",
       " 'precise',\n",
       " 'yard',\n",
       " 'side',\n",
       " 'great',\n",
       " 'patriarchal',\n",
       " 'willows',\n",
       " 'prim',\n",
       " 'lombardies',\n",
       " 'stray',\n",
       " 'stick',\n",
       " 'nor',\n",
       " 'stone',\n",
       " 'seen',\n",
       " 'privately',\n",
       " 'opinion',\n",
       " 'swept',\n",
       " 'eaten',\n",
       " 'meal',\n",
       " 'ground',\n",
       " 'overbrimming',\n",
       " 'proverbial',\n",
       " 'peck',\n",
       " 'dirt',\n",
       " 'rapped',\n",
       " 'smartly',\n",
       " 'bidden',\n",
       " 'do',\n",
       " 'cheerful',\n",
       " 'apartment',\n",
       " 'painfully',\n",
       " 'give',\n",
       " 'appearance',\n",
       " 'unused',\n",
       " 'parlor',\n",
       " 'windows',\n",
       " 'looked',\n",
       " 'east',\n",
       " 'west',\n",
       " 'looking',\n",
       " 'came',\n",
       " 'flood',\n",
       " 'mellow',\n",
       " 'sunlight',\n",
       " 'whence',\n",
       " 'you',\n",
       " 'glimpse',\n",
       " 'cherry',\n",
       " 'left',\n",
       " 'nodding',\n",
       " 'slender',\n",
       " 'birches',\n",
       " 'greened',\n",
       " 'tangle',\n",
       " 'vines',\n",
       " 'sat',\n",
       " 'slightly',\n",
       " 'distrustful',\n",
       " 'sunshine',\n",
       " 'table',\n",
       " 'behind',\n",
       " 'laid',\n",
       " 'supper',\n",
       " 'fairly',\n",
       " 'closed',\n",
       " 'plates',\n",
       " 'expecting',\n",
       " 'some',\n",
       " 'dishes',\n",
       " 'everyday',\n",
       " 'only',\n",
       " 'crab',\n",
       " 'apple',\n",
       " 'preserves',\n",
       " 'kind',\n",
       " 'cake',\n",
       " 'expected',\n",
       " 'particular',\n",
       " 'matthews',\n",
       " 'getting',\n",
       " 'dizzy',\n",
       " 'mystery',\n",
       " 'unmysterious',\n",
       " 'briskly',\n",
       " 'real',\n",
       " 'fine',\n",
       " 'isnt',\n",
       " 'how',\n",
       " 'your',\n",
       " 'lack',\n",
       " 'name',\n",
       " 'friendship',\n",
       " 'existed',\n",
       " 'between',\n",
       " 'spite',\n",
       " 'perhaps',\n",
       " 'dissimilarity',\n",
       " 'tall',\n",
       " 'thin',\n",
       " 'angles',\n",
       " 'curves',\n",
       " 'hair',\n",
       " 'showed',\n",
       " 'gray',\n",
       " 'streaks',\n",
       " 'twisted',\n",
       " 'hard',\n",
       " 'knot',\n",
       " 'two',\n",
       " 'wire',\n",
       " 'hairpins',\n",
       " 'stuck',\n",
       " 'aggressively',\n",
       " 'like',\n",
       " 'narrow',\n",
       " 'experience',\n",
       " 'rigid',\n",
       " 'saving',\n",
       " 'mouth',\n",
       " 'ever',\n",
       " 'developed',\n",
       " 'considered',\n",
       " 'indicative',\n",
       " 'sense',\n",
       " 'humor',\n",
       " 'afraid',\n",
       " 'werent',\n",
       " 'saw',\n",
       " 'starting',\n",
       " 'thought',\n",
       " 'maybe',\n",
       " 'doctors',\n",
       " 'marillas',\n",
       " 'lips',\n",
       " 'twitched',\n",
       " 'understandingly',\n",
       " 'sight',\n",
       " 'jaunting',\n",
       " 'unaccountably',\n",
       " 'too',\n",
       " 'curiosity',\n",
       " 'oh',\n",
       " 'quite',\n",
       " 'although',\n",
       " 'bad',\n",
       " 'headache',\n",
       " 'yesterday',\n",
       " 'river',\n",
       " 'boy',\n",
       " 'orphan',\n",
       " 'asylum',\n",
       " 'nova',\n",
       " 'scotia',\n",
       " 'train',\n",
       " 'tonight',\n",
       " 'meet',\n",
       " 'kangaroo',\n",
       " 'australia',\n",
       " 'astonished',\n",
       " 'stricken',\n",
       " 'dumb',\n",
       " 'five',\n",
       " 'seconds',\n",
       " 'unsupposable',\n",
       " 'making',\n",
       " 'fun',\n",
       " 'almost',\n",
       " 'forced',\n",
       " 'earnest',\n",
       " 'demanded',\n",
       " 'voice',\n",
       " 'returned',\n",
       " 'yes',\n",
       " 'boys',\n",
       " 'asylums',\n",
       " 'part',\n",
       " 'usual',\n",
       " 'spring',\n",
       " 'regulated',\n",
       " 'farm',\n",
       " 'instead',\n",
       " 'unheard',\n",
       " 'innovation',\n",
       " 'felt',\n",
       " 'received',\n",
       " 'severe',\n",
       " 'mental',\n",
       " 'jolt',\n",
       " 'exclamation',\n",
       " 'points',\n",
       " 'adopting',\n",
       " 'world',\n",
       " 'certainly',\n",
       " 'turning',\n",
       " 'upside',\n",
       " 'earth',\n",
       " 'put',\n",
       " 'notion',\n",
       " 'head',\n",
       " 'disapprovingly',\n",
       " 'advice',\n",
       " 'perforce',\n",
       " 'disapproved',\n",
       " 'weve',\n",
       " 'thinking',\n",
       " 'winter',\n",
       " 'fact',\n",
       " 'alexander',\n",
       " 'spencer',\n",
       " 'christmas',\n",
       " 'girl',\n",
       " 'hopeton',\n",
       " 'talked',\n",
       " 'we',\n",
       " 'wed',\n",
       " 'years',\n",
       " 'sixty',\n",
       " 'spry',\n",
       " 'once',\n",
       " 'heart',\n",
       " 'troubles',\n",
       " 'desperate',\n",
       " 'hired',\n",
       " 'help',\n",
       " 'theres',\n",
       " 'stupid',\n",
       " 'grown',\n",
       " 'french',\n",
       " 'soon',\n",
       " 'broke',\n",
       " 'ways',\n",
       " 'taught',\n",
       " 'lobster',\n",
       " 'canneries',\n",
       " 'states',\n",
       " 'first',\n",
       " 'suggested',\n",
       " 'flat',\n",
       " 'may',\n",
       " 'right',\n",
       " 'saying',\n",
       " 'london',\n",
       " 'street',\n",
       " 'arabs',\n",
       " 'me',\n",
       " 'native',\n",
       " 'born',\n",
       " 'least',\n",
       " 'therell',\n",
       " 'risk',\n",
       " 'matter',\n",
       " 'feel',\n",
       " 'easier',\n",
       " 'my',\n",
       " 'sleep',\n",
       " 'sounder',\n",
       " 'nights',\n",
       " 'canadian',\n",
       " 'end',\n",
       " 'decided',\n",
       " 'ask',\n",
       " 'pick',\n",
       " 'us',\n",
       " 'week',\n",
       " 'sent',\n",
       " 'word',\n",
       " 'richard',\n",
       " 'spencers',\n",
       " 'bring',\n",
       " 'smart',\n",
       " 'likely',\n",
       " 'ten',\n",
       " 'eleven',\n",
       " 'age',\n",
       " 'use',\n",
       " 'doing',\n",
       " 'chores',\n",
       " 'young',\n",
       " 'trained',\n",
       " 'proper',\n",
       " 'mean',\n",
       " 'schooling',\n",
       " 'telegram',\n",
       " 'mail',\n",
       " 'brought',\n",
       " 'station',\n",
       " 'thirty',\n",
       " 'will',\n",
       " 'drop',\n",
       " 'goes',\n",
       " 'sands',\n",
       " 'herself',\n",
       " 'prided',\n",
       " 'speaking',\n",
       " 'proceeded',\n",
       " 'speak',\n",
       " 'having',\n",
       " 'adjusted',\n",
       " 'attitude',\n",
       " 'amazing',\n",
       " 'piece',\n",
       " 'news',\n",
       " 'think',\n",
       " 'youre',\n",
       " 'mighty',\n",
       " 'foolish',\n",
       " 'thing',\n",
       " 'risky',\n",
       " 'dont',\n",
       " 'bringing',\n",
       " 'strange',\n",
       " 'child',\n",
       " 'single',\n",
       " 'disposition',\n",
       " 'sort',\n",
       " 'parents',\n",
       " 'turn',\n",
       " 'read',\n",
       " 'paper',\n",
       " 'wife',\n",
       " 'island',\n",
       " 'took',\n",
       " 'fire',\n",
       " 'purpose',\n",
       " 'nearly',\n",
       " 'burnt',\n",
       " 'crisp',\n",
       " 'beds',\n",
       " 'another',\n",
       " 'case',\n",
       " 'adopted',\n",
       " 'suck',\n",
       " 'eggs',\n",
       " 'couldnt',\n",
       " 'break',\n",
       " 'mercys',\n",
       " 'sake',\n",
       " 'jobs',\n",
       " 'comforting',\n",
       " 'seemed',\n",
       " 'neither',\n",
       " 'offend',\n",
       " 'alarm',\n",
       " 'steadily',\n",
       " 'deny',\n",
       " 'say',\n",
       " 'ive',\n",
       " 'qualms',\n",
       " 'myself',\n",
       " 'terrible',\n",
       " 'see',\n",
       " 'gave',\n",
       " 'seldom',\n",
       " 'sets',\n",
       " 'does',\n",
       " 'duty',\n",
       " 'risks',\n",
       " 'near',\n",
       " 'peoples',\n",
       " 'comes',\n",
       " 'close',\n",
       " 'england',\n",
       " 'cant',\n",
       " 'different',\n",
       " 'ourselves',\n",
       " 'hope',\n",
       " 'tone',\n",
       " 'plainly',\n",
       " 'indicated',\n",
       " 'painful',\n",
       " 'doubts',\n",
       " 'warn',\n",
       " 'burns',\n",
       " 'puts',\n",
       " 'strychnine',\n",
       " 'new',\n",
       " 'brunswick',\n",
       " 'family',\n",
       " 'died',\n",
       " 'fearful',\n",
       " 'agonies',\n",
       " 'instance',\n",
       " 'poisoning',\n",
       " 'wells',\n",
       " 'purely',\n",
       " 'feminine',\n",
       " 'accomplishment',\n",
       " 'dreaded',\n",
       " 'dream',\n",
       " 'shrink',\n",
       " 'liked',\n",
       " 'stay',\n",
       " 'imported',\n",
       " 'reflecting',\n",
       " 'arrival',\n",
       " 'robert',\n",
       " 'bells',\n",
       " 'sensation',\n",
       " 'second',\n",
       " 'none',\n",
       " 'dearly',\n",
       " 'loved',\n",
       " 'somewhat',\n",
       " 'relief',\n",
       " 'latter',\n",
       " 'fears',\n",
       " 'reviving',\n",
       " 'under',\n",
       " 'influence',\n",
       " 'pessimism',\n",
       " 'things',\n",
       " 'ejaculated',\n",
       " 'safely',\n",
       " 'really',\n",
       " 'dreaming',\n",
       " 'sorry',\n",
       " 'poor',\n",
       " 'mistake',\n",
       " 'theyll',\n",
       " 'expect',\n",
       " 'wiser',\n",
       " 'steadier',\n",
       " 'grandfather',\n",
       " 'seems',\n",
       " 'uncanny',\n",
       " 'somehow',\n",
       " 'believe',\n",
       " 'looks',\n",
       " 'orphans',\n",
       " 'shoes',\n",
       " 'pity',\n",
       " 'fulness',\n",
       " 'jogged',\n",
       " 'comfortably',\n",
       " 'eight',\n",
       " 'miles',\n",
       " 'running',\n",
       " 'snug',\n",
       " 'farmsteads',\n",
       " 'again',\n",
       " 'bit',\n",
       " 'balsamy',\n",
       " 'fir',\n",
       " 'wood',\n",
       " 'drive',\n",
       " 'plums',\n",
       " 'hung',\n",
       " 'filmy',\n",
       " 'air',\n",
       " 'sweet',\n",
       " 'breath',\n",
       " 'many',\n",
       " 'orchards',\n",
       " 'meadows',\n",
       " 'sloped',\n",
       " 'horizon',\n",
       " 'mists',\n",
       " 'pearl',\n",
       " 'purple',\n",
       " 'while',\n",
       " 'birds',\n",
       " 'sang',\n",
       " 'summer',\n",
       " 'enjoyed',\n",
       " 'fashion',\n",
       " 'except',\n",
       " 'during',\n",
       " 'moments',\n",
       " 'met',\n",
       " 'women',\n",
       " 'nod',\n",
       " 'prince',\n",
       " 'edward',\n",
       " 'supposed',\n",
       " 'sundry',\n",
       " 'whether',\n",
       " 'uncomfortable',\n",
       " 'feeling',\n",
       " 'mysterious',\n",
       " 'secretly',\n",
       " 'laughing',\n",
       " 'personage',\n",
       " 'ungainly',\n",
       " 'figure',\n",
       " 'iron',\n",
       " 'touched',\n",
       " 'stooping',\n",
       " 'shoulders',\n",
       " 'full',\n",
       " 'soft',\n",
       " 'brown',\n",
       " 'beard',\n",
       " 'worn',\n",
       " 'twenty',\n",
       " 'lacking',\n",
       " 'grayness',\n",
       " 'sign',\n",
       " 'tied',\n",
       " 'horse',\n",
       " 'small',\n",
       " 'hotel',\n",
       " 'platform',\n",
       " 'deserted',\n",
       " 'creature',\n",
       " 'pile',\n",
       " 'shingles',\n",
       " 'extreme',\n",
       " 'noting',\n",
       " 'sidled',\n",
       " 'quickly',\n",
       " 'possible',\n",
       " 'hardly',\n",
       " 'failed',\n",
       " 'notice',\n",
       " 'tense',\n",
       " 'rigidity',\n",
       " 'expectation',\n",
       " 'expression',\n",
       " 'waiting',\n",
       " 'somebody',\n",
       " 'waited',\n",
       " 'encountered',\n",
       " 'stationmaster',\n",
       " 'locking',\n",
       " 'ticket',\n",
       " 'office',\n",
       " 'preparatory',\n",
       " 'hour',\n",
       " 'ago',\n",
       " 'answered',\n",
       " 'brisk',\n",
       " 'official',\n",
       " 'passenger',\n",
       " 'dropped',\n",
       " 'shes',\n",
       " 'room',\n",
       " 'informed',\n",
       " 'gravely',\n",
       " 'preferred',\n",
       " 'outside',\n",
       " 'should',\n",
       " 'blankly',\n",
       " 'come',\n",
       " 'whistled',\n",
       " 'charge',\n",
       " 'sister',\n",
       " 'presently',\n",
       " 'havent',\n",
       " 'concealed',\n",
       " 'hereabouts',\n",
       " 'understand',\n",
       " 'helplessly',\n",
       " 'wishing',\n",
       " 'hand',\n",
       " 'cope',\n",
       " 'situation',\n",
       " 'youd',\n",
       " 'better',\n",
       " 'question',\n",
       " 'master',\n",
       " 'carelessly',\n",
       " 'dare',\n",
       " 'shell',\n",
       " 'able',\n",
       " 'explain',\n",
       " 'tongue',\n",
       " 'certain',\n",
       " 'brand',\n",
       " 'wanted',\n",
       " 'walked',\n",
       " ...]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, [4, 5, 6]]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = [1,2,3]\n",
    "b = [4,5,6]\n",
    "a.append(b)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars 32\n",
      "Num training examples 93314\n",
      "                id                                               sent\n",
      "0  100-121669-0000                              TOM THE PIPER'S SON\\n\n",
      "1  100-121669-0001  THE PIG WAS EAT AND TOM WAS BEAT AND TOM RAN C...\n",
      "2  100-121669-0002  HE NEVER DID ANY WORK EXCEPT TO PLAY THE PIPES...\n",
      "3  100-121669-0003  BUT HE WAS SO SLY AND CAUTIOUS THAT NO ONE HAD...\n",
      "4  100-121669-0004  AND THEY LIVED ALL ALONE IN A LITTLE HUT AWAY ...\n"
     ]
    }
   ],
   "source": [
    "#train_df = make_train_df(dataset_dir)\n",
    "train_df = pd.read_csv(os.path.join(root_dir, 'train_df.csv'), names=['id', 'sent'])\n",
    "train_df = train_df.dropna(how='any')\n",
    "print(\"Num training examples\", train_df.shape[0])\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, dataset_dir, char_to_token, file_ext='.flac')\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=64, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "save_dir = os.path.join('trained_models_librispeech', f'Training_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 256  # 256*2 nodes in each LSTM\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 256\n",
    "embed_dim = 30\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size}\n",
    "\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Seq2Seq(encoder, decoder, criterion, tf_ratio = 1.0, device=DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [2496/93314 (3%)]\tMean Loss : 13.302426\t time 0:01:34.288800:\n",
      "Train Epoch: 0 [5056/93314 (5%)]\tMean Loss : 13.108905\t time 0:01:32.994594:\n",
      "Train Epoch: 0 [7616/93314 (8%)]\tMean Loss : 12.869395\t time 0:01:31.861588:\n",
      "Train Epoch: 0 [10176/93314 (11%)]\tMean Loss : 12.590679\t time 0:01:29.919430:\n",
      "Train Epoch: 0 [12736/93314 (14%)]\tMean Loss : 12.200976\t time 0:01:31.825354:\n",
      "Train Epoch: 0 [15296/93314 (16%)]\tMean Loss : 11.901207\t time 0:01:31.531493:\n",
      "Train Epoch: 0 [17856/93314 (19%)]\tMean Loss : 11.816909\t time 0:01:31.345978:\n",
      "Train Epoch: 0 [20416/93314 (22%)]\tMean Loss : 11.709612\t time 0:01:34.322289:\n",
      "Train Epoch: 0 [22976/93314 (25%)]\tMean Loss : 11.673304\t time 0:01:34.533173:\n",
      "Train Epoch: 0 [25536/93314 (27%)]\tMean Loss : 11.666648\t time 0:01:34.239619:\n",
      "Train Epoch: 0 [28096/93314 (30%)]\tMean Loss : 11.583640\t time 0:01:29.957466:\n",
      "Train Epoch: 0 [30656/93314 (33%)]\tMean Loss : 11.589910\t time 0:01:32.751857:\n",
      "Train Epoch: 0 [33216/93314 (36%)]\tMean Loss : 11.571265\t time 0:01:34.224616:\n",
      "Train Epoch: 0 [35776/93314 (38%)]\tMean Loss : 11.533404\t time 0:01:35.788843:\n",
      "Train Epoch: 0 [38336/93314 (41%)]\tMean Loss : 11.519050\t time 0:01:33.413242:\n",
      "Train Epoch: 0 [40896/93314 (44%)]\tMean Loss : 11.524197\t time 0:01:32.632655:\n",
      "Train Epoch: 0 [43456/93314 (47%)]\tMean Loss : 11.517521\t time 0:01:33.382133:\n",
      "Train Epoch: 0 [46016/93314 (49%)]\tMean Loss : 11.505325\t time 0:01:30.478219:\n",
      "Train Epoch: 0 [48576/93314 (52%)]\tMean Loss : 11.474409\t time 0:01:33.422310:\n",
      "Train Epoch: 0 [51136/93314 (55%)]\tMean Loss : 11.482925\t time 0:01:31.728081:\n",
      "Train Epoch: 0 [53696/93314 (58%)]\tMean Loss : 11.499308\t time 0:01:30.190974:\n",
      "Train Epoch: 0 [56256/93314 (60%)]\tMean Loss : 11.455844\t time 0:01:33.233759:\n",
      "Train Epoch: 0 [58816/93314 (63%)]\tMean Loss : 11.458398\t time 0:01:31.068562:\n",
      "Train Epoch: 0 [61376/93314 (66%)]\tMean Loss : 11.466714\t time 0:01:37.459809:\n",
      "Train Epoch: 0 [63936/93314 (69%)]\tMean Loss : 11.428919\t time 0:01:31.850340:\n",
      "Train Epoch: 0 [66496/93314 (71%)]\tMean Loss : 11.449272\t time 0:01:34.749928:\n",
      "Train Epoch: 0 [69056/93314 (74%)]\tMean Loss : 11.427693\t time 0:01:31.337071:\n",
      "Train Epoch: 0 [71616/93314 (77%)]\tMean Loss : 11.397500\t time 0:01:31.198874:\n",
      "Train Epoch: 0 [74176/93314 (79%)]\tMean Loss : 11.409410\t time 0:01:31.585484:\n",
      "Train Epoch: 0 [76736/93314 (82%)]\tMean Loss : 11.371109\t time 0:01:33.340252:\n",
      "Train Epoch: 0 [79296/93314 (85%)]\tMean Loss : 11.352293\t time 0:01:28.788112:\n",
      "Train Epoch: 0 [81856/93314 (88%)]\tMean Loss : 11.334124\t time 0:01:31.102542:\n",
      "Train Epoch: 0 [84416/93314 (90%)]\tMean Loss : 11.288328\t time 0:01:29.431555:\n",
      "Train Epoch: 0 [86976/93314 (93%)]\tMean Loss : 11.289681\t time 0:01:32.137628:\n",
      "Train Epoch: 0 [89536/93314 (96%)]\tMean Loss : 11.281468\t time 0:01:32.738666:\n",
      "Train Epoch: 0 [92096/93314 (99%)]\tMean Loss : 11.275699\t time 0:01:30.061966:\n",
      "\n",
      "Teacher forcing ratio: 0.95\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 1 [2496/93314 (3%)]\tMean Loss : 11.357534\t time 0:01:29.473892:\n",
      "Train Epoch: 1 [5056/93314 (5%)]\tMean Loss : 11.353375\t time 0:01:29.942977:\n",
      "Train Epoch: 1 [7616/93314 (8%)]\tMean Loss : 11.332392\t time 0:01:32.423638:\n",
      "Train Epoch: 1 [10176/93314 (11%)]\tMean Loss : 11.307464\t time 0:01:29.678755:\n",
      "Train Epoch: 1 [12736/93314 (14%)]\tMean Loss : 11.324853\t time 0:01:30.921577:\n",
      "Train Epoch: 1 [15296/93314 (16%)]\tMean Loss : 11.335502\t time 0:01:30.984003:\n",
      "Train Epoch: 1 [17856/93314 (19%)]\tMean Loss : 11.329221\t time 0:01:37.923442:\n",
      "Train Epoch: 1 [20416/93314 (22%)]\tMean Loss : 11.339958\t time 0:01:37.629984:\n",
      "Train Epoch: 1 [22976/93314 (25%)]\tMean Loss : 11.343532\t time 0:01:30.421316:\n",
      "Train Epoch: 1 [25536/93314 (27%)]\tMean Loss : 11.309568\t time 0:01:28.303261:\n",
      "Train Epoch: 1 [28096/93314 (30%)]\tMean Loss : 11.332794\t time 0:01:31.831649:\n",
      "Train Epoch: 1 [30656/93314 (33%)]\tMean Loss : 11.322247\t time 0:01:30.687385:\n",
      "Train Epoch: 1 [33216/93314 (36%)]\tMean Loss : 11.257812\t time 0:01:28.907485:\n",
      "Train Epoch: 1 [35776/93314 (38%)]\tMean Loss : 11.277310\t time 0:01:32.906095:\n",
      "Train Epoch: 1 [38336/93314 (41%)]\tMean Loss : 11.292909\t time 0:01:32.231269:\n",
      "Train Epoch: 1 [40896/93314 (44%)]\tMean Loss : 11.275184\t time 0:01:34.765719:\n",
      "Train Epoch: 1 [43456/93314 (47%)]\tMean Loss : 11.283546\t time 0:01:32.360335:\n",
      "Train Epoch: 1 [46016/93314 (49%)]\tMean Loss : 11.283582\t time 0:01:33.668045:\n",
      "Train Epoch: 1 [48576/93314 (52%)]\tMean Loss : 11.271223\t time 0:01:32.121208:\n",
      "Train Epoch: 1 [51136/93314 (55%)]\tMean Loss : 11.331959\t time 0:01:37.737049:\n",
      "Train Epoch: 1 [53696/93314 (58%)]\tMean Loss : 11.298228\t time 0:01:37.289546:\n",
      "Train Epoch: 1 [56256/93314 (60%)]\tMean Loss : 11.294876\t time 0:01:30.269874:\n",
      "Train Epoch: 1 [58816/93314 (63%)]\tMean Loss : 11.286611\t time 0:01:28.823189:\n",
      "Train Epoch: 1 [61376/93314 (66%)]\tMean Loss : 11.298708\t time 0:01:29.903616:\n",
      "Train Epoch: 1 [63936/93314 (69%)]\tMean Loss : 11.278399\t time 0:01:35.309979:\n",
      "Train Epoch: 1 [66496/93314 (71%)]\tMean Loss : 11.231960\t time 0:01:31.146907:\n",
      "Train Epoch: 1 [69056/93314 (74%)]\tMean Loss : 11.217555\t time 0:01:29.724591:\n",
      "Train Epoch: 1 [71616/93314 (77%)]\tMean Loss : 11.291869\t time 0:01:32.514462:\n",
      "Train Epoch: 1 [74176/93314 (79%)]\tMean Loss : 11.240164\t time 0:01:33.324598:\n",
      "Train Epoch: 1 [76736/93314 (82%)]\tMean Loss : 11.289653\t time 0:01:31.587909:\n",
      "Train Epoch: 1 [79296/93314 (85%)]\tMean Loss : 11.273452\t time 0:01:31.540933:\n",
      "Train Epoch: 1 [81856/93314 (88%)]\tMean Loss : 11.265511\t time 0:01:30.292139:\n",
      "Train Epoch: 1 [84416/93314 (90%)]\tMean Loss : 11.283086\t time 0:01:29.193344:\n",
      "Train Epoch: 1 [86976/93314 (93%)]\tMean Loss : 11.275536\t time 0:01:32.993564:\n",
      "Train Epoch: 1 [89536/93314 (96%)]\tMean Loss : 11.266083\t time 0:01:30.928170:\n",
      "Train Epoch: 1 [92096/93314 (99%)]\tMean Loss : 11.246560\t time 0:01:31.369704:\n",
      "\n",
      "Teacher forcing ratio: 0.8999999999999999\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 2 [2496/93314 (3%)]\tMean Loss : 11.341904\t time 0:01:31.211534:\n",
      "Train Epoch: 2 [5056/93314 (5%)]\tMean Loss : 11.352582\t time 0:01:31.985732:\n",
      "Train Epoch: 2 [7616/93314 (8%)]\tMean Loss : 11.320459\t time 0:01:30.645591:\n",
      "Train Epoch: 2 [10176/93314 (11%)]\tMean Loss : 11.326836\t time 0:01:29.696002:\n",
      "Train Epoch: 2 [12736/93314 (14%)]\tMean Loss : 11.316718\t time 0:01:29.127075:\n",
      "Train Epoch: 2 [15296/93314 (16%)]\tMean Loss : 11.335252\t time 0:01:30.943859:\n",
      "Train Epoch: 2 [17856/93314 (19%)]\tMean Loss : 11.343343\t time 0:01:30.134354:\n",
      "Train Epoch: 2 [20416/93314 (22%)]\tMean Loss : 11.318187\t time 0:01:30.039644:\n",
      "Train Epoch: 2 [22976/93314 (25%)]\tMean Loss : 11.334109\t time 0:01:31.191472:\n",
      "Train Epoch: 2 [25536/93314 (27%)]\tMean Loss : 11.355051\t time 0:01:29.492858:\n",
      "Train Epoch: 2 [28096/93314 (30%)]\tMean Loss : 11.286733\t time 0:01:30.188290:\n",
      "Train Epoch: 2 [30656/93314 (33%)]\tMean Loss : 11.310389\t time 0:01:30.606835:\n",
      "Train Epoch: 2 [33216/93314 (36%)]\tMean Loss : 11.323174\t time 0:01:31.414967:\n",
      "Train Epoch: 2 [35776/93314 (38%)]\tMean Loss : 11.313935\t time 0:01:33.116431:\n",
      "Train Epoch: 2 [38336/93314 (41%)]\tMean Loss : 11.307767\t time 0:01:31.180973:\n",
      "Train Epoch: 2 [40896/93314 (44%)]\tMean Loss : 11.276461\t time 0:01:30.242361:\n",
      "Train Epoch: 2 [43456/93314 (47%)]\tMean Loss : 11.312132\t time 0:01:36.040232:\n",
      "Train Epoch: 2 [46016/93314 (49%)]\tMean Loss : 11.288393\t time 0:01:32.159993:\n",
      "Train Epoch: 2 [48576/93314 (52%)]\tMean Loss : 11.278310\t time 0:01:32.084249:\n",
      "Train Epoch: 2 [51136/93314 (55%)]\tMean Loss : 11.289566\t time 0:01:31.579868:\n",
      "Train Epoch: 2 [53696/93314 (58%)]\tMean Loss : 11.349629\t time 0:01:48.994538:\n",
      "Train Epoch: 2 [56256/93314 (60%)]\tMean Loss : 11.324663\t time 0:01:47.056127:\n",
      "Train Epoch: 2 [58816/93314 (63%)]\tMean Loss : 11.304136\t time 0:01:47.239710:\n",
      "Train Epoch: 2 [61376/93314 (66%)]\tMean Loss : 11.321260\t time 0:01:51.268093:\n",
      "Train Epoch: 2 [63936/93314 (69%)]\tMean Loss : 11.274060\t time 0:01:46.590745:\n",
      "Train Epoch: 2 [66496/93314 (71%)]\tMean Loss : 11.304659\t time 0:01:44.503048:\n",
      "Train Epoch: 2 [69056/93314 (74%)]\tMean Loss : 11.323256\t time 0:01:45.236369:\n",
      "Train Epoch: 2 [71616/93314 (77%)]\tMean Loss : 11.307817\t time 0:01:49.725338:\n",
      "Train Epoch: 2 [74176/93314 (79%)]\tMean Loss : 11.277178\t time 0:01:43.952414:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [76736/93314 (82%)]\tMean Loss : 11.271579\t time 0:01:36.648754:\n",
      "Train Epoch: 2 [79296/93314 (85%)]\tMean Loss : 11.251705\t time 0:01:34.746429:\n",
      "Train Epoch: 2 [81856/93314 (88%)]\tMean Loss : 11.287656\t time 0:01:30.358439:\n",
      "Train Epoch: 2 [84416/93314 (90%)]\tMean Loss : 11.274141\t time 0:01:37.461059:\n",
      "Train Epoch: 2 [86976/93314 (93%)]\tMean Loss : 11.246233\t time 0:01:34.373304:\n",
      "Train Epoch: 2 [89536/93314 (96%)]\tMean Loss : 11.256752\t time 0:01:27.929825:\n",
      "Train Epoch: 2 [92096/93314 (99%)]\tMean Loss : 11.254165\t time 0:01:28.820860:\n",
      "\n",
      "Teacher forcing ratio: 0.8499999999999999\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 3 [2496/93314 (3%)]\tMean Loss : 11.359455\t time 0:01:30.132443:\n",
      "Train Epoch: 3 [5056/93314 (5%)]\tMean Loss : 11.314065\t time 0:01:32.834080:\n",
      "Train Epoch: 3 [7616/93314 (8%)]\tMean Loss : 11.327893\t time 0:01:32.992677:\n",
      "Train Epoch: 3 [10176/93314 (11%)]\tMean Loss : 11.333943\t time 0:01:32.387827:\n",
      "Train Epoch: 3 [12736/93314 (14%)]\tMean Loss : 11.328419\t time 0:01:32.183909:\n",
      "Train Epoch: 3 [15296/93314 (16%)]\tMean Loss : 11.363199\t time 0:01:32.540868:\n",
      "Train Epoch: 3 [17856/93314 (19%)]\tMean Loss : 11.319014\t time 0:01:32.353778:\n",
      "Train Epoch: 3 [20416/93314 (22%)]\tMean Loss : 11.294634\t time 0:01:32.325411:\n",
      "Train Epoch: 3 [22976/93314 (25%)]\tMean Loss : 11.301733\t time 0:01:32.780958:\n",
      "Train Epoch: 3 [25536/93314 (27%)]\tMean Loss : 11.330696\t time 0:01:32.765930:\n",
      "Train Epoch: 3 [28096/93314 (30%)]\tMean Loss : 11.279345\t time 0:01:32.159275:\n",
      "Train Epoch: 3 [30656/93314 (33%)]\tMean Loss : 11.303449\t time 0:01:32.634746:\n",
      "Train Epoch: 3 [33216/93314 (36%)]\tMean Loss : 11.312614\t time 0:01:32.538453:\n",
      "Train Epoch: 3 [35776/93314 (38%)]\tMean Loss : 11.291545\t time 0:01:32.270112:\n",
      "Train Epoch: 3 [38336/93314 (41%)]\tMean Loss : 11.288049\t time 0:01:33.380498:\n",
      "Train Epoch: 3 [40896/93314 (44%)]\tMean Loss : 11.306439\t time 0:01:33.143518:\n",
      "Train Epoch: 3 [43456/93314 (47%)]\tMean Loss : 11.321821\t time 0:01:34.490607:\n",
      "Train Epoch: 3 [46016/93314 (49%)]\tMean Loss : 11.318312\t time 0:01:34.414526:\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.ASGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "optimizer = optim.Adadelta(model.parameters(), )\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    scheduler.step()                                    # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.05, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True sent :  <sos>who was now approaching womanhood he would sometimes talk with her differently from the manner in which he would speak to a mere girl but on her part she seemed not to notice the difference and for their daily amusement either go<eos>\n",
      "\n",
      "Pred sent :   uholshs sornsnpropsh ng shmpndsun sorshuld shmp hnpl shll shlh hor shgfordds f soom shv sord r sn sholl sorshmld shrrssh nsnd r dsorlpsurosf sor srrshsho shnnud sor shvsor sorshvososfordd krsnd sor shv r sonnh snosh ond sngh<eos>r soo\n",
      "Loss : 722.2744140625\n",
      "\n",
      "\n",
      "True sent :  <sos>now sworn to the service of his most christian majesty<eos>\n",
      "\n",
      "Pred sent :   uor shordssh shj shnoongdshosos sors soooshgnn soruss  \n",
      "Loss : 178.7652587890625\n",
      "\n",
      "\n",
      "True sent :  <sos>and a paper cap on his head has the strong conscience and the strong sense the blended susceptibility and self command of our friend adam he was not an average man yet such men as he are reared here and there in every generation of our peasant artisans<eos>\n",
      "\n",
      "Pred sent :   und shsroprssorpsf sos sorrdshmpshv shoong sonsoordsossnd shoushoong shndonshv suoss soshrposp nlffnu snd shndosonsond sf sfr soognd snondsor orhsor sndsnor lonson souhshrhoupd shosousno soslsd sor  snd sho r sn sndso sonfrslunn sf sf shrrrsnd snohnhnd \n",
      "Loss : 809.318603515625\n",
      "\n",
      "\n",
      "True sent :  <sos>uncommon patience in planning a revenge that is worth while m is for moses who slew the egyptian as sweet as a rose is the meekness of moses no monument shows his post mortem inscription<eos>\n",
      "\n",
      "Pred sent :   undopponssrrhor hssn sroogor ssnsouor lrshvn sn shoooou hnfhsosn sor sors  sholshossshvosnpordnn sn shordhsndsnsous sn shv sonoross shosors  sorsorsspn  shoulosos srrsosornhrpsn ooosprnn \n",
      "Loss : 608.6458129882812\n",
      "\n",
      "\n",
      "True sent :  <sos>i'm not a coward as a general rule went on the promoter but i always said that if i ever met the sucker that bought that lot i'd run like a turkey now you see that old babe in the wood over there well he's the boy that drew the prize<eos>\n",
      "\n",
      "Pred sent :   ursosor snsonsrd sn sndorddsllsosh shooosf sho sroppnhd sur hnshoor  sholsshvn sn snsnduosonhshv shrhrdsshodlulush  shvn sonhsns sosgsogfosnshrn d sornsour  lfshvn shooshmlrsn shv shud sho rshv ousholosors shv suu shvn soossshv sroppd\n",
      "Loss : 762.9634399414062\n",
      "\n",
      "\n",
      "True sent :  <sos>if we discover that they have the least desire to get the better of us<eos>\n",
      "\n",
      "Pred sent :   uroprrsossooorsshvn shv  sodf shv sosrs sossrsdsh son shv surhor sf sn \n",
      "Loss : 230.7517547607422\n",
      "\n",
      "\n",
      "True sent :  <sos>the promise long with the fulfilment short will make thee triumph in thy lofty seat francis came afterward when i was dead for me but one of the black cherubim said to him take him not do me no wrong<eos>\n",
      "\n",
      "Pred sent :   uhp sroppnhdsong shlh hoolsollorlord shous shll sons shv orh<eos><sos>rplosn shv song  shndrsoonghnhsorp snfor ord shon shshs sosrssor sonsur hf  sf shv suonk shousslnpshmd sh sos shlfosos sor shmsorsorsholdh\n",
      "Loss : 639.7150268554688\n",
      "\n",
      "\n",
      "True sent :  <sos>it was plain to see that mister sandford esteemed her less and less every day and as he was the person who most influenced the opinion of her guardian he became to her very soon an object not merely of dislike but of abhorrence<eos>\n",
      "\n",
      "Pred sent :   ud shs sronn sh sho shvn sors r shmd urd snshrsod sor sonsosnd soss snors son snd sndsorshs shv srrsonssholsors sn oosddk  shvouklngnn sf sor sorrd nn sorsurolpdsh sor sors shmn sndsflurkusor sonosf sf shghonf sur hf snlousoddks\n",
      "Loss : 720.8135986328125\n",
      "\n",
      "\n",
      "True sent :  <sos>there came a letter addressed to doctor livesey with this addition to be opened in the case of his absence by tom redruth or young hawkins<eos>\n",
      "\n",
      "Pred sent :   uhppprsorp snsoss r snoooss soshrsonkor sogo  l shgo hhvs snoonunn sh sursfpn d sn shv sors sf sos snlosdh su sh ssosuosh hf sourd sodn ng \n",
      "Loss : 451.2442626953125\n",
      "\n",
      "\n",
      "True sent :  <sos>what is he that every cheek turns pale at the mention of his name asked capitola black donald oh my child may you never know more of black donald than i can tell you black donald is the chief of a band of ruthless desperadoes that infest these mountain roads<eos>\n",
      "\n",
      "Pred sent :   uhor sn soushjn snors soorr shrn  srrsdsn shv sonoonn sf shghsomplsn od sorroorlgsuookosongll sfrsorsoosloshm soursosor snoo sorn sf shork songoo shvn snsonrshllosoursuonk songll sn shv sooososf shsurd sf sosh f shsosslrslons shvn sn ors shv orsornd sn sour  \n",
      "Loss : 821.7781372070312\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "model.device = DEVICE\n",
    "model.tf_ratio = 0.9\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, eos_token, file_extension='.flac')\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
