{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars(lang, train_df=None):\n",
    "    if lang=='eng':\n",
    "        chars = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \\\n",
    "                'y', 'z', ' ', \"'\", '<eos>', '<pad>']\n",
    "    elif lang=='chinese':\n",
    "        chars = [' ', '<sos>']\n",
    "        for idx in range(train_df.shape[0]):\n",
    "            _, sent = train_df.iloc[idx]\n",
    "            for c in sent:\n",
    "                if c not in chars:\n",
    "                    chars.append(c)\n",
    "        chars = chars + ['<eos>', '<pad>', '<unk>']        \n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(dataset_dir):\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text:\n",
    "                data.append((f.replace('.txt', ''), text.readline()))\n",
    "                \n",
    "    train_df = pd.DataFrame(data, columns=['id', 'sent'])\n",
    "    train_df.to_csv(os.path.join(dataset_dir, 'train_df.csv'), header=None)\n",
    "    print(train_df.head())\n",
    "\n",
    "\n",
    "# Used for ai_shell dataset, when all sentences are in a single text file\n",
    "def read_transcrpt(transcript_dir):\n",
    "    transcript_dir = '../../../Dataset'\n",
    "    with open(os.path.join(transcript_dir, 'aishell_transcript_v0.8.txt')) as f:\n",
    "        data_list = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    for example in data_list:\n",
    "        id_, sent = str(example.split(' ')[0]), str(' '.join(example.split(' ')[1:-1])) # -1 to remove '\\n'\n",
    "        data.append((id_, sent))\n",
    "\n",
    "    print('Num examples:', len(data))\n",
    "    data_df = pd.DataFrame(data, columns=['id', 'sent'])\n",
    "    data_df.to_csv(os.path.join(transcript_dir, 'train_df.csv'))\n",
    "    data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "DEVICE : cuda\n                 id                        sent\n0  BAC009S0002W0122     而 对 楼市 成交 抑制 作用 最 大 的 限\n1  BAC009S0002W0123             也 成为 地方 政府 的 眼中\n2  BAC009S0002W0124  自 六月 底 呼和浩特 市 率先 宣布 取消 限 购\n3  BAC009S0002W0125                  各地 政府 便 纷纷\n4  BAC009S0002W0126              仅 一 个 多 月 的 时间\nNumber of chars 4256\n\nTeacher forcing ratio: 1.0\nTraining, Logging: Mean loss of previous 40 batches \n\nTrain Epoch: 7 [1248/141569 (1%)]\tMean Loss : 12.375593\t time 0:00:28.956483:\nTrain Epoch: 7 [2528/141569 (2%)]\tMean Loss : 11.804982\t time 0:00:27.732719:\nTrain Epoch: 7 [3808/141569 (3%)]\tMean Loss : 12.199051\t time 0:00:28.012606:\nTrain Epoch: 7 [5088/141569 (4%)]\tMean Loss : 11.812438\t time 0:00:27.619491:\nTrain Epoch: 7 [6368/141569 (4%)]\tMean Loss : 12.224764\t time 0:00:27.261424:\nTrain Epoch: 7 [7648/141569 (5%)]\tMean Loss : 12.113549\t time 0:00:27.783398:\nTrain Epoch: 7 [8928/141569 (6%)]\tMean Loss : 11.791881\t time 0:00:26.269549:\nTrain Epoch: 7 [10208/141569 (7%)]\tMean Loss : 12.051673\t time 0:00:27.042105:\nTrain Epoch: 7 [11488/141569 (8%)]\tMean Loss : 12.364949\t time 0:00:32.214353:\nTrain Epoch: 7 [12768/141569 (9%)]\tMean Loss : 11.912260\t time 0:00:27.361234:\nTrain Epoch: 7 [14048/141569 (10%)]\tMean Loss : 11.930967\t time 0:00:27.361066:\nTrain Epoch: 7 [15328/141569 (11%)]\tMean Loss : 12.325504\t time 0:00:28.270756:\nTrain Epoch: 7 [16608/141569 (12%)]\tMean Loss : 11.633907\t time 0:00:27.069107:\nTrain Epoch: 7 [17888/141569 (13%)]\tMean Loss : 12.127954\t time 0:00:27.906466:\nTrain Epoch: 7 [19168/141569 (14%)]\tMean Loss : 12.390232\t time 0:00:28.197318:\nTrain Epoch: 7 [20448/141569 (14%)]\tMean Loss : 12.323614\t time 0:00:28.039920:\nTrain Epoch: 7 [21728/141569 (15%)]\tMean Loss : 11.951475\t time 0:00:27.261323:\nTrain Epoch: 7 [23008/141569 (16%)]\tMean Loss : 11.914257\t time 0:00:26.409531:\nTrain Epoch: 7 [24288/141569 (17%)]\tMean Loss : 12.500919\t time 0:00:27.712978:\nTrain Epoch: 7 [25568/141569 (18%)]\tMean Loss : 12.262611\t time 0:00:28.333909:\nTrain Epoch: 7 [26848/141569 (19%)]\tMean Loss : 12.048968\t time 0:00:28.127332:\nTrain Epoch: 7 [28128/141569 (20%)]\tMean Loss : 12.013552\t time 0:00:27.916747:\nTrain Epoch: 7 [29408/141569 (21%)]\tMean Loss : 11.579693\t time 0:00:27.781022:\nTrain Epoch: 7 [30688/141569 (22%)]\tMean Loss : 11.575000\t time 0:00:27.781453:\nTrain Epoch: 7 [31968/141569 (23%)]\tMean Loss : 11.497771\t time 0:00:27.650289:\nTrain Epoch: 7 [33248/141569 (23%)]\tMean Loss : 12.500650\t time 0:00:29.089161:\nTrain Epoch: 7 [34528/141569 (24%)]\tMean Loss : 12.270541\t time 0:00:28.597681:\nTrain Epoch: 7 [35808/141569 (25%)]\tMean Loss : 12.289263\t time 0:00:27.638307:\nTrain Epoch: 7 [37088/141569 (26%)]\tMean Loss : 11.607164\t time 0:00:25.878629:\nTrain Epoch: 7 [38368/141569 (27%)]\tMean Loss : 11.935151\t time 0:00:28.202966:\nTrain Epoch: 7 [39648/141569 (28%)]\tMean Loss : 12.362404\t time 0:00:28.104805:\nTrain Epoch: 7 [40928/141569 (29%)]\tMean Loss : 11.547519\t time 0:00:27.646923:\nTrain Epoch: 7 [42208/141569 (30%)]\tMean Loss : 12.108320\t time 0:00:28.200987:\nTrain Epoch: 7 [43488/141569 (31%)]\tMean Loss : 11.923584\t time 0:00:28.039326:\nTrain Epoch: 7 [44768/141569 (32%)]\tMean Loss : 11.976344\t time 0:00:28.002850:\nTrain Epoch: 7 [46048/141569 (33%)]\tMean Loss : 11.910585\t time 0:00:27.875333:\nTrain Epoch: 7 [47328/141569 (33%)]\tMean Loss : 12.211370\t time 0:00:27.841202:\nTrain Epoch: 7 [48608/141569 (34%)]\tMean Loss : 12.328981\t time 0:00:28.198992:\nTrain Epoch: 7 [49888/141569 (35%)]\tMean Loss : 11.394839\t time 0:00:27.386106:\nTrain Epoch: 7 [51168/141569 (36%)]\tMean Loss : 12.313731\t time 0:00:26.732647:\nTrain Epoch: 7 [52448/141569 (37%)]\tMean Loss : 11.900717\t time 0:00:27.813665:\nTrain Epoch: 7 [53728/141569 (38%)]\tMean Loss : 11.842873\t time 0:00:27.755810:\nTrain Epoch: 7 [55008/141569 (39%)]\tMean Loss : 12.099159\t time 0:00:27.960184:\nTrain Epoch: 7 [56288/141569 (40%)]\tMean Loss : 11.772252\t time 0:00:18.530118:\nTrain Epoch: 7 [57568/141569 (41%)]\tMean Loss : 11.733417\t time 0:00:18.134217:\nTrain Epoch: 7 [58848/141569 (42%)]\tMean Loss : 12.074725\t time 0:00:18.102540:\nTrain Epoch: 7 [60128/141569 (42%)]\tMean Loss : 11.863909\t time 0:00:18.200031:\nTrain Epoch: 7 [61408/141569 (43%)]\tMean Loss : 11.525091\t time 0:00:18.029879:\nTrain Epoch: 7 [62688/141569 (44%)]\tMean Loss : 12.310992\t time 0:00:18.485886:\nTrain Epoch: 7 [63968/141569 (45%)]\tMean Loss : 12.203831\t time 0:00:18.587990:\nTrain Epoch: 7 [65248/141569 (46%)]\tMean Loss : 11.565517\t time 0:00:18.065043:\nTrain Epoch: 7 [66528/141569 (47%)]\tMean Loss : 12.241230\t time 0:00:18.118860:\nTrain Epoch: 7 [67808/141569 (48%)]\tMean Loss : 11.542440\t time 0:00:18.081816:\nTrain Epoch: 7 [69088/141569 (49%)]\tMean Loss : 11.621910\t time 0:00:17.544568:\nTrain Epoch: 7 [70368/141569 (50%)]\tMean Loss : 11.868006\t time 0:00:18.041429:\nTrain Epoch: 7 [71648/141569 (51%)]\tMean Loss : 12.138487\t time 0:00:18.410715:\nTrain Epoch: 7 [72928/141569 (52%)]\tMean Loss : 12.018884\t time 0:00:17.925777:\nTrain Epoch: 7 [74208/141569 (52%)]\tMean Loss : 11.749155\t time 0:00:24.007120:\nTrain Epoch: 7 [75488/141569 (53%)]\tMean Loss : 12.621650\t time 0:00:28.643108:\nTrain Epoch: 7 [76768/141569 (54%)]\tMean Loss : 12.220657\t time 0:00:27.948878:\nTrain Epoch: 7 [78048/141569 (55%)]\tMean Loss : 12.506329\t time 0:00:27.123899:\nTrain Epoch: 7 [79328/141569 (56%)]\tMean Loss : 12.456841\t time 0:00:27.806242:\nTrain Epoch: 7 [80608/141569 (57%)]\tMean Loss : 11.773575\t time 0:00:27.582016:\nTrain Epoch: 7 [81888/141569 (58%)]\tMean Loss : 11.904975\t time 0:00:27.685032:\nTrain Epoch: 7 [83168/141569 (59%)]\tMean Loss : 12.104438\t time 0:00:27.884951:\nTrain Epoch: 7 [84448/141569 (60%)]\tMean Loss : 11.748764\t time 0:00:27.962090:\nTrain Epoch: 7 [85728/141569 (61%)]\tMean Loss : 11.677566\t time 0:00:27.627733:\nTrain Epoch: 7 [87008/141569 (61%)]\tMean Loss : 12.396995\t time 0:00:28.421244:\nTrain Epoch: 7 [88288/141569 (62%)]\tMean Loss : 12.030100\t time 0:00:27.724435:\nTrain Epoch: 7 [89568/141569 (63%)]\tMean Loss : 11.892821\t time 0:00:27.854457:\nTrain Epoch: 7 [90848/141569 (64%)]\tMean Loss : 11.911263\t time 0:00:27.505627:\nTrain Epoch: 7 [92128/141569 (65%)]\tMean Loss : 12.201623\t time 0:00:27.288064:\nTrain Epoch: 7 [93408/141569 (66%)]\tMean Loss : 11.837136\t time 0:00:27.953797:\nTrain Epoch: 7 [94688/141569 (67%)]\tMean Loss : 11.705521\t time 0:00:27.871617:\nTrain Epoch: 7 [95968/141569 (68%)]\tMean Loss : 12.015105\t time 0:00:26.134321:\nTrain Epoch: 7 [97248/141569 (69%)]\tMean Loss : 12.179421\t time 0:00:28.134771:\nTrain Epoch: 7 [98528/141569 (70%)]\tMean Loss : 11.960655\t time 0:00:27.841563:\nTrain Epoch: 7 [99808/141569 (71%)]\tMean Loss : 12.399173\t time 0:00:28.529843:\nTrain Epoch: 7 [101088/141569 (71%)]\tMean Loss : 11.731733\t time 0:00:27.166917:\nTrain Epoch: 7 [102368/141569 (72%)]\tMean Loss : 12.352155\t time 0:00:28.401905:\nTrain Epoch: 7 [103648/141569 (73%)]\tMean Loss : 12.119927\t time 0:00:27.969006:\nTrain Epoch: 7 [104928/141569 (74%)]\tMean Loss : 11.898666\t time 0:00:27.627772:\nTrain Epoch: 7 [106208/141569 (75%)]\tMean Loss : 11.931084\t time 0:00:25.924895:\nTrain Epoch: 7 [107488/141569 (76%)]\tMean Loss : 12.325723\t time 0:00:18.375543:\nTrain Epoch: 7 [108768/141569 (77%)]\tMean Loss : 12.567756\t time 0:00:17.998145:\nTrain Epoch: 7 [110048/141569 (78%)]\tMean Loss : 11.846433\t time 0:00:18.150578:\nTrain Epoch: 7 [111328/141569 (79%)]\tMean Loss : 11.891834\t time 0:00:18.248843:\nTrain Epoch: 7 [112608/141569 (80%)]\tMean Loss : 11.603238\t time 0:00:17.641892:\nTrain Epoch: 7 [113888/141569 (80%)]\tMean Loss : 11.719403\t time 0:00:18.261549:\nTrain Epoch: 7 [115168/141569 (81%)]\tMean Loss : 12.546028\t time 0:00:18.931642:\nTrain Epoch: 7 [116448/141569 (82%)]\tMean Loss : 12.146881\t time 0:00:18.226867:\nTrain Epoch: 7 [117728/141569 (83%)]\tMean Loss : 11.763555\t time 0:00:18.285272:\nTrain Epoch: 7 [119008/141569 (84%)]\tMean Loss : 11.490115\t time 0:00:17.920707:\nTrain Epoch: 7 [120288/141569 (85%)]\tMean Loss : 11.803624\t time 0:00:17.938747:\nTrain Epoch: 7 [121568/141569 (86%)]\tMean Loss : 11.838152\t time 0:00:18.098148:\nTrain Epoch: 7 [122848/141569 (87%)]\tMean Loss : 11.310857\t time 0:00:17.617371:\nTrain Epoch: 7 [124128/141569 (88%)]\tMean Loss : 12.239921\t time 0:00:18.284877:\nTrain Epoch: 7 [125408/141569 (89%)]\tMean Loss : 12.239974\t time 0:00:18.533810:\nTrain Epoch: 7 [126688/141569 (89%)]\tMean Loss : 11.780211\t time 0:00:17.860123:\nTrain Epoch: 7 [127968/141569 (90%)]\tMean Loss : 12.199988\t time 0:00:17.725899:\nTrain Epoch: 7 [129248/141569 (91%)]\tMean Loss : 12.377195\t time 0:00:18.850760:\nTrain Epoch: 7 [130528/141569 (92%)]\tMean Loss : 11.954698\t time 0:00:18.353951:\nTrain Epoch: 7 [131808/141569 (93%)]\tMean Loss : 12.182082\t time 0:00:18.356373:\nTrain Epoch: 7 [133088/141569 (94%)]\tMean Loss : 12.392532\t time 0:00:18.583686:\nTrain Epoch: 7 [134368/141569 (95%)]\tMean Loss : 11.909765\t time 0:00:18.446553:\nTrain Epoch: 7 [135648/141569 (96%)]\tMean Loss : 11.438917\t time 0:00:17.526925:\nTrain Epoch: 7 [136928/141569 (97%)]\tMean Loss : 11.876385\t time 0:00:18.033163:\nTrain Epoch: 7 [138208/141569 (98%)]\tMean Loss : 12.287676\t time 0:00:18.386221:\nTrain Epoch: 7 [139488/141569 (99%)]\tMean Loss : 11.517972\t time 0:00:17.902118:\nTrain Epoch: 7 [140768/141569 (99%)]\tMean Loss : 11.884203\t time 0:00:17.994898:\n\nTeacher forcing ratio: 0.99\nTraining, Logging: Mean loss of previous 40 batches \n\nTrain Epoch: 8 [1248/141569 (1%)]\tMean Loss : 12.195900\t time 0:00:18.347891:\nTrain Epoch: 8 [2528/141569 (2%)]\tMean Loss : 11.973346\t time 0:00:18.351897:\nTrain Epoch: 8 [3808/141569 (3%)]\tMean Loss : 12.495635\t time 0:00:18.340806:\nTrain Epoch: 8 [5088/141569 (4%)]\tMean Loss : 12.462905\t time 0:00:18.090698:\nTrain Epoch: 8 [6368/141569 (4%)]\tMean Loss : 11.768379\t time 0:00:18.161872:\nTrain Epoch: 8 [7648/141569 (5%)]\tMean Loss : 12.285689\t time 0:00:18.522148:\nTrain Epoch: 8 [8928/141569 (6%)]\tMean Loss : 12.072030\t time 0:00:18.038386:\nTrain Epoch: 8 [10208/141569 (7%)]\tMean Loss : 12.067304\t time 0:00:18.416582:\nTrain Epoch: 8 [11488/141569 (8%)]\tMean Loss : 12.039412\t time 0:00:18.130286:\nTrain Epoch: 8 [12768/141569 (9%)]\tMean Loss : 12.256752\t time 0:00:18.303679:\nTrain Epoch: 8 [14048/141569 (10%)]\tMean Loss : 11.981803\t time 0:00:17.855811:\nTrain Epoch: 8 [15328/141569 (11%)]\tMean Loss : 11.607125\t time 0:00:18.144485:\nTrain Epoch: 8 [16608/141569 (12%)]\tMean Loss : 11.658137\t time 0:00:17.827007:\nTrain Epoch: 8 [17888/141569 (13%)]\tMean Loss : 12.174059\t time 0:00:17.787765:\nTrain Epoch: 8 [19168/141569 (14%)]\tMean Loss : 12.297061\t time 0:00:18.239465:\nTrain Epoch: 8 [20448/141569 (14%)]\tMean Loss : 12.089282\t time 0:00:18.288930:\nTrain Epoch: 8 [21728/141569 (15%)]\tMean Loss : 12.056800\t time 0:00:17.875412:\nTrain Epoch: 8 [23008/141569 (16%)]\tMean Loss : 11.794547\t time 0:00:18.044110:\nTrain Epoch: 8 [24288/141569 (17%)]\tMean Loss : 12.301831\t time 0:00:18.358163:\nTrain Epoch: 8 [25568/141569 (18%)]\tMean Loss : 12.196067\t time 0:00:18.261105:\nTrain Epoch: 8 [26848/141569 (19%)]\tMean Loss : 12.161315\t time 0:00:18.106364:\nTrain Epoch: 8 [28128/141569 (20%)]\tMean Loss : 12.440331\t time 0:00:18.201778:\nTrain Epoch: 8 [29408/141569 (21%)]\tMean Loss : 11.946597\t time 0:00:18.078487:\nTrain Epoch: 8 [30688/141569 (22%)]\tMean Loss : 12.752489\t time 0:00:18.604342:\nTrain Epoch: 8 [31968/141569 (23%)]\tMean Loss : 12.611664\t time 0:00:18.754602:\nTrain Epoch: 8 [33248/141569 (23%)]\tMean Loss : 12.145411\t time 0:00:18.413061:\nTrain Epoch: 8 [34528/141569 (24%)]\tMean Loss : 12.077523\t time 0:00:18.500349:\nTrain Epoch: 8 [35808/141569 (25%)]\tMean Loss : 11.183685\t time 0:00:17.874795:\nTrain Epoch: 8 [37088/141569 (26%)]\tMean Loss : 11.533182\t time 0:00:18.029539:\nTrain Epoch: 8 [38368/141569 (27%)]\tMean Loss : 11.777530\t time 0:00:18.000455:\nTrain Epoch: 8 [39648/141569 (28%)]\tMean Loss : 11.391215\t time 0:00:17.940547:\nTrain Epoch: 8 [40928/141569 (29%)]\tMean Loss : 12.044026\t time 0:00:18.439785:\nTrain Epoch: 8 [42208/141569 (30%)]\tMean Loss : 12.039334\t time 0:00:18.353932:\nTrain Epoch: 8 [43488/141569 (31%)]\tMean Loss : 12.154479\t time 0:00:18.304827:\nTrain Epoch: 8 [44768/141569 (32%)]\tMean Loss : 12.190993\t time 0:00:17.822163:\nTrain Epoch: 8 [46048/141569 (33%)]\tMean Loss : 11.861145\t time 0:00:17.948691:\nTrain Epoch: 8 [47328/141569 (33%)]\tMean Loss : 11.856345\t time 0:00:17.942666:\nTrain Epoch: 8 [48608/141569 (34%)]\tMean Loss : 11.789122\t time 0:00:18.249166:\nTrain Epoch: 8 [49888/141569 (35%)]\tMean Loss : 12.068201\t time 0:00:18.284009:\nTrain Epoch: 8 [51168/141569 (36%)]\tMean Loss : 12.091747\t time 0:00:18.446087:\nTrain Epoch: 8 [52448/141569 (37%)]\tMean Loss : 11.780372\t time 0:00:18.189664:\nTrain Epoch: 8 [53728/141569 (38%)]\tMean Loss : 11.753208\t time 0:00:18.120337:\nTrain Epoch: 8 [55008/141569 (39%)]\tMean Loss : 12.153981\t time 0:00:17.928316:\nTrain Epoch: 8 [56288/141569 (40%)]\tMean Loss : 12.056702\t time 0:00:18.024725:\nTrain Epoch: 8 [57568/141569 (41%)]\tMean Loss : 12.268198\t time 0:00:17.994105:\nTrain Epoch: 8 [58848/141569 (42%)]\tMean Loss : 11.958067\t time 0:00:17.978736:\nTrain Epoch: 8 [60128/141569 (42%)]\tMean Loss : 12.399647\t time 0:00:18.598693:\nTrain Epoch: 8 [61408/141569 (43%)]\tMean Loss : 11.922973\t time 0:00:18.598832:\nTrain Epoch: 8 [62688/141569 (44%)]\tMean Loss : 12.408665\t time 0:00:28.914081:\nTrain Epoch: 8 [63968/141569 (45%)]\tMean Loss : 12.032517\t time 0:00:27.036936:\nTrain Epoch: 8 [65248/141569 (46%)]\tMean Loss : 12.463418\t time 0:00:33.893623:\nTrain Epoch: 8 [66528/141569 (47%)]\tMean Loss : 12.344493\t time 0:00:29.508158:\nTrain Epoch: 8 [67808/141569 (48%)]\tMean Loss : 11.885965\t time 0:00:29.623097:\nTrain Epoch: 8 [69088/141569 (49%)]\tMean Loss : 12.210725\t time 0:00:28.142616:\nTrain Epoch: 8 [70368/141569 (50%)]\tMean Loss : 11.898641\t time 0:00:31.692878:\nTrain Epoch: 8 [71648/141569 (51%)]\tMean Loss : 12.097851\t time 0:00:32.737116:\nTrain Epoch: 8 [72928/141569 (52%)]\tMean Loss : 12.237157\t time 0:00:32.029499:\nTrain Epoch: 8 [74208/141569 (52%)]\tMean Loss : 11.637595\t time 0:00:31.300299:\nTrain Epoch: 8 [75488/141569 (53%)]\tMean Loss : 12.208562\t time 0:00:31.594121:\nTrain Epoch: 8 [76768/141569 (54%)]\tMean Loss : 11.999010\t time 0:00:30.089157:\nTrain Epoch: 8 [78048/141569 (55%)]\tMean Loss : 12.010759\t time 0:00:31.814639:\nTrain Epoch: 8 [79328/141569 (56%)]\tMean Loss : 12.021657\t time 0:00:31.911264:\nTrain Epoch: 8 [80608/141569 (57%)]\tMean Loss : 11.813756\t time 0:00:32.093537:\nTrain Epoch: 8 [81888/141569 (58%)]\tMean Loss : 12.320436\t time 0:00:33.184493:\nTrain Epoch: 8 [83168/141569 (59%)]\tMean Loss : 11.499475\t time 0:00:31.578405:\nTrain Epoch: 8 [84448/141569 (60%)]\tMean Loss : 11.867327\t time 0:00:32.335941:\nTrain Epoch: 8 [85728/141569 (61%)]\tMean Loss : 12.269311\t time 0:00:32.062162:\nTrain Epoch: 8 [87008/141569 (61%)]\tMean Loss : 11.760845\t time 0:00:33.088233:\nTrain Epoch: 8 [88288/141569 (62%)]\tMean Loss : 11.880692\t time 0:00:31.969180:\nTrain Epoch: 8 [89568/141569 (63%)]\tMean Loss : 11.978414\t time 0:00:31.917440:\nTrain Epoch: 8 [90848/141569 (64%)]\tMean Loss : 12.254545\t time 0:00:33.120064:\nTrain Epoch: 8 [92128/141569 (65%)]\tMean Loss : 11.989015\t time 0:00:32.957715:\nTrain Epoch: 8 [93408/141569 (66%)]\tMean Loss : 12.269584\t time 0:00:29.377804:\nTrain Epoch: 8 [94688/141569 (67%)]\tMean Loss : 11.683221\t time 0:00:29.542546:\nTrain Epoch: 8 [95968/141569 (68%)]\tMean Loss : 12.197194\t time 0:00:31.079773:\nTrain Epoch: 8 [97248/141569 (69%)]\tMean Loss : 11.522440\t time 0:00:30.883711:\nTrain Epoch: 8 [98528/141569 (70%)]\tMean Loss : 11.539501\t time 0:00:30.438700:\nTrain Epoch: 8 [99808/141569 (71%)]\tMean Loss : 11.693318\t time 0:00:29.075560:\n"
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    dataset_dir = '../../../Dataset/data_aishell'\n",
    "    DEVICE = torch.device('cuda') if torch.cuda.is_available() else 'cpu'\n",
    "    print('DEVICE :', DEVICE)\n",
    "    \n",
    "  \n",
    "    train_df = pd.read_csv(os.path.join(dataset_dir, 'train_df.csv'), names=['id', 'sent'])\n",
    "    train_df = train_df.dropna(how='any')\n",
    "    print(train_df.head())\n",
    "    # test_df = pd.read_csv('test_df.csv', names=['id', 'sent'])\n",
    "    \n",
    "    chars = get_chars('chinese', train_df)\n",
    "    char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "    token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "    sos_token = char_to_token['<sos>']\n",
    "    eos_token = char_to_token['<eos>']\n",
    "    pad_token = char_to_token['<pad>']\n",
    "   \n",
    "    tensorboard_dir = os.path.join('tb_summary')\n",
    "    train_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, eos_token)\n",
    "    train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "    # #test_dataset = SpeechDataset(test_df, dataset_dir)\n",
    "    # #test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "    input_size = 128    # num rows in instagram\n",
    "    hidden_dim = 64    # 256*2 nodes in each LSTM\n",
    "    num_layers = 3\n",
    "    dropout = 0.1\n",
    "    layer_norm = False   \n",
    "    encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "    hid_sz = 64\n",
    "    embed_dim = 15\n",
    "    vocab_size = len(chars)\n",
    "    decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "    hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, 'num_layers':num_layers\n",
    "                    'dropout':dropout, 'layer_norm':layer_norm, 'hid_sz':hid_sz, 'embed_dim':embed_dim}\n",
    "                        \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = Seq2Seq(encoder, decoder, criterion, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "    \n",
    "    # optimizer = optim.ASGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "    optimizer = optim.Adadelta(model.parameters())\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "    log_interval = 5\n",
    "    print_interval = 40\n",
    "\n",
    "    \n",
    "    epochs = 20\n",
    "    load = True\n",
    "    if load:\n",
    "        saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "        model.load_state_dict(torch.load(saved_file))\n",
    "        start_epoch = int(saved_file[-1]) + 1\n",
    "        time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "    else:\n",
    "        start_epoch = 0\n",
    "        time = str(datetime.datetime.now())\n",
    "\n",
    "    save_dir = os.path.join('Trained Models', f'Training_{time}')\n",
    "    try:    \n",
    "        os.mkdir(save_dir);\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    summary_dir = os.path.join(tensorboard_dir, time)\n",
    "    writer = SummaryWriter(summary_dir)\n",
    "\n",
    "    # Saving hyperparmas\n",
    "    with open(os.path.join(save_dir, 'info.txt'), 'w') as f:\n",
    "        pickle.dump(hyperparams, f)\n",
    "        \n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "        train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "        scheduler.step()                                    # Decrease learning rate\n",
    "        torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "        model.tf_ratio = max(model.tf_ratio - 0.01, 0.8)    # Decrease teacher force ratio                       "
   ]
  }
 ]
}