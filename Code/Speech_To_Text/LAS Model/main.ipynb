{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chars(lang, save_file=None, train_df=None):\n",
    "    if lang=='eng':\n",
    "        chars = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \\\n",
    "                'y', 'z', ' ', \"'\", '<eos>', '<pad>']\n",
    "    elif lang=='chinese':            \n",
    "        try:\n",
    "            with open(save_file, 'rb') as f:\n",
    "                chars = pickle.load(f) # load file\n",
    "        except FileNotFoundError:\n",
    "            chars = [' ', '<sos>']\n",
    "            for idx in range(train_df.shape[0]):\n",
    "                _, sent = train_df.iloc[idx]\n",
    "                for c in sent:\n",
    "                    if c not in chars:\n",
    "                        chars.append(c)\n",
    "            chars = chars + ['<eos>', '<pad>']\n",
    "            with open(save_file, 'wb') as f:\n",
    "                pickle.dump(chars, f) # save file\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(dataset_dir):\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text:\n",
    "                data.append((f.replace('.txt', ''), text.readline()))\n",
    "                \n",
    "    train_df = pd.DataFrame(data, columns=['id', 'sent'])\n",
    "    train_df.to_csv(os.path.join(dataset_dir, 'train_df.csv'), header=None)\n",
    "    print(train_df.head())\n",
    "\n",
    "\n",
    "# Used for ai_shell dataset, when all sentences are in a single text file\n",
    "def read_transcrpt(transcript_dir):\n",
    "    transcript_dir = '../../../Dataset'\n",
    "    with open(os.path.join(transcript_dir, 'aishell_transcript_v0.8.txt')) as f:\n",
    "        data_list = f.readlines()\n",
    "\n",
    "    data = []\n",
    "    for example in data_list:\n",
    "        id_, sent = str(example.split(' ')[0]), str(' '.join(example.split(' ')[1:-1])) # -1 to remove '\\n'\n",
    "        data.append((id_, sent))\n",
    "\n",
    "    print('Num examples:', len(data))\n",
    "    data_df = pd.DataFrame(data, columns=['id', 'sent'])\n",
    "    data_df.to_csv(os.path.join(transcript_dir, 'train_df.csv'))\n",
    "    data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n",
      "                 id                        sent\n",
      "0  BAC009S0002W0122     而 对 楼市 成交 抑制 作用 最 大 的 限\n",
      "1  BAC009S0002W0123             也 成为 地方 政府 的 眼中\n",
      "2  BAC009S0002W0124  自 六月 底 呼和浩特 市 率先 宣布 取消 限 购\n",
      "3  BAC009S0002W0125                  各地 政府 便 纷纷\n",
      "4  BAC009S0002W0126              仅 一 个 多 月 的 时间\n",
      "Number of chars 4255\n"
     ]
    }
   ],
   "source": [
    "dataset_dir = '../../../Dataset/data_aishell'\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(os.path.join(dataset_dir, 'train_df.csv'), names=['id', 'sent'])\n",
    "train_df = train_df.dropna(how='any')\n",
    "print(train_df.head())\n",
    "# test_df = pd.read_csv('test_df.csv', names=['id', 'sent'])\n",
    "\n",
    "save_file = os.path.join('Trained Models', 'chars')\n",
    "chars = get_chars('chinese', save_file, train_df)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']\n",
    "\n",
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, eos_token)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=32, shuffle=True, drop_last=True)\n",
    "\n",
    "# #test_dataset = SpeechDataset(test_df, dataset_dir)\n",
    "# #test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 256    # 256*2 nodes in each LSTM\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 256\n",
    "embed_dim = 100\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, 'num_layers':num_layers,\n",
    "                'dropout':dropout, 'layer_norm':layer_norm, 'hid_sz':hid_sz, 'embed_dim':embed_dim}\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Seq2Seq(encoder, decoder, criterion, tf_ratio = 1.0, device=DEVICE).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [1248/141569 (1%)]\tMean Loss : 0.261120\t time 0:00:29.867798:\n",
      "Train Epoch: 0 [2528/141569 (2%)]\tMean Loss : 0.261120\t time 0:00:26.448450:\n",
      "Train Epoch: 0 [3808/141569 (3%)]\tMean Loss : 0.261119\t time 0:00:27.068087:\n",
      "Train Epoch: 0 [5088/141569 (4%)]\tMean Loss : 0.261119\t time 0:00:26.559825:\n",
      "Train Epoch: 0 [6368/141569 (4%)]\tMean Loss : 0.261118\t time 0:00:26.030298:\n",
      "Train Epoch: 0 [7648/141569 (5%)]\tMean Loss : 0.261118\t time 0:00:26.209467:\n",
      "Train Epoch: 0 [8928/141569 (6%)]\tMean Loss : 0.261117\t time 0:00:25.764259:\n",
      "Train Epoch: 0 [10208/141569 (7%)]\tMean Loss : 0.261116\t time 0:00:26.393853:\n",
      "Train Epoch: 0 [11488/141569 (8%)]\tMean Loss : 0.261115\t time 0:00:26.098833:\n",
      "Train Epoch: 0 [12768/141569 (9%)]\tMean Loss : 0.261111\t time 0:00:27.027586:\n",
      "Train Epoch: 0 [14048/141569 (10%)]\tMean Loss : 0.261104\t time 0:00:26.251614:\n",
      "Train Epoch: 0 [15328/141569 (11%)]\tMean Loss : 0.261083\t time 0:00:25.776258:\n",
      "Train Epoch: 0 [16608/141569 (12%)]\tMean Loss : 0.260986\t time 0:00:26.668551:\n",
      "Train Epoch: 0 [17888/141569 (13%)]\tMean Loss : 0.260740\t time 0:00:25.634130:\n",
      "Train Epoch: 0 [19168/141569 (14%)]\tMean Loss : 0.260404\t time 0:00:26.010410:\n",
      "Train Epoch: 0 [20448/141569 (14%)]\tMean Loss : 0.260070\t time 0:00:26.900140:\n",
      "Train Epoch: 0 [21728/141569 (15%)]\tMean Loss : 0.259607\t time 0:00:26.643287:\n",
      "Train Epoch: 0 [23008/141569 (16%)]\tMean Loss : 0.258891\t time 0:00:26.654465:\n",
      "Train Epoch: 0 [24288/141569 (17%)]\tMean Loss : 0.257739\t time 0:00:26.673551:\n",
      "Train Epoch: 0 [25568/141569 (18%)]\tMean Loss : 0.255885\t time 0:00:26.243342:\n",
      "Train Epoch: 0 [26848/141569 (19%)]\tMean Loss : 0.253763\t time 0:00:26.146321:\n",
      "Train Epoch: 0 [28128/141569 (20%)]\tMean Loss : 0.251472\t time 0:00:26.472574:\n",
      "Train Epoch: 0 [29408/141569 (21%)]\tMean Loss : 0.249615\t time 0:00:25.803050:\n",
      "Train Epoch: 0 [30688/141569 (22%)]\tMean Loss : 0.247408\t time 0:00:26.871975:\n",
      "Train Epoch: 0 [31968/141569 (23%)]\tMean Loss : 0.245005\t time 0:00:26.615101:\n",
      "Train Epoch: 0 [33248/141569 (23%)]\tMean Loss : 0.243292\t time 0:00:26.848265:\n",
      "Train Epoch: 0 [34528/141569 (24%)]\tMean Loss : 0.242755\t time 0:00:26.696028:\n",
      "Train Epoch: 0 [35808/141569 (25%)]\tMean Loss : 0.241521\t time 0:00:27.037070:\n",
      "Train Epoch: 0 [37088/141569 (26%)]\tMean Loss : 0.242049\t time 0:00:26.553594:\n",
      "Train Epoch: 0 [38368/141569 (27%)]\tMean Loss : 0.241515\t time 0:00:26.650402:\n",
      "Train Epoch: 0 [39648/141569 (28%)]\tMean Loss : 0.240784\t time 0:00:27.202379:\n",
      "Train Epoch: 0 [40928/141569 (29%)]\tMean Loss : 0.241272\t time 0:00:26.745561:\n",
      "Train Epoch: 0 [42208/141569 (30%)]\tMean Loss : 0.240658\t time 0:00:26.727329:\n",
      "Train Epoch: 0 [43488/141569 (31%)]\tMean Loss : 0.240647\t time 0:00:25.741788:\n",
      "Train Epoch: 0 [44768/141569 (32%)]\tMean Loss : 0.240502\t time 0:00:27.007843:\n",
      "Train Epoch: 0 [46048/141569 (33%)]\tMean Loss : 0.240353\t time 0:00:27.550566:\n",
      "Train Epoch: 0 [47328/141569 (33%)]\tMean Loss : 0.240236\t time 0:00:26.664388:\n",
      "Train Epoch: 0 [48608/141569 (34%)]\tMean Loss : 0.239929\t time 0:00:26.535541:\n",
      "Train Epoch: 0 [49888/141569 (35%)]\tMean Loss : 0.240350\t time 0:00:27.398960:\n",
      "Train Epoch: 0 [51168/141569 (36%)]\tMean Loss : 0.240018\t time 0:00:27.052710:\n",
      "Train Epoch: 0 [52448/141569 (37%)]\tMean Loss : 0.240346\t time 0:00:26.197593:\n",
      "Train Epoch: 0 [53728/141569 (38%)]\tMean Loss : 0.240433\t time 0:00:26.949122:\n",
      "Train Epoch: 0 [55008/141569 (39%)]\tMean Loss : 0.239630\t time 0:00:27.193628:\n",
      "Train Epoch: 0 [56288/141569 (40%)]\tMean Loss : 0.240215\t time 0:00:26.239971:\n",
      "Train Epoch: 0 [57568/141569 (41%)]\tMean Loss : 0.240109\t time 0:00:26.673715:\n",
      "Train Epoch: 0 [58848/141569 (42%)]\tMean Loss : 0.239859\t time 0:00:26.862031:\n",
      "Train Epoch: 0 [60128/141569 (42%)]\tMean Loss : 0.240191\t time 0:00:26.687096:\n",
      "Train Epoch: 0 [61408/141569 (43%)]\tMean Loss : 0.240293\t time 0:00:26.786615:\n",
      "Train Epoch: 0 [62688/141569 (44%)]\tMean Loss : 0.239788\t time 0:00:26.432777:\n",
      "Train Epoch: 0 [63968/141569 (45%)]\tMean Loss : 0.239913\t time 0:00:25.990566:\n",
      "Train Epoch: 0 [65248/141569 (46%)]\tMean Loss : 0.240031\t time 0:00:25.516615:\n",
      "Train Epoch: 0 [66528/141569 (47%)]\tMean Loss : 0.239970\t time 0:00:26.016694:\n",
      "Train Epoch: 0 [67808/141569 (48%)]\tMean Loss : 0.240299\t time 0:00:26.034825:\n",
      "Train Epoch: 0 [69088/141569 (49%)]\tMean Loss : 0.239881\t time 0:00:26.827692:\n",
      "Train Epoch: 0 [70368/141569 (50%)]\tMean Loss : 0.240408\t time 0:00:26.307658:\n",
      "Train Epoch: 0 [71648/141569 (51%)]\tMean Loss : 0.240363\t time 0:00:26.725733:\n",
      "Train Epoch: 0 [72928/141569 (52%)]\tMean Loss : 0.239729\t time 0:00:26.493640:\n",
      "Train Epoch: 0 [74208/141569 (52%)]\tMean Loss : 0.239774\t time 0:00:26.243978:\n",
      "Train Epoch: 0 [75488/141569 (53%)]\tMean Loss : 0.239955\t time 0:00:26.197025:\n",
      "Train Epoch: 0 [76768/141569 (54%)]\tMean Loss : 0.240217\t time 0:00:26.589769:\n",
      "Train Epoch: 0 [78048/141569 (55%)]\tMean Loss : 0.240171\t time 0:00:26.398606:\n",
      "Train Epoch: 0 [79328/141569 (56%)]\tMean Loss : 0.240045\t time 0:00:26.314837:\n",
      "Train Epoch: 0 [80608/141569 (57%)]\tMean Loss : 0.240421\t time 0:00:26.630329:\n",
      "Train Epoch: 0 [81888/141569 (58%)]\tMean Loss : 0.239679\t time 0:00:26.684599:\n",
      "Train Epoch: 0 [83168/141569 (59%)]\tMean Loss : 0.239751\t time 0:00:27.074404:\n",
      "Train Epoch: 0 [84448/141569 (60%)]\tMean Loss : 0.239668\t time 0:00:27.353553:\n",
      "Train Epoch: 0 [85728/141569 (61%)]\tMean Loss : 0.239673\t time 0:00:27.096478:\n",
      "Train Epoch: 0 [87008/141569 (61%)]\tMean Loss : 0.239573\t time 0:00:26.828356:\n",
      "Train Epoch: 0 [88288/141569 (62%)]\tMean Loss : 0.239932\t time 0:00:26.780833:\n",
      "Train Epoch: 0 [89568/141569 (63%)]\tMean Loss : 0.239664\t time 0:00:26.210713:\n",
      "Train Epoch: 0 [90848/141569 (64%)]\tMean Loss : 0.239789\t time 0:00:26.156940:\n",
      "Train Epoch: 0 [92128/141569 (65%)]\tMean Loss : 0.239593\t time 0:00:27.236611:\n",
      "Train Epoch: 0 [93408/141569 (66%)]\tMean Loss : 0.240081\t time 0:00:26.326483:\n",
      "Train Epoch: 0 [94688/141569 (67%)]\tMean Loss : 0.239841\t time 0:00:26.345279:\n",
      "Train Epoch: 0 [95968/141569 (68%)]\tMean Loss : 0.239982\t time 0:00:27.307448:\n",
      "Train Epoch: 0 [97248/141569 (69%)]\tMean Loss : 0.239567\t time 0:00:27.059675:\n",
      "Train Epoch: 0 [98528/141569 (70%)]\tMean Loss : 0.239636\t time 0:00:26.580025:\n",
      "Train Epoch: 0 [99808/141569 (71%)]\tMean Loss : 0.239929\t time 0:00:26.980980:\n",
      "Train Epoch: 0 [101088/141569 (71%)]\tMean Loss : 0.240069\t time 0:00:26.190861:\n",
      "Train Epoch: 0 [102368/141569 (72%)]\tMean Loss : 0.239616\t time 0:00:26.963599:\n",
      "Train Epoch: 0 [103648/141569 (73%)]\tMean Loss : 0.240104\t time 0:00:26.625231:\n",
      "Train Epoch: 0 [104928/141569 (74%)]\tMean Loss : 0.239781\t time 0:00:26.819619:\n",
      "Train Epoch: 0 [106208/141569 (75%)]\tMean Loss : 0.239589\t time 0:00:26.971836:\n",
      "Train Epoch: 0 [107488/141569 (76%)]\tMean Loss : 0.240063\t time 0:00:26.443643:\n",
      "Train Epoch: 0 [108768/141569 (77%)]\tMean Loss : 0.239589\t time 0:00:26.945325:\n",
      "Train Epoch: 0 [110048/141569 (78%)]\tMean Loss : 0.239420\t time 0:00:26.835266:\n",
      "Train Epoch: 0 [111328/141569 (79%)]\tMean Loss : 0.239768\t time 0:00:26.890218:\n",
      "Train Epoch: 0 [112608/141569 (80%)]\tMean Loss : 0.239628\t time 0:00:26.960368:\n",
      "Train Epoch: 0 [113888/141569 (80%)]\tMean Loss : 0.239604\t time 0:00:26.604202:\n",
      "Train Epoch: 0 [115168/141569 (81%)]\tMean Loss : 0.239863\t time 0:00:27.803375:\n",
      "Train Epoch: 0 [116448/141569 (82%)]\tMean Loss : 0.239970\t time 0:00:27.176023:\n",
      "Train Epoch: 0 [117728/141569 (83%)]\tMean Loss : 0.239545\t time 0:00:27.442401:\n",
      "Train Epoch: 0 [119008/141569 (84%)]\tMean Loss : 0.239731\t time 0:00:27.419009:\n",
      "Train Epoch: 0 [120288/141569 (85%)]\tMean Loss : 0.239812\t time 0:00:26.190125:\n",
      "Train Epoch: 0 [121568/141569 (86%)]\tMean Loss : 0.239387\t time 0:00:27.253312:\n",
      "Train Epoch: 0 [122848/141569 (87%)]\tMean Loss : 0.239937\t time 0:00:27.729459:\n",
      "Train Epoch: 0 [124128/141569 (88%)]\tMean Loss : 0.239558\t time 0:00:26.813688:\n",
      "Train Epoch: 0 [125408/141569 (89%)]\tMean Loss : 0.239295\t time 0:00:27.395523:\n",
      "Train Epoch: 0 [126688/141569 (89%)]\tMean Loss : 0.239693\t time 0:00:26.980637:\n",
      "Train Epoch: 0 [127968/141569 (90%)]\tMean Loss : 0.239533\t time 0:00:27.337841:\n",
      "Train Epoch: 0 [129248/141569 (91%)]\tMean Loss : 0.239855\t time 0:00:26.855348:\n",
      "Train Epoch: 0 [130528/141569 (92%)]\tMean Loss : 0.239447\t time 0:00:26.342450:\n",
      "Train Epoch: 0 [131808/141569 (93%)]\tMean Loss : 0.239660\t time 0:00:27.393839:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [133088/141569 (94%)]\tMean Loss : 0.240397\t time 0:00:26.419357:\n",
      "Train Epoch: 0 [134368/141569 (95%)]\tMean Loss : 0.239443\t time 0:00:27.348112:\n",
      "Train Epoch: 0 [135648/141569 (96%)]\tMean Loss : 0.239922\t time 0:00:26.731726:\n",
      "Train Epoch: 0 [136928/141569 (97%)]\tMean Loss : 0.239369\t time 0:00:27.618272:\n",
      "Train Epoch: 0 [138208/141569 (98%)]\tMean Loss : 0.239488\t time 0:00:26.950408:\n",
      "Train Epoch: 0 [139488/141569 (99%)]\tMean Loss : 0.239564\t time 0:00:27.492356:\n",
      "Train Epoch: 0 [140768/141569 (99%)]\tMean Loss : 0.239501\t time 0:00:27.297704:\n",
      "\n",
      "Teacher forcing ratio: 0.99\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 1 [1248/141569 (1%)]\tMean Loss : 0.239727\t time 0:00:26.455587:\n",
      "Train Epoch: 1 [2528/141569 (2%)]\tMean Loss : 0.239919\t time 0:00:26.597855:\n",
      "Train Epoch: 1 [3808/141569 (3%)]\tMean Loss : 0.239910\t time 0:00:26.136995:\n",
      "Train Epoch: 1 [5088/141569 (4%)]\tMean Loss : 0.240045\t time 0:00:25.614074:\n",
      "Train Epoch: 1 [6368/141569 (4%)]\tMean Loss : 0.239691\t time 0:00:26.229790:\n",
      "Train Epoch: 1 [7648/141569 (5%)]\tMean Loss : 0.239403\t time 0:00:26.479253:\n",
      "Train Epoch: 1 [8928/141569 (6%)]\tMean Loss : 0.239263\t time 0:00:26.357883:\n",
      "Train Epoch: 1 [10208/141569 (7%)]\tMean Loss : 0.239410\t time 0:00:26.345938:\n",
      "Train Epoch: 1 [11488/141569 (8%)]\tMean Loss : 0.240078\t time 0:00:25.590434:\n",
      "Train Epoch: 1 [12768/141569 (9%)]\tMean Loss : 0.240190\t time 0:00:26.133601:\n",
      "Train Epoch: 1 [14048/141569 (10%)]\tMean Loss : 0.239759\t time 0:00:26.646790:\n",
      "Train Epoch: 1 [15328/141569 (11%)]\tMean Loss : 0.239632\t time 0:00:25.987668:\n",
      "Train Epoch: 1 [16608/141569 (12%)]\tMean Loss : 0.240017\t time 0:00:26.696270:\n",
      "Train Epoch: 1 [17888/141569 (13%)]\tMean Loss : 0.239452\t time 0:00:27.117414:\n",
      "Train Epoch: 1 [19168/141569 (14%)]\tMean Loss : 0.239805\t time 0:00:26.892871:\n",
      "Train Epoch: 1 [20448/141569 (14%)]\tMean Loss : 0.239625\t time 0:00:26.601788:\n",
      "Train Epoch: 1 [21728/141569 (15%)]\tMean Loss : 0.240495\t time 0:00:26.007969:\n",
      "Train Epoch: 1 [23008/141569 (16%)]\tMean Loss : 0.239582\t time 0:00:26.200896:\n",
      "Train Epoch: 1 [24288/141569 (17%)]\tMean Loss : 0.239634\t time 0:00:25.995210:\n",
      "Train Epoch: 1 [25568/141569 (18%)]\tMean Loss : 0.239672\t time 0:00:25.814524:\n",
      "Train Epoch: 1 [26848/141569 (19%)]\tMean Loss : 0.239836\t time 0:00:26.390195:\n",
      "Train Epoch: 1 [28128/141569 (20%)]\tMean Loss : 0.239804\t time 0:00:26.272032:\n",
      "Train Epoch: 1 [29408/141569 (21%)]\tMean Loss : 0.239642\t time 0:00:26.648338:\n",
      "Train Epoch: 1 [30688/141569 (22%)]\tMean Loss : 0.239680\t time 0:00:26.126485:\n",
      "Train Epoch: 1 [31968/141569 (23%)]\tMean Loss : 0.239700\t time 0:00:26.229140:\n",
      "Train Epoch: 1 [33248/141569 (23%)]\tMean Loss : 0.239439\t time 0:00:26.753992:\n",
      "Train Epoch: 1 [34528/141569 (24%)]\tMean Loss : 0.239238\t time 0:00:26.780881:\n",
      "Train Epoch: 1 [35808/141569 (25%)]\tMean Loss : 0.239865\t time 0:00:26.251597:\n",
      "Train Epoch: 1 [37088/141569 (26%)]\tMean Loss : 0.239540\t time 0:00:27.509022:\n",
      "Train Epoch: 1 [38368/141569 (27%)]\tMean Loss : 0.239697\t time 0:00:27.069194:\n",
      "Train Epoch: 1 [39648/141569 (28%)]\tMean Loss : 0.239578\t time 0:00:26.686596:\n",
      "Train Epoch: 1 [40928/141569 (29%)]\tMean Loss : 0.239638\t time 0:00:27.654534:\n",
      "Train Epoch: 1 [42208/141569 (30%)]\tMean Loss : 0.240066\t time 0:00:26.817865:\n",
      "Train Epoch: 1 [43488/141569 (31%)]\tMean Loss : 0.239473\t time 0:00:26.592029:\n",
      "Train Epoch: 1 [44768/141569 (32%)]\tMean Loss : 0.239803\t time 0:00:26.442372:\n",
      "Train Epoch: 1 [46048/141569 (33%)]\tMean Loss : 0.239887\t time 0:00:27.602656:\n",
      "Train Epoch: 1 [47328/141569 (33%)]\tMean Loss : 0.239644\t time 0:00:26.037783:\n",
      "Train Epoch: 1 [48608/141569 (34%)]\tMean Loss : 0.239715\t time 0:00:26.654458:\n",
      "Train Epoch: 1 [49888/141569 (35%)]\tMean Loss : 0.239926\t time 0:00:25.915777:\n",
      "Train Epoch: 1 [51168/141569 (36%)]\tMean Loss : 0.239840\t time 0:00:26.768140:\n",
      "Train Epoch: 1 [52448/141569 (37%)]\tMean Loss : 0.239442\t time 0:00:26.141007:\n",
      "Train Epoch: 1 [53728/141569 (38%)]\tMean Loss : 0.239723\t time 0:00:26.819046:\n",
      "Train Epoch: 1 [55008/141569 (39%)]\tMean Loss : 0.239354\t time 0:00:26.820203:\n",
      "Train Epoch: 1 [56288/141569 (40%)]\tMean Loss : 0.239922\t time 0:00:26.034041:\n",
      "Train Epoch: 1 [57568/141569 (41%)]\tMean Loss : 0.239647\t time 0:00:26.870369:\n",
      "Train Epoch: 1 [58848/141569 (42%)]\tMean Loss : 0.239926\t time 0:00:27.379202:\n",
      "Train Epoch: 1 [60128/141569 (42%)]\tMean Loss : 0.239821\t time 0:00:27.417677:\n",
      "Train Epoch: 1 [61408/141569 (43%)]\tMean Loss : 0.239919\t time 0:00:27.270311:\n",
      "Train Epoch: 1 [62688/141569 (44%)]\tMean Loss : 0.239661\t time 0:00:26.893388:\n",
      "Train Epoch: 1 [63968/141569 (45%)]\tMean Loss : 0.239905\t time 0:00:26.624950:\n",
      "Train Epoch: 1 [65248/141569 (46%)]\tMean Loss : 0.240187\t time 0:00:26.196435:\n",
      "Train Epoch: 1 [66528/141569 (47%)]\tMean Loss : 0.239625\t time 0:00:26.106138:\n",
      "Train Epoch: 1 [67808/141569 (48%)]\tMean Loss : 0.239542\t time 0:00:27.274704:\n",
      "Train Epoch: 1 [69088/141569 (49%)]\tMean Loss : 0.239561\t time 0:00:26.388260:\n",
      "Train Epoch: 1 [70368/141569 (50%)]\tMean Loss : 0.239559\t time 0:00:26.330355:\n",
      "Train Epoch: 1 [71648/141569 (51%)]\tMean Loss : 0.239873\t time 0:00:25.526743:\n",
      "Train Epoch: 1 [72928/141569 (52%)]\tMean Loss : 0.239579\t time 0:00:26.621556:\n",
      "Train Epoch: 1 [74208/141569 (52%)]\tMean Loss : 0.239974\t time 0:00:26.010309:\n",
      "Train Epoch: 1 [75488/141569 (53%)]\tMean Loss : 0.239481\t time 0:00:25.830617:\n",
      "Train Epoch: 1 [76768/141569 (54%)]\tMean Loss : 0.239579\t time 0:00:25.903641:\n",
      "Train Epoch: 1 [78048/141569 (55%)]\tMean Loss : 0.239664\t time 0:00:26.346965:\n",
      "Train Epoch: 1 [79328/141569 (56%)]\tMean Loss : 0.239867\t time 0:00:26.535100:\n",
      "Train Epoch: 1 [80608/141569 (57%)]\tMean Loss : 0.239808\t time 0:00:26.921683:\n",
      "Train Epoch: 1 [81888/141569 (58%)]\tMean Loss : 0.239479\t time 0:00:26.582532:\n",
      "Train Epoch: 1 [83168/141569 (59%)]\tMean Loss : 0.239790\t time 0:00:27.176120:\n",
      "Train Epoch: 1 [84448/141569 (60%)]\tMean Loss : 0.239624\t time 0:00:26.897188:\n",
      "Train Epoch: 1 [85728/141569 (61%)]\tMean Loss : 0.239690\t time 0:00:27.330430:\n",
      "Train Epoch: 1 [87008/141569 (61%)]\tMean Loss : 0.239923\t time 0:00:26.280073:\n",
      "Train Epoch: 1 [88288/141569 (62%)]\tMean Loss : 0.239634\t time 0:00:26.614424:\n",
      "Train Epoch: 1 [89568/141569 (63%)]\tMean Loss : 0.239703\t time 0:00:27.531088:\n",
      "Train Epoch: 1 [90848/141569 (64%)]\tMean Loss : 0.239795\t time 0:00:26.752099:\n",
      "Train Epoch: 1 [92128/141569 (65%)]\tMean Loss : 0.239610\t time 0:00:26.534490:\n",
      "Train Epoch: 1 [93408/141569 (66%)]\tMean Loss : 0.239384\t time 0:00:26.621092:\n",
      "Train Epoch: 1 [94688/141569 (67%)]\tMean Loss : 0.239715\t time 0:00:26.561835:\n",
      "Train Epoch: 1 [95968/141569 (68%)]\tMean Loss : 0.240032\t time 0:00:25.972081:\n",
      "Train Epoch: 1 [97248/141569 (69%)]\tMean Loss : 0.239136\t time 0:00:27.717088:\n",
      "Train Epoch: 1 [98528/141569 (70%)]\tMean Loss : 0.239833\t time 0:00:26.578211:\n",
      "Train Epoch: 1 [99808/141569 (71%)]\tMean Loss : 0.239502\t time 0:00:26.554894:\n",
      "Train Epoch: 1 [101088/141569 (71%)]\tMean Loss : 0.239268\t time 0:00:27.189803:\n",
      "Train Epoch: 1 [102368/141569 (72%)]\tMean Loss : 0.239829\t time 0:00:26.619317:\n",
      "Train Epoch: 1 [103648/141569 (73%)]\tMean Loss : 0.239724\t time 0:00:26.876216:\n",
      "Train Epoch: 1 [104928/141569 (74%)]\tMean Loss : 0.240043\t time 0:00:26.298697:\n",
      "Train Epoch: 1 [106208/141569 (75%)]\tMean Loss : 0.239626\t time 0:00:27.013044:\n",
      "Train Epoch: 1 [107488/141569 (76%)]\tMean Loss : 0.239980\t time 0:00:26.892464:\n",
      "Train Epoch: 1 [108768/141569 (77%)]\tMean Loss : 0.239581\t time 0:00:26.335405:\n",
      "Train Epoch: 1 [110048/141569 (78%)]\tMean Loss : 0.239944\t time 0:00:26.494905:\n",
      "Train Epoch: 1 [111328/141569 (79%)]\tMean Loss : 0.239879\t time 0:00:26.617799:\n",
      "Train Epoch: 1 [112608/141569 (80%)]\tMean Loss : 0.239412\t time 0:00:27.106375:\n",
      "Train Epoch: 1 [113888/141569 (80%)]\tMean Loss : 0.239459\t time 0:00:26.350150:\n",
      "Train Epoch: 1 [115168/141569 (81%)]\tMean Loss : 0.239806\t time 0:00:26.933432:\n",
      "Train Epoch: 1 [116448/141569 (82%)]\tMean Loss : 0.239424\t time 0:00:26.745647:\n",
      "Train Epoch: 1 [117728/141569 (83%)]\tMean Loss : 0.240056\t time 0:00:27.493824:\n",
      "Train Epoch: 1 [119008/141569 (84%)]\tMean Loss : 0.240042\t time 0:00:26.634579:\n",
      "Train Epoch: 1 [120288/141569 (85%)]\tMean Loss : 0.239674\t time 0:00:26.572675:\n",
      "Train Epoch: 1 [121568/141569 (86%)]\tMean Loss : 0.240062\t time 0:00:26.412598:\n",
      "Train Epoch: 1 [122848/141569 (87%)]\tMean Loss : 0.240015\t time 0:00:26.901706:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [124128/141569 (88%)]\tMean Loss : 0.239491\t time 0:00:26.935020:\n",
      "Train Epoch: 1 [125408/141569 (89%)]\tMean Loss : 0.239460\t time 0:00:26.961754:\n",
      "Train Epoch: 1 [126688/141569 (89%)]\tMean Loss : 0.239741\t time 0:00:26.041079:\n",
      "Train Epoch: 1 [127968/141569 (90%)]\tMean Loss : 0.239705\t time 0:00:26.484581:\n",
      "Train Epoch: 1 [129248/141569 (91%)]\tMean Loss : 0.239702\t time 0:00:26.596082:\n",
      "Train Epoch: 1 [130528/141569 (92%)]\tMean Loss : 0.239455\t time 0:00:27.664334:\n",
      "Train Epoch: 1 [131808/141569 (93%)]\tMean Loss : 0.239427\t time 0:00:25.379719:\n",
      "Train Epoch: 1 [133088/141569 (94%)]\tMean Loss : 0.239566\t time 0:00:25.088972:\n",
      "Train Epoch: 1 [134368/141569 (95%)]\tMean Loss : 0.239501\t time 0:00:24.876092:\n",
      "Train Epoch: 1 [135648/141569 (96%)]\tMean Loss : 0.239599\t time 0:00:24.434563:\n",
      "Train Epoch: 1 [136928/141569 (97%)]\tMean Loss : 0.239832\t time 0:00:24.248281:\n",
      "Train Epoch: 1 [138208/141569 (98%)]\tMean Loss : 0.239987\t time 0:00:24.194678:\n",
      "Train Epoch: 1 [139488/141569 (99%)]\tMean Loss : 0.239631\t time 0:00:24.526693:\n",
      "Train Epoch: 1 [140768/141569 (99%)]\tMean Loss : 0.239985\t time 0:00:23.860632:\n",
      "\n",
      "Teacher forcing ratio: 0.98\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 2 [1248/141569 (1%)]\tMean Loss : 0.239590\t time 0:00:24.528610:\n",
      "Train Epoch: 2 [2528/141569 (2%)]\tMean Loss : 0.239757\t time 0:00:24.049933:\n",
      "Train Epoch: 2 [3808/141569 (3%)]\tMean Loss : 0.240057\t time 0:00:23.743155:\n",
      "Train Epoch: 2 [5088/141569 (4%)]\tMean Loss : 0.239793\t time 0:00:24.808130:\n",
      "Train Epoch: 2 [6368/141569 (4%)]\tMean Loss : 0.239869\t time 0:00:24.605551:\n",
      "Train Epoch: 2 [7648/141569 (5%)]\tMean Loss : 0.239681\t time 0:00:24.655274:\n",
      "Train Epoch: 2 [8928/141569 (6%)]\tMean Loss : 0.239477\t time 0:00:24.358401:\n",
      "Train Epoch: 2 [10208/141569 (7%)]\tMean Loss : 0.239671\t time 0:00:24.120706:\n",
      "Train Epoch: 2 [11488/141569 (8%)]\tMean Loss : 0.239795\t time 0:00:24.052356:\n",
      "Train Epoch: 2 [12768/141569 (9%)]\tMean Loss : 0.239606\t time 0:00:23.944468:\n",
      "Train Epoch: 2 [14048/141569 (10%)]\tMean Loss : 0.239496\t time 0:00:24.434258:\n",
      "Train Epoch: 2 [15328/141569 (11%)]\tMean Loss : 0.239526\t time 0:00:24.183477:\n",
      "Train Epoch: 2 [16608/141569 (12%)]\tMean Loss : 0.239491\t time 0:00:24.249825:\n",
      "Train Epoch: 2 [17888/141569 (13%)]\tMean Loss : 0.239538\t time 0:00:23.689093:\n",
      "Train Epoch: 2 [19168/141569 (14%)]\tMean Loss : 0.239839\t time 0:00:24.044234:\n",
      "Train Epoch: 2 [20448/141569 (14%)]\tMean Loss : 0.239684\t time 0:00:24.383244:\n",
      "Train Epoch: 2 [21728/141569 (15%)]\tMean Loss : 0.239405\t time 0:00:23.873845:\n",
      "Train Epoch: 2 [23008/141569 (16%)]\tMean Loss : 0.239986\t time 0:00:23.998397:\n",
      "Train Epoch: 2 [24288/141569 (17%)]\tMean Loss : 0.240124\t time 0:00:23.285121:\n",
      "Train Epoch: 2 [25568/141569 (18%)]\tMean Loss : 0.239658\t time 0:00:23.905772:\n",
      "Train Epoch: 2 [26848/141569 (19%)]\tMean Loss : 0.239565\t time 0:00:23.812055:\n",
      "Train Epoch: 2 [28128/141569 (20%)]\tMean Loss : 0.239528\t time 0:00:24.301943:\n",
      "Train Epoch: 2 [29408/141569 (21%)]\tMean Loss : 0.240063\t time 0:00:23.835907:\n",
      "Train Epoch: 2 [30688/141569 (22%)]\tMean Loss : 0.239700\t time 0:00:24.076723:\n",
      "Train Epoch: 2 [31968/141569 (23%)]\tMean Loss : 0.239673\t time 0:00:24.206940:\n",
      "Train Epoch: 2 [33248/141569 (23%)]\tMean Loss : 0.239619\t time 0:00:24.494501:\n",
      "Train Epoch: 2 [34528/141569 (24%)]\tMean Loss : 0.240084\t time 0:00:23.927267:\n",
      "Train Epoch: 2 [35808/141569 (25%)]\tMean Loss : 0.239912\t time 0:00:24.189333:\n",
      "Train Epoch: 2 [37088/141569 (26%)]\tMean Loss : 0.240053\t time 0:00:23.982358:\n",
      "Train Epoch: 2 [38368/141569 (27%)]\tMean Loss : 0.239573\t time 0:00:24.169082:\n",
      "Train Epoch: 2 [39648/141569 (28%)]\tMean Loss : 0.239592\t time 0:00:24.535440:\n",
      "Train Epoch: 2 [40928/141569 (29%)]\tMean Loss : 0.239377\t time 0:00:24.243190:\n",
      "Train Epoch: 2 [42208/141569 (30%)]\tMean Loss : 0.239630\t time 0:00:24.395762:\n",
      "Train Epoch: 2 [43488/141569 (31%)]\tMean Loss : 0.239842\t time 0:00:24.134506:\n",
      "Train Epoch: 2 [44768/141569 (32%)]\tMean Loss : 0.239511\t time 0:00:24.368230:\n",
      "Train Epoch: 2 [46048/141569 (33%)]\tMean Loss : 0.239566\t time 0:00:24.089944:\n",
      "Train Epoch: 2 [47328/141569 (33%)]\tMean Loss : 0.239567\t time 0:00:23.926475:\n",
      "Train Epoch: 2 [48608/141569 (34%)]\tMean Loss : 0.239802\t time 0:00:24.312299:\n",
      "Train Epoch: 2 [49888/141569 (35%)]\tMean Loss : 0.239598\t time 0:00:24.637292:\n",
      "Train Epoch: 2 [51168/141569 (36%)]\tMean Loss : 0.240028\t time 0:00:24.089524:\n",
      "Train Epoch: 2 [52448/141569 (37%)]\tMean Loss : 0.239701\t time 0:00:24.459989:\n",
      "Train Epoch: 2 [53728/141569 (38%)]\tMean Loss : 0.239794\t time 0:00:23.893888:\n",
      "Train Epoch: 2 [55008/141569 (39%)]\tMean Loss : 0.239627\t time 0:00:24.080318:\n",
      "Train Epoch: 2 [56288/141569 (40%)]\tMean Loss : 0.239359\t time 0:00:24.495613:\n",
      "Train Epoch: 2 [57568/141569 (41%)]\tMean Loss : 0.239500\t time 0:00:24.444124:\n",
      "Train Epoch: 2 [58848/141569 (42%)]\tMean Loss : 0.239728\t time 0:00:23.955251:\n",
      "Train Epoch: 2 [60128/141569 (42%)]\tMean Loss : 0.240212\t time 0:00:24.109110:\n",
      "Train Epoch: 2 [61408/141569 (43%)]\tMean Loss : 0.239373\t time 0:00:23.997297:\n",
      "Train Epoch: 2 [62688/141569 (44%)]\tMean Loss : 0.239508\t time 0:00:24.166646:\n",
      "Train Epoch: 2 [63968/141569 (45%)]\tMean Loss : 0.239751\t time 0:00:24.238878:\n",
      "Train Epoch: 2 [65248/141569 (46%)]\tMean Loss : 0.239361\t time 0:00:24.347523:\n",
      "Train Epoch: 2 [66528/141569 (47%)]\tMean Loss : 0.239851\t time 0:00:24.028455:\n",
      "Train Epoch: 2 [67808/141569 (48%)]\tMean Loss : 0.239437\t time 0:00:24.301938:\n",
      "Train Epoch: 2 [69088/141569 (49%)]\tMean Loss : 0.239584\t time 0:00:24.670452:\n",
      "Train Epoch: 2 [70368/141569 (50%)]\tMean Loss : 0.239505\t time 0:00:24.634709:\n",
      "Train Epoch: 2 [71648/141569 (51%)]\tMean Loss : 0.239843\t time 0:00:25.097585:\n",
      "Train Epoch: 2 [72928/141569 (52%)]\tMean Loss : 0.239791\t time 0:00:24.160908:\n",
      "Train Epoch: 2 [74208/141569 (52%)]\tMean Loss : 0.239906\t time 0:00:24.110289:\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.ASGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "save_dir = os.path.join('Trained Models', f'Training_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.txt'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    scheduler.step()                                    # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.01, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "model.tf_ratio = 0.9\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, dataset_dir, sos_token, char_to_token, eos_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y))\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
