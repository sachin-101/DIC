{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower().replace('\\n', '')\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(root_dir, dataset, file_ext, csv_file):\n",
    "    dataset_dir = os.path.join(root_dir, dataset)\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text_file:\n",
    "                data_list = text_file.readlines()\n",
    "            for example in data_list:\n",
    "                path = os.path.join(dataset, str(example.split(' ')[0])) + file_ext   \n",
    "                sent = preprocess(str(' '.join(example.split(' ')[1:])))\n",
    "                data.append((path, sent))\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['path', 'sent'])\n",
    "    data_df.to_csv(os.path.join(root_dir, csv_file), header=None)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "root_dir = '../../../Dataset/LibriSpeech'\n",
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 281241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0001.flac</td>\n",
       "      <td>that had its source away back in the woods of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0002.flac</td>\n",
       "      <td>for not even a brook could run past missus rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_100/103-1240-0003.flac</td>\n",
       "      <td>and that if she noticed anything odd or out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_100/103-1240-0004.flac</td>\n",
       "      <td>but missus rachel lynde was one of those capab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0001.flac   \n",
       "2  dataset_100/103-1240-0002.flac   \n",
       "3  dataset_100/103-1240-0003.flac   \n",
       "4  dataset_100/103-1240-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  that had its source away back in the woods of ...  \n",
       "2  for not even a brook could run past missus rac...  \n",
       "3  and that if she noticed anything odd or out of...  \n",
       "4  but missus rachel lynde was one of those capab...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100 = pd.read_csv(os.path.join(root_dir, 'train_100.csv'), header=None, names=['path', 'sent'])\n",
    "train_360 = pd.read_csv(os.path.join(root_dir, 'train_360.csv'), header=None, names=['path', 'sent'])\n",
    "train_500 = pd.read_csv(os.path.join(root_dir, 'train_500.csv'), header=None, names=['path', 'sent'])\n",
    "\n",
    "# combine all of them\n",
    "train_df = pd.concat([train_100, train_360, train_500])\n",
    "print(\"Number of training examples:\", train_df.shape[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 165807\n"
     ]
    }
   ],
   "source": [
    "# Removing very large sentences\n",
    "def remove_long_sent(train_df, max_len):\n",
    "    data = []\n",
    "    for idx in range(train_df.shape[0]):\n",
    "        path, sent = train_df.iloc[idx]\n",
    "        if len(sent) > max_len:\n",
    "            continue\n",
    "        data.append((path, sent))\n",
    "    return pd.DataFrame(data, columns=['path', 'sent'])\n",
    "\n",
    "max_len = 200\n",
    "train_df = remove_long_sent(train_df, max_len)\n",
    "print(\"Number of training examples:\",  train_df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars 32\n"
     ]
    }
   ],
   "source": [
    "def get_chars(include_digits=True):\n",
    "    if include_digits:\n",
    "        chars = ['<sos>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \\\n",
    "                 'x', 'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    else:\n",
    "        chars = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \\\n",
    "                'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "chars = get_chars(include_digits=False)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=96, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "save_dir = os.path.join('trained_models_librispeech', f'Training_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 256, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(1024, 256, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (embedding): Embedding(32, 30)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=1280, out_features=640, bias=True)\n",
       "      (linear2): Linear(in_features=640, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(1054, 256)\n",
       "    (post_lstm_cell): LSTMCell(1280, 256)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=256, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       "  (loss): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 256  # 256*2 nodes in each LSTM\n",
    "num_layers = 3\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 256\n",
    "embed_dim = 30\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size}\n",
    "\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model = Seq2Seq(encoder, decoder, criterion, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 20 batches \n",
      "\n",
      "Train Epoch: 0 [1824/165807 (1%)]\tMean Loss : 7.174016\t time 0:01:04.656842:\n",
      "Train Epoch: 0 [3744/165807 (2%)]\tMean Loss : 7.120739\t time 0:00:56.408542:\n",
      "Train Epoch: 0 [5664/165807 (3%)]\tMean Loss : 7.097307\t time 0:00:56.588946:\n",
      "Train Epoch: 0 [7584/165807 (5%)]\tMean Loss : 7.019993\t time 0:00:56.319480:\n",
      "Train Epoch: 0 [9504/165807 (6%)]\tMean Loss : 6.948741\t time 0:00:56.259772:\n",
      "Train Epoch: 0 [11424/165807 (7%)]\tMean Loss : 6.893038\t time 0:00:56.275688:\n",
      "Train Epoch: 0 [13344/165807 (8%)]\tMean Loss : 6.825622\t time 0:00:55.999560:\n",
      "Train Epoch: 0 [15264/165807 (9%)]\tMean Loss : 6.740560\t time 0:00:55.949626:\n",
      "Train Epoch: 0 [17184/165807 (10%)]\tMean Loss : 6.660089\t time 0:00:56.187891:\n",
      "Train Epoch: 0 [19104/165807 (12%)]\tMean Loss : 6.571464\t time 0:00:55.279843:\n",
      "Train Epoch: 0 [21024/165807 (13%)]\tMean Loss : 6.486125\t time 0:00:56.711603:\n",
      "Train Epoch: 0 [22944/165807 (14%)]\tMean Loss : 6.414462\t time 0:00:55.798857:\n",
      "Train Epoch: 0 [24864/165807 (15%)]\tMean Loss : 6.352860\t time 0:00:56.289221:\n",
      "Train Epoch: 0 [26784/165807 (16%)]\tMean Loss : 6.299317\t time 0:00:56.064728:\n",
      "Train Epoch: 0 [28704/165807 (17%)]\tMean Loss : 6.253804\t time 0:00:56.374397:\n",
      "Train Epoch: 0 [30624/165807 (18%)]\tMean Loss : 6.207967\t time 0:00:56.302729:\n",
      "Train Epoch: 0 [32544/165807 (20%)]\tMean Loss : 6.186765\t time 0:00:56.662685:\n",
      "Train Epoch: 0 [34464/165807 (21%)]\tMean Loss : 6.167510\t time 0:00:56.632018:\n",
      "Train Epoch: 0 [36384/165807 (22%)]\tMean Loss : 6.151189\t time 0:00:56.202981:\n",
      "Train Epoch: 0 [38304/165807 (23%)]\tMean Loss : 6.123897\t time 0:00:56.458401:\n",
      "Train Epoch: 0 [40224/165807 (24%)]\tMean Loss : 6.111853\t time 0:00:56.122070:\n",
      "Train Epoch: 0 [42144/165807 (25%)]\tMean Loss : 6.095856\t time 0:00:55.909805:\n",
      "Train Epoch: 0 [44064/165807 (27%)]\tMean Loss : 6.090183\t time 0:00:55.845708:\n",
      "Train Epoch: 0 [45984/165807 (28%)]\tMean Loss : 6.091930\t time 0:00:56.383186:\n",
      "Train Epoch: 0 [47904/165807 (29%)]\tMean Loss : 6.060651\t time 0:00:55.375513:\n",
      "Train Epoch: 0 [49824/165807 (30%)]\tMean Loss : 6.057025\t time 0:00:55.850195:\n",
      "Train Epoch: 0 [51744/165807 (31%)]\tMean Loss : 6.062928\t time 0:00:55.823180:\n",
      "Train Epoch: 0 [53664/165807 (32%)]\tMean Loss : 6.048362\t time 0:00:55.962249:\n",
      "Train Epoch: 0 [55584/165807 (34%)]\tMean Loss : 6.037099\t time 0:00:56.277915:\n",
      "Train Epoch: 0 [57504/165807 (35%)]\tMean Loss : 6.029491\t time 0:00:56.311794:\n",
      "Train Epoch: 0 [59424/165807 (36%)]\tMean Loss : 6.020585\t time 0:00:56.336700:\n",
      "Train Epoch: 0 [61344/165807 (37%)]\tMean Loss : 6.037608\t time 0:00:56.247875:\n",
      "Train Epoch: 0 [63264/165807 (38%)]\tMean Loss : 6.023317\t time 0:00:55.758170:\n",
      "Train Epoch: 0 [65184/165807 (39%)]\tMean Loss : 6.020204\t time 0:00:55.461229:\n",
      "Train Epoch: 0 [67104/165807 (40%)]\tMean Loss : 6.022068\t time 0:00:56.396704:\n",
      "Train Epoch: 0 [69024/165807 (42%)]\tMean Loss : 6.029381\t time 0:00:57.025986:\n",
      "Train Epoch: 0 [70944/165807 (43%)]\tMean Loss : 6.007894\t time 0:00:56.099739:\n",
      "Train Epoch: 0 [72864/165807 (44%)]\tMean Loss : 6.019903\t time 0:00:56.352842:\n",
      "Train Epoch: 0 [74784/165807 (45%)]\tMean Loss : 6.001473\t time 0:00:55.799076:\n",
      "Train Epoch: 0 [76704/165807 (46%)]\tMean Loss : 6.014301\t time 0:00:55.909687:\n",
      "Train Epoch: 0 [78624/165807 (47%)]\tMean Loss : 5.987881\t time 0:00:55.617503:\n",
      "Train Epoch: 0 [80544/165807 (49%)]\tMean Loss : 5.993734\t time 0:00:55.866630:\n",
      "Train Epoch: 0 [82464/165807 (50%)]\tMean Loss : 6.002519\t time 0:00:56.690024:\n",
      "Train Epoch: 0 [84384/165807 (51%)]\tMean Loss : 6.001308\t time 0:00:56.358225:\n",
      "Train Epoch: 0 [86304/165807 (52%)]\tMean Loss : 6.000927\t time 0:00:55.905158:\n",
      "Train Epoch: 0 [88224/165807 (53%)]\tMean Loss : 6.012768\t time 0:00:56.671346:\n",
      "Train Epoch: 0 [90144/165807 (54%)]\tMean Loss : 5.999818\t time 0:00:56.707789:\n",
      "Train Epoch: 0 [92064/165807 (55%)]\tMean Loss : 5.992827\t time 0:00:56.444626:\n",
      "Train Epoch: 0 [93984/165807 (57%)]\tMean Loss : 5.998127\t time 0:00:56.169055:\n",
      "Train Epoch: 0 [95904/165807 (58%)]\tMean Loss : 5.988266\t time 0:00:56.223045:\n",
      "Train Epoch: 0 [97824/165807 (59%)]\tMean Loss : 5.987202\t time 0:00:55.813216:\n",
      "Train Epoch: 0 [99744/165807 (60%)]\tMean Loss : 5.983670\t time 0:00:55.726141:\n",
      "Train Epoch: 0 [101664/165807 (61%)]\tMean Loss : 5.979203\t time 0:00:55.524192:\n",
      "Train Epoch: 0 [103584/165807 (62%)]\tMean Loss : 5.982473\t time 0:00:55.743499:\n",
      "Train Epoch: 0 [105504/165807 (64%)]\tMean Loss : 5.992496\t time 0:00:56.658027:\n",
      "Train Epoch: 0 [107424/165807 (65%)]\tMean Loss : 5.974000\t time 0:00:56.096278:\n",
      "Train Epoch: 0 [109344/165807 (66%)]\tMean Loss : 6.001190\t time 0:00:56.721035:\n",
      "Train Epoch: 0 [111264/165807 (67%)]\tMean Loss : 5.988247\t time 0:00:56.149641:\n",
      "Train Epoch: 0 [113184/165807 (68%)]\tMean Loss : 5.983085\t time 0:00:56.047628:\n",
      "Train Epoch: 0 [115104/165807 (69%)]\tMean Loss : 5.991617\t time 0:00:56.063524:\n",
      "Train Epoch: 0 [117024/165807 (71%)]\tMean Loss : 5.966448\t time 0:00:55.484463:\n",
      "Train Epoch: 0 [118944/165807 (72%)]\tMean Loss : 5.974615\t time 0:00:55.230292:\n",
      "Train Epoch: 0 [120864/165807 (73%)]\tMean Loss : 5.980966\t time 0:00:56.141834:\n",
      "Train Epoch: 0 [122784/165807 (74%)]\tMean Loss : 5.977714\t time 0:00:56.526128:\n",
      "Train Epoch: 0 [124704/165807 (75%)]\tMean Loss : 5.978609\t time 0:00:55.493481:\n",
      "Train Epoch: 0 [126624/165807 (76%)]\tMean Loss : 5.977495\t time 0:00:55.667656:\n",
      "Train Epoch: 0 [128544/165807 (77%)]\tMean Loss : 5.980916\t time 0:00:55.521281:\n",
      "Train Epoch: 0 [130464/165807 (79%)]\tMean Loss : 5.977472\t time 0:00:55.467311:\n",
      "Train Epoch: 0 [132384/165807 (80%)]\tMean Loss : 5.975587\t time 0:00:55.880206:\n",
      "Train Epoch: 0 [134304/165807 (81%)]\tMean Loss : 5.983867\t time 0:00:56.378757:\n",
      "Train Epoch: 0 [136224/165807 (82%)]\tMean Loss : 5.973546\t time 0:00:55.898293:\n",
      "Train Epoch: 0 [138144/165807 (83%)]\tMean Loss : 5.965167\t time 0:00:55.758821:\n",
      "Train Epoch: 0 [140064/165807 (84%)]\tMean Loss : 5.976462\t time 0:00:56.060124:\n",
      "Train Epoch: 0 [141984/165807 (86%)]\tMean Loss : 5.966035\t time 0:00:56.133292:\n",
      "Train Epoch: 0 [143904/165807 (87%)]\tMean Loss : 5.971419\t time 0:00:55.423473:\n",
      "Train Epoch: 0 [145824/165807 (88%)]\tMean Loss : 5.959646\t time 0:00:55.367025:\n",
      "Train Epoch: 0 [147744/165807 (89%)]\tMean Loss : 5.966609\t time 0:00:55.950489:\n",
      "Train Epoch: 0 [149664/165807 (90%)]\tMean Loss : 5.964207\t time 0:00:55.586612:\n",
      "Train Epoch: 0 [151584/165807 (91%)]\tMean Loss : 5.960803\t time 0:00:55.509998:\n",
      "Train Epoch: 0 [153504/165807 (93%)]\tMean Loss : 5.963252\t time 0:00:56.041041:\n",
      "Train Epoch: 0 [155424/165807 (94%)]\tMean Loss : 5.963092\t time 0:00:55.299614:\n",
      "Train Epoch: 0 [157344/165807 (95%)]\tMean Loss : 5.954609\t time 0:00:55.957600:\n",
      "Train Epoch: 0 [159264/165807 (96%)]\tMean Loss : 5.961657\t time 0:00:55.437644:\n",
      "Train Epoch: 0 [161184/165807 (97%)]\tMean Loss : 5.966741\t time 0:00:55.753472:\n",
      "Train Epoch: 0 [163104/165807 (98%)]\tMean Loss : 5.963992\t time 0:00:55.875621:\n",
      "Train Epoch: 0 [165024/165807 (99%)]\tMean Loss : 5.956570\t time 0:00:55.584419:\n",
      "\n",
      "Teacher forcing ratio: 0.95\n",
      "Training, Logging: Mean loss of previous 20 batches \n",
      "\n",
      "Train Epoch: 1 [1824/165807 (1%)]\tMean Loss : 5.997324\t time 0:00:56.010170:\n",
      "Train Epoch: 1 [3744/165807 (2%)]\tMean Loss : 6.001007\t time 0:00:55.969058:\n",
      "Train Epoch: 1 [5664/165807 (3%)]\tMean Loss : 5.997245\t time 0:00:55.996099:\n",
      "Train Epoch: 1 [7584/165807 (5%)]\tMean Loss : 5.996432\t time 0:00:56.063404:\n",
      "Train Epoch: 1 [9504/165807 (6%)]\tMean Loss : 5.998948\t time 0:00:55.628058:\n",
      "Train Epoch: 1 [11424/165807 (7%)]\tMean Loss : 6.012704\t time 0:00:56.595401:\n",
      "Train Epoch: 1 [13344/165807 (8%)]\tMean Loss : 5.999106\t time 0:00:56.160762:\n",
      "Train Epoch: 1 [15264/165807 (9%)]\tMean Loss : 5.998261\t time 0:00:55.882448:\n",
      "Train Epoch: 1 [17184/165807 (10%)]\tMean Loss : 5.995599\t time 0:00:55.714153:\n",
      "Train Epoch: 1 [19104/165807 (12%)]\tMean Loss : 5.989839\t time 0:00:55.278147:\n",
      "Train Epoch: 1 [21024/165807 (13%)]\tMean Loss : 5.990702\t time 0:00:55.656455:\n",
      "Train Epoch: 1 [22944/165807 (14%)]\tMean Loss : 6.003759\t time 0:00:55.776892:\n",
      "Train Epoch: 1 [24864/165807 (15%)]\tMean Loss : 6.001811\t time 0:00:55.815832:\n",
      "Train Epoch: 1 [26784/165807 (16%)]\tMean Loss : 5.987387\t time 0:00:55.813731:\n",
      "Train Epoch: 1 [28704/165807 (17%)]\tMean Loss : 5.991907\t time 0:00:55.665242:\n",
      "Train Epoch: 1 [30624/165807 (18%)]\tMean Loss : 5.994100\t time 0:00:55.660280:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [32544/165807 (20%)]\tMean Loss : 5.995000\t time 0:00:55.641545:\n",
      "Train Epoch: 1 [34464/165807 (21%)]\tMean Loss : 5.989332\t time 0:00:55.109312:\n",
      "Train Epoch: 1 [36384/165807 (22%)]\tMean Loss : 5.997731\t time 0:00:55.562200:\n",
      "Train Epoch: 1 [38304/165807 (23%)]\tMean Loss : 6.006486\t time 0:00:55.716408:\n",
      "Train Epoch: 1 [40224/165807 (24%)]\tMean Loss : 5.996071\t time 0:00:56.121093:\n",
      "Train Epoch: 1 [42144/165807 (25%)]\tMean Loss : 5.993966\t time 0:00:55.976035:\n",
      "Train Epoch: 1 [44064/165807 (27%)]\tMean Loss : 5.987737\t time 0:00:56.057923:\n",
      "Train Epoch: 1 [45984/165807 (28%)]\tMean Loss : 6.001422\t time 0:00:55.967177:\n",
      "Train Epoch: 1 [47904/165807 (29%)]\tMean Loss : 5.993463\t time 0:00:56.045572:\n",
      "Train Epoch: 1 [49824/165807 (30%)]\tMean Loss : 5.991308\t time 0:00:55.700651:\n"
     ]
    }
   ],
   "source": [
    "# optimizer = optim.ASGD(model.parameters(), lr=0.05)  # lr = 0.2 used in paper\n",
    "optimizer = optim.Adadelta(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "log_interval = 5\n",
    "print_interval = 20\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    scheduler.step()                                    # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.05, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
