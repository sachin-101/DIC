{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import torch\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:6\n"
     ]
    }
   ],
   "source": [
    "def preprocess(s):\n",
    "    s = s.lower().replace('\\n', '')\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "# Used when each sentence is in a separate text file\n",
    "def make_train_df(root_dir, dataset, file_ext, csv_file):\n",
    "    dataset_dir = os.path.join(root_dir, dataset)\n",
    "    data = []\n",
    "    files = os.listdir(dataset_dir)\n",
    "    for f in files:\n",
    "        if '.txt' in f:\n",
    "            with open(os.path.join(dataset_dir, f), 'r') as text_file:\n",
    "                data_list = text_file.readlines()\n",
    "            for example in data_list:\n",
    "                path = os.path.join(dataset, str(example.split(' ')[0])) + file_ext   \n",
    "                sent = preprocess(str(' '.join(example.split(' ')[1:])))\n",
    "                data.append((path, sent))\n",
    "\n",
    "    data_df = pd.DataFrame(data, columns=['path', 'sent'])\n",
    "    data_df.to_csv(os.path.join(root_dir, csv_file), header=None)\n",
    "    return data_df\n",
    "\n",
    "\n",
    "root_dir = '../../../Dataset/LibriSpeech'\n",
    "DEVICE = torch.device('cuda:6') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 281241\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0001.flac</td>\n",
       "      <td>that had its source away back in the woods of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0002.flac</td>\n",
       "      <td>for not even a brook could run past missus rac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dataset_100/103-1240-0003.flac</td>\n",
       "      <td>and that if she noticed anything odd or out of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dataset_100/103-1240-0004.flac</td>\n",
       "      <td>but missus rachel lynde was one of those capab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0001.flac   \n",
       "2  dataset_100/103-1240-0002.flac   \n",
       "3  dataset_100/103-1240-0003.flac   \n",
       "4  dataset_100/103-1240-0004.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  that had its source away back in the woods of ...  \n",
       "2  for not even a brook could run past missus rac...  \n",
       "3  and that if she noticed anything odd or out of...  \n",
       "4  but missus rachel lynde was one of those capab...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_100 = pd.read_csv(os.path.join(root_dir, 'train_100.csv'), header=None, names=['path', 'sent'])\n",
    "train_360 = pd.read_csv(os.path.join(root_dir, 'train_360.csv'), header=None, names=['path', 'sent'])\n",
    "train_500 = pd.read_csv(os.path.join(root_dir, 'train_500.csv'), header=None, names=['path', 'sent'])\n",
    "\n",
    "# combine all of them\n",
    "train_df = pd.concat([train_100, train_360, train_500])\n",
    "print(\"Number of training examples:\", train_df.shape[0])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Removing very large sentences\n",
    "# def remove_long_sent(train_df, max_len):\n",
    "#     data = []\n",
    "#     for idx in range(train_df.shape[0]):\n",
    "#         path, sent = train_df.iloc[idx]\n",
    "#         if len(sent) > max_len:\n",
    "#             continue\n",
    "#         data.append((path, sent))\n",
    "#     return pd.DataFrame(data, columns=['path', 'sent'])\n",
    "\n",
    "# max_len = 225\n",
    "# train_df = remove_long_sent(train_df, max_len)\n",
    "# print(\"Number of training examples:\",  train_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 219709\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dataset_100/103-1240-0000.flac</td>\n",
       "      <td>chapter one missus rachel lynde is surprised m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dataset_100/103-1240-0006.flac</td>\n",
       "      <td>as avonlea housekeepers were wont to tell in a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dataset_100/103-1240-0009.flac</td>\n",
       "      <td>missus rachel knew that he ought because she h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             path  \\\n",
       "0  dataset_100/103-1240-0000.flac   \n",
       "1  dataset_100/103-1240-0006.flac   \n",
       "2  dataset_100/103-1240-0009.flac   \n",
       "\n",
       "                                                sent  \n",
       "0  chapter one missus rachel lynde is surprised m...  \n",
       "1  as avonlea housekeepers were wont to tell in a...  \n",
       "2  missus rachel knew that he ought because she h...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save train_df\n",
    "#train_df.to_csv(os.path.join(root_dir, 'total_train.csv'), header=None)\n",
    "# load train_df\n",
    "train_df = pd.read_csv(os.path.join(root_dir, 'total_train.csv'), header=None, names=['path', 'sent'])\n",
    "print(\"Number of training examples:\",  train_df.shape[0])\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders and hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chars 32\n"
     ]
    }
   ],
   "source": [
    "def get_chars(include_digits=True):\n",
    "    if include_digits:\n",
    "        chars = ['<sos>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', \\\n",
    "                 'x', 'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    else:\n",
    "        chars = ['<sos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \\\n",
    "                'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', \\\n",
    "                'y', 'z', ' ', \"'\", '<eos>', '<pad>', '<unk>']\n",
    "    print('Number of chars', len(chars))\n",
    "    return chars\n",
    "\n",
    "\n",
    "chars = get_chars(include_digits=False)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=32, \n",
    "                               shuffle=True, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "save_dir = os.path.join('trained_models_librispeech', f'Training_new_loss_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (embedding): Embedding(32, 40)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "      (linear2): Linear(in_features=1280, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(2088, 512)\n",
       "    (post_lstm_cell): LSTMCell(2560, 512)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 512  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 512\n",
    "embed_dim = 40\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size, \n",
    "              'info':'Starting with 1.0 tf_ratio and 0.025 decay.'}\n",
    "\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [1248/219709 (1%)]\tMean Loss : 3.454820\t time 0:00:53.007265:\n",
      "Train Epoch: 0 [2528/219709 (1%)]\tMean Loss : 3.452135\t time 0:00:51.452945:\n",
      "Train Epoch: 0 [3808/219709 (2%)]\tMean Loss : 3.449643\t time 0:00:50.902332:\n",
      "Train Epoch: 0 [5088/219709 (2%)]\tMean Loss : 3.447662\t time 0:00:52.340084:\n",
      "Train Epoch: 0 [6368/219709 (3%)]\tMean Loss : 3.445289\t time 0:00:51.927844:\n",
      "Train Epoch: 0 [7648/219709 (3%)]\tMean Loss : 3.443196\t time 0:00:51.917668:\n",
      "Train Epoch: 0 [8928/219709 (4%)]\tMean Loss : 3.437341\t time 0:00:51.825668:\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.ASGD(model.parameters(), lr=0.001)  # lr = 0.2 used in paper\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.98)\n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, time)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    scheduler.step()                                    # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.025, 0.7)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    for t in out:\n",
    "        lol = t.max(dim=1)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, root_dir, char_to_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
