{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "NAME = 'AMSGrad_pat_10_cool_5' # helps to differentiate between various training instances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from models.las_model.data import SpeechDataset, AudioDataLoader\n",
    "from models.las_model.listener import Listener\n",
    "from models.las_model.attend_and_spell import AttendAndSpell\n",
    "from models.las_model.seq2seq import Seq2Seq\n",
    "# from models.las_model.utils import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training examples: 149569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000f47c22.flac</td>\n",
       "      <td>මහවැලි ගඟට ගොස් ආපසු එන ගමනේදී</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000101700f.flac</td>\n",
       "      <td>උන්වහන්සේ කපාපු</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000107b539.flac</td>\n",
       "      <td>එය එතනින් අවසන් නොවී</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016825d3.flac</td>\n",
       "      <td>සිතින් අයහපතෙහි හැසිරීම නිසයි</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002205a57.flac</td>\n",
       "      <td>ඊට අවසරයද හිමිවූ බව ඇය කියන්නීය</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              path                             sent\n",
       "0  0000f47c22.flac   මහවැලි ගඟට ගොස් ආපසු එන ගමනේදී\n",
       "1  000101700f.flac                  උන්වහන්සේ කපාපු\n",
       "2  000107b539.flac             එය එතනින් අවසන් නොවී\n",
       "3  00016825d3.flac    සිතින් අයහපතෙහි හැසිරීම නිසයි\n",
       "4  0002205a57.flac  ඊට අවසරයද හිමිවූ බව ඇය කියන්නීය"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../../Dataset/Sinhala'\n",
    "\n",
    "remove_chars = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \\\n",
    "                'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',  'x', 'y', 'z', \\\n",
    "                '“', '”', '\\u200b', '\\u200c', '\\u200d', 'µ', '\\x94', '»', 'ª', '’', '‘']\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.replace('\\n', '')  # remove '\\n'\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "\n",
    "# reading the main transcript\n",
    "lines = []\n",
    "with open(os.path.join(data_dir, 'utt_spk_text.tsv'), 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "examples = []\n",
    "for l in lines:\n",
    "    append = True\n",
    "    id_, _, sent = l.split('\\t')\n",
    "    sent = preprocess(sent)\n",
    "    for c in sent:\n",
    "        if c in remove_chars:  # removing sentences with eng_chars\n",
    "            append = False\n",
    "            break\n",
    "    if append:\n",
    "        examples.append((id_+'.flac', sent))\n",
    "\n",
    "data_df = pd.DataFrame(examples, columns=['path', 'sent'])\n",
    "data_df.to_csv(os.path.join(data_dir, 'data_df.csv')) # save\n",
    "print(\"Number of Training examples:\", data_df.shape[0])\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried removing all the unnecessary characters from the dataset. The others will be replaced by unknown token, while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training example: (148073, 2)\n",
      "Num validation example (1496, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30931</th>\n",
       "      <td>354cb92d3a.flac</td>\n",
       "      <td>ඇවිල්ල මහන්සි හින්ද</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97697</th>\n",
       "      <td>a7957688f2.flac</td>\n",
       "      <td>ඒවායේ නියම කරන</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46228</th>\n",
       "      <td>4fa1969ee0.flac</td>\n",
       "      <td>ඉතින් මචන් ලක්ෂයක් නොවේ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37775</th>\n",
       "      <td>412e85986b.flac</td>\n",
       "      <td>ඔවුන් තුළ ඇතිවන්නේ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105650</th>\n",
       "      <td>b5683e2366.flac</td>\n",
       "      <td>ඊට එරෙහිව දකුණේ සිංහල තරුණ කැරලි මගින් ඇති කරන</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   path                                            sent\n",
       "30931   354cb92d3a.flac                             ඇවිල්ල මහන්සි හින්ද\n",
       "97697   a7957688f2.flac                                  ඒවායේ නියම කරන\n",
       "46228   4fa1969ee0.flac                         ඉතින් මචන් ලක්ෂයක් නොවේ\n",
       "37775   412e85986b.flac                              ඔවුන් තුළ ඇතිවන්නේ\n",
       "105650  b5683e2366.flac  ඊට එරෙහිව දකුණේ සිංහල තරුණ කැරලි මගින් ඇති කරන"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = pd.read_csv(os.path.join(data_dir, 'data_df.csv'), usecols=['path', 'sent'])\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.01)\n",
    "print(\"Num training example:\", train_df.shape)\n",
    "print(\"Num validation example\", val_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 82\n",
      "['<pad>', '<unk>', '<sos>', '<eos>', 'ඇ', 'ව', 'ි', 'ල', '්', ' ', 'ම', 'හ', 'න', 'ස', 'ද', 'ඒ', 'ා', 'ය', 'ේ', 'ක', 'ර', 'ඉ', 'ත', 'ච', 'ෂ', 'ො', 'ඔ', 'ු', 'ළ', 'ඊ', 'ට', 'එ', 'ෙ', 'ණ', 'ං', 'ැ', 'ග', 'අ', 'ප', 'ධ', 'ී', 'ජ', 'ශ', 'ඞ', 'බ', 'ඳ', 'ඩ', 'ඕ', 'ෑ', 'ආ', 'ඥ', 'ූ', 'උ', 'ෞ', 'ෝ', 'ඛ', 'ථ', 'ඟ', 'භ', 'ෘ', 'ඹ', '–', 'ඬ', 'ඝ', 'ෆ', 'ඨ', 'ඈ', 'ඡ', 'ඓ', 'ෛ', 'ඌ', 'ඤ', 'ඃ', 'ඖ', 'ඵ', 'ෲ', 'ඣ', 'ඍ', 'ෳ', 'ඪ', 'ෟ', '෴']\n"
     ]
    }
   ],
   "source": [
    "def get_chars(train_df):\n",
    "    chars = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    for idx in range(train_df.shape[0]):\n",
    "        id_, sent = train_df.iloc[idx]\n",
    "        for c in sent:\n",
    "            if c not in chars:\n",
    "                chars.append(c)\n",
    "    return chars\n",
    "    \n",
    "\n",
    "chars = get_chars(train_df)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']\n",
    "unk_token = char_to_token['<unk>']\n",
    "\n",
    "print(\"Number of characters:\", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (embedding): Embedding(82, 50)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=3200, out_features=1600, bias=True)\n",
       "      (linear2): Linear(in_features=1600, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(2610, 640)\n",
       "    (post_lstm_cell): LSTMCell(3200, 640)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=640, out_features=82, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 640  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "layer_norm = True   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 640\n",
    "embed_dim = 50\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size}\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()\n",
    "\n",
    "# load = False\n",
    "# if load:\n",
    "#     saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "#     model.load_state_dict(torch.load(saved_file))\n",
    "#     start_epoch = int(saved_file[-1]) + 1\n",
    "#     time = os.listdir(tensorboard_dir)[-1]  # use the last one \n",
    "\n",
    "time = str(datetime.datetime.now())\n",
    "save_dir = os.path.join('trained_models', f'{NAME}_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "\n",
    "train_dataset = SpeechDataset(train_df, data_dir, char_to_token, n_fft=1024, hop_length=256)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=64, \n",
    "                               shuffle=True, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, \n",
    "          print_interval, writer=None, log_interval=-1, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    print(f'Training, Logging: Mean loss of previous {print_interval} batches \\n')\n",
    "    \n",
    "    running_loss = []\n",
    "    date1 = datetime.datetime.now()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        loss, _ = model(data, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.detach().item())    # update running loss\n",
    "        \n",
    "        # writing to console after print_interval batches\n",
    "        if (batch_idx+1) % print_interval == 0:\n",
    "            date2 = datetime.datetime.now()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tMean Loss : {:.6f}\\t lr {}\\t time {}:'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), \n",
    "                np.mean(running_loss[-print_interval:]), \n",
    "                optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "                date2 - date1))\n",
    "            date1 = date2\n",
    "            if scheduler:\n",
    "                scheduler.step(np.mean(running_loss[-print_interval:]))\n",
    "\n",
    "        # Writing to tensorboard\n",
    "        if (batch_idx+1) % log_interval == 0:\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_loader) + batch_idx\n",
    "                writer.add_scalar('Loss', np.mean(running_loss[-log_interval:]), global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models/AMSGrad_pat_10_cool_5_2019-12-30 22:04:33.443664\n",
      "Training, Logging: Mean loss of previous 50 batches \n",
      "\n",
      "Train Epoch: 0 [3136/148073 (2%)]\tMean Loss : 4.392427\t lr 0.001\t time 0:00:50.135852:\n",
      "Train Epoch: 0 [6336/148073 (4%)]\tMean Loss : 4.383052\t lr 0.001\t time 0:00:51.753693:\n",
      "Train Epoch: 0 [9536/148073 (6%)]\tMean Loss : 4.371854\t lr 0.001\t time 0:00:56.116892:\n",
      "Train Epoch: 0 [12736/148073 (9%)]\tMean Loss : 4.370805\t lr 0.001\t time 0:00:55.172427:\n",
      "Train Epoch: 0 [15936/148073 (11%)]\tMean Loss : 4.354231\t lr 0.001\t time 0:00:54.651080:\n",
      "Train Epoch: 0 [19136/148073 (13%)]\tMean Loss : 4.341693\t lr 0.001\t time 0:00:52.592129:\n",
      "Train Epoch: 0 [22336/148073 (15%)]\tMean Loss : 4.330747\t lr 0.001\t time 0:00:55.284796:\n",
      "Train Epoch: 0 [25536/148073 (17%)]\tMean Loss : 4.317935\t lr 0.001\t time 0:00:57.433644:\n",
      "Train Epoch: 0 [28736/148073 (19%)]\tMean Loss : 4.303819\t lr 0.001\t time 0:00:54.293969:\n",
      "Train Epoch: 0 [31936/148073 (22%)]\tMean Loss : 4.290679\t lr 0.001\t time 0:00:57.450048:\n",
      "Train Epoch: 0 [35136/148073 (24%)]\tMean Loss : 4.273007\t lr 0.001\t time 0:00:59.586808:\n",
      "Train Epoch: 0 [38336/148073 (26%)]\tMean Loss : 4.258775\t lr 0.001\t time 0:00:56.902483:\n",
      "Train Epoch: 0 [41536/148073 (28%)]\tMean Loss : 4.238376\t lr 0.001\t time 0:00:58.281326:\n",
      "Train Epoch: 0 [44736/148073 (30%)]\tMean Loss : 4.217027\t lr 0.001\t time 0:00:56.233222:\n",
      "Train Epoch: 0 [47936/148073 (32%)]\tMean Loss : 4.198964\t lr 0.001\t time 0:01:01.430997:\n",
      "Train Epoch: 0 [51136/148073 (35%)]\tMean Loss : 4.184207\t lr 0.001\t time 0:00:54.012030:\n",
      "Train Epoch: 0 [54336/148073 (37%)]\tMean Loss : 4.163051\t lr 0.001\t time 0:00:58.522524:\n",
      "Train Epoch: 0 [57536/148073 (39%)]\tMean Loss : 4.151931\t lr 0.001\t time 0:00:55.670595:\n",
      "Train Epoch: 0 [60736/148073 (41%)]\tMean Loss : 4.154884\t lr 0.001\t time 0:01:00.890180:\n",
      "Train Epoch: 0 [63936/148073 (43%)]\tMean Loss : 4.122843\t lr 0.001\t time 0:00:56.012613:\n",
      "Train Epoch: 0 [67136/148073 (45%)]\tMean Loss : 4.116782\t lr 0.001\t time 0:00:52.944798:\n",
      "Train Epoch: 0 [70336/148073 (48%)]\tMean Loss : 4.115039\t lr 0.001\t time 0:00:58.865014:\n",
      "Train Epoch: 0 [73536/148073 (50%)]\tMean Loss : 4.101063\t lr 0.001\t time 0:00:54.650492:\n",
      "Train Epoch: 0 [76736/148073 (52%)]\tMean Loss : 4.088801\t lr 0.001\t time 0:00:57.753177:\n",
      "Train Epoch: 0 [79936/148073 (54%)]\tMean Loss : 4.070795\t lr 0.001\t time 0:00:51.559724:\n",
      "Train Epoch: 0 [83136/148073 (56%)]\tMean Loss : 4.056708\t lr 0.001\t time 0:00:54.498908:\n",
      "Train Epoch: 0 [86336/148073 (58%)]\tMean Loss : 4.054215\t lr 0.001\t time 0:00:56.374046:\n",
      "Train Epoch: 0 [89536/148073 (60%)]\tMean Loss : 4.025926\t lr 0.001\t time 0:00:53.407377:\n",
      "Train Epoch: 0 [92736/148073 (63%)]\tMean Loss : 4.031109\t lr 0.001\t time 0:00:58.418193:\n",
      "Train Epoch: 0 [95936/148073 (65%)]\tMean Loss : 4.004171\t lr 0.001\t time 0:00:56.828156:\n",
      "Train Epoch: 0 [99136/148073 (67%)]\tMean Loss : 4.032092\t lr 0.001\t time 0:00:55.566360:\n",
      "Train Epoch: 0 [102336/148073 (69%)]\tMean Loss : 4.023772\t lr 0.001\t time 0:00:53.665630:\n",
      "Train Epoch: 0 [105536/148073 (71%)]\tMean Loss : 4.004429\t lr 0.001\t time 0:01:00.426992:\n",
      "Train Epoch: 0 [108736/148073 (73%)]\tMean Loss : 3.996559\t lr 0.001\t time 0:00:56.678518:\n",
      "Train Epoch: 0 [111936/148073 (76%)]\tMean Loss : 3.980021\t lr 0.001\t time 0:00:59.569637:\n",
      "Train Epoch: 0 [115136/148073 (78%)]\tMean Loss : 3.971016\t lr 0.001\t time 0:01:00.008817:\n",
      "Train Epoch: 0 [118336/148073 (80%)]\tMean Loss : 3.965307\t lr 0.001\t time 0:00:56.108645:\n",
      "Train Epoch: 0 [121536/148073 (82%)]\tMean Loss : 3.959432\t lr 0.001\t time 0:00:51.794916:\n",
      "Train Epoch: 0 [124736/148073 (84%)]\tMean Loss : 3.954101\t lr 0.001\t time 0:00:55.209865:\n",
      "Train Epoch: 0 [127936/148073 (86%)]\tMean Loss : 3.950239\t lr 0.001\t time 0:00:53.198596:\n",
      "Train Epoch: 0 [131136/148073 (89%)]\tMean Loss : 3.949440\t lr 0.001\t time 0:00:53.763275:\n",
      "Train Epoch: 0 [134336/148073 (91%)]\tMean Loss : 3.941139\t lr 0.001\t time 0:00:54.638278:\n",
      "Train Epoch: 0 [137536/148073 (93%)]\tMean Loss : 3.932278\t lr 0.001\t time 0:00:52.527351:\n",
      "Train Epoch: 0 [140736/148073 (95%)]\tMean Loss : 3.929823\t lr 0.001\t time 0:00:55.091792:\n",
      "Train Epoch: 0 [143936/148073 (97%)]\tMean Loss : 3.912744\t lr 0.001\t time 0:00:54.146329:\n",
      "Train Epoch: 0 [147136/148073 (99%)]\tMean Loss : 3.909796\t lr 0.001\t time 0:00:54.804782:\n",
      "Training, Logging: Mean loss of previous 50 batches \n",
      "\n",
      "Train Epoch: 1 [3136/148073 (2%)]\tMean Loss : 3.901577\t lr 0.001\t time 0:00:57.393095:\n",
      "Train Epoch: 1 [6336/148073 (4%)]\tMean Loss : 3.894089\t lr 0.001\t time 0:00:54.447916:\n",
      "Train Epoch: 1 [9536/148073 (6%)]\tMean Loss : 3.912192\t lr 0.001\t time 0:00:51.812217:\n",
      "Train Epoch: 1 [12736/148073 (9%)]\tMean Loss : 3.887204\t lr 0.001\t time 0:00:58.280100:\n",
      "Train Epoch: 1 [15936/148073 (11%)]\tMean Loss : 3.889592\t lr 0.001\t time 0:00:54.690635:\n",
      "Train Epoch: 1 [19136/148073 (13%)]\tMean Loss : 3.881278\t lr 0.001\t time 0:00:54.345988:\n",
      "Train Epoch: 1 [22336/148073 (15%)]\tMean Loss : 3.879850\t lr 0.001\t time 0:00:59.512832:\n",
      "Train Epoch: 1 [25536/148073 (17%)]\tMean Loss : 3.880008\t lr 0.001\t time 0:00:54.973517:\n",
      "Train Epoch: 1 [28736/148073 (19%)]\tMean Loss : 3.876781\t lr 0.001\t time 0:00:53.203639:\n",
      "Train Epoch: 1 [31936/148073 (22%)]\tMean Loss : 3.880159\t lr 0.001\t time 0:00:56.233466:\n",
      "Train Epoch: 1 [35136/148073 (24%)]\tMean Loss : 3.876138\t lr 0.001\t time 0:00:54.666999:\n",
      "Train Epoch: 1 [38336/148073 (26%)]\tMean Loss : 3.864308\t lr 0.001\t time 0:00:57.441296:\n",
      "Train Epoch: 1 [41536/148073 (28%)]\tMean Loss : 3.862742\t lr 0.001\t time 0:00:57.080083:\n",
      "Train Epoch: 1 [44736/148073 (30%)]\tMean Loss : 3.857577\t lr 0.001\t time 0:00:54.241118:\n",
      "Train Epoch: 1 [47936/148073 (32%)]\tMean Loss : 3.873614\t lr 0.001\t time 0:00:55.833731:\n",
      "Train Epoch: 1 [51136/148073 (35%)]\tMean Loss : 3.864002\t lr 0.001\t time 0:00:58.085760:\n",
      "Train Epoch: 1 [54336/148073 (37%)]\tMean Loss : 3.856213\t lr 0.001\t time 0:00:56.281464:\n",
      "Train Epoch: 1 [57536/148073 (39%)]\tMean Loss : 3.866988\t lr 0.001\t time 0:00:54.140733:\n",
      "Train Epoch: 1 [60736/148073 (41%)]\tMean Loss : 3.861515\t lr 0.001\t time 0:00:52.613862:\n",
      "Train Epoch: 1 [63936/148073 (43%)]\tMean Loss : 3.855820\t lr 0.001\t time 0:00:56.322192:\n",
      "Train Epoch: 1 [67136/148073 (45%)]\tMean Loss : 3.856824\t lr 0.001\t time 0:00:57.276583:\n",
      "Train Epoch: 1 [70336/148073 (48%)]\tMean Loss : 3.854932\t lr 0.001\t time 0:00:55.676679:\n",
      "Train Epoch: 1 [73536/148073 (50%)]\tMean Loss : 3.859632\t lr 0.001\t time 0:00:54.543372:\n",
      "Train Epoch: 1 [76736/148073 (52%)]\tMean Loss : 3.853524\t lr 0.001\t time 0:00:54.931723:\n",
      "Train Epoch: 1 [79936/148073 (54%)]\tMean Loss : 3.855929\t lr 0.001\t time 0:00:53.537370:\n",
      "Train Epoch: 1 [83136/148073 (56%)]\tMean Loss : 3.856651\t lr 0.001\t time 0:00:54.162777:\n",
      "Train Epoch: 1 [86336/148073 (58%)]\tMean Loss : 3.848066\t lr 0.001\t time 0:00:55.535120:\n",
      "Train Epoch: 1 [89536/148073 (60%)]\tMean Loss : 3.850822\t lr 0.001\t time 0:00:59.251143:\n",
      "Train Epoch: 1 [92736/148073 (63%)]\tMean Loss : 3.851272\t lr 0.001\t time 0:00:57.710214:\n",
      "Train Epoch: 1 [95936/148073 (65%)]\tMean Loss : 3.849235\t lr 0.001\t time 0:00:53.769886:\n",
      "Train Epoch: 1 [99136/148073 (67%)]\tMean Loss : 3.847932\t lr 0.001\t time 0:00:55.840501:\n",
      "Train Epoch: 1 [102336/148073 (69%)]\tMean Loss : 3.854292\t lr 0.001\t time 0:00:51.034900:\n",
      "Train Epoch: 1 [105536/148073 (71%)]\tMean Loss : 3.837342\t lr 0.001\t time 0:00:57.326890:\n",
      "Train Epoch: 1 [108736/148073 (73%)]\tMean Loss : 3.839480\t lr 0.001\t time 0:00:54.257826:\n",
      "Train Epoch: 1 [111936/148073 (76%)]\tMean Loss : 3.852918\t lr 0.001\t time 0:00:53.700875:\n",
      "Train Epoch: 1 [115136/148073 (78%)]\tMean Loss : 3.839659\t lr 0.001\t time 0:00:58.570675:\n",
      "Train Epoch: 1 [118336/148073 (80%)]\tMean Loss : 3.844819\t lr 0.001\t time 0:00:52.517385:\n",
      "Train Epoch: 1 [121536/148073 (82%)]\tMean Loss : 3.832925\t lr 0.001\t time 0:00:59.302197:\n",
      "Train Epoch: 1 [124736/148073 (84%)]\tMean Loss : 3.830140\t lr 0.001\t time 0:00:56.889529:\n",
      "Train Epoch: 1 [127936/148073 (86%)]\tMean Loss : 3.841490\t lr 0.001\t time 0:00:56.865525:\n",
      "Train Epoch: 1 [131136/148073 (89%)]\tMean Loss : 3.855261\t lr 0.001\t time 0:00:56.306218:\n",
      "Train Epoch: 1 [134336/148073 (91%)]\tMean Loss : 3.844535\t lr 0.001\t time 0:00:55.839503:\n",
      "Train Epoch: 1 [137536/148073 (93%)]\tMean Loss : 3.828074\t lr 0.001\t time 0:01:02.041097:\n",
      "Train Epoch: 1 [140736/148073 (95%)]\tMean Loss : 3.832581\t lr 0.001\t time 0:00:57.012985:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [143936/148073 (97%)]\tMean Loss : 3.850213\t lr 0.001\t time 0:00:53.522616:\n",
      "Train Epoch: 1 [147136/148073 (99%)]\tMean Loss : 3.847200\t lr 0.001\t time 0:00:52.357333:\n",
      "Training, Logging: Mean loss of previous 50 batches \n",
      "\n",
      "Train Epoch: 2 [3136/148073 (2%)]\tMean Loss : 3.850514\t lr 0.001\t time 0:00:55.271588:\n",
      "Train Epoch: 2 [6336/148073 (4%)]\tMean Loss : 3.846132\t lr 0.001\t time 0:00:52.760615:\n",
      "Train Epoch: 2 [9536/148073 (6%)]\tMean Loss : 3.841829\t lr 0.001\t time 0:00:52.745075:\n",
      "Train Epoch: 2 [12736/148073 (9%)]\tMean Loss : 3.842461\t lr 0.001\t time 0:00:54.310358:\n",
      "Train Epoch: 2 [15936/148073 (11%)]\tMean Loss : 3.837222\t lr 0.001\t time 0:00:55.093892:\n",
      "Train Epoch: 2 [19136/148073 (13%)]\tMean Loss : 3.832522\t lr 0.001\t time 0:00:54.392777:\n",
      "Train Epoch: 2 [22336/148073 (15%)]\tMean Loss : 3.841203\t lr 0.001\t time 0:00:58.517745:\n",
      "Train Epoch: 2 [25536/148073 (17%)]\tMean Loss : 3.839534\t lr 0.001\t time 0:00:53.383989:\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Train Epoch: 2 [28736/148073 (19%)]\tMean Loss : 3.847051\t lr 0.0001\t time 0:00:54.252606:\n",
      "Train Epoch: 2 [31936/148073 (22%)]\tMean Loss : 3.832142\t lr 0.0001\t time 0:01:00.403004:\n",
      "Train Epoch: 2 [35136/148073 (24%)]\tMean Loss : 3.845154\t lr 0.0001\t time 0:00:54.286771:\n",
      "Train Epoch: 2 [38336/148073 (26%)]\tMean Loss : 3.843768\t lr 0.0001\t time 0:00:57.902805:\n",
      "Train Epoch: 2 [41536/148073 (28%)]\tMean Loss : 3.835809\t lr 0.0001\t time 0:00:52.063688:\n",
      "Train Epoch: 2 [44736/148073 (30%)]\tMean Loss : 3.829380\t lr 0.0001\t time 0:01:00.052378:\n",
      "Train Epoch: 2 [47936/148073 (32%)]\tMean Loss : 3.837012\t lr 0.0001\t time 0:00:57.328600:\n",
      "Train Epoch: 2 [51136/148073 (35%)]\tMean Loss : 3.830841\t lr 0.0001\t time 0:01:00.283634:\n",
      "Train Epoch: 2 [54336/148073 (37%)]\tMean Loss : 3.838513\t lr 0.0001\t time 0:00:59.052626:\n",
      "Train Epoch: 2 [57536/148073 (39%)]\tMean Loss : 3.839118\t lr 0.0001\t time 0:00:57.248046:\n",
      "Train Epoch: 2 [60736/148073 (41%)]\tMean Loss : 3.828142\t lr 0.0001\t time 0:00:56.767710:\n",
      "Train Epoch: 2 [63936/148073 (43%)]\tMean Loss : 3.845359\t lr 0.0001\t time 0:00:55.924469:\n",
      "Train Epoch: 2 [67136/148073 (45%)]\tMean Loss : 3.837303\t lr 0.0001\t time 0:00:56.691956:\n",
      "Train Epoch: 2 [70336/148073 (48%)]\tMean Loss : 3.838411\t lr 0.0001\t time 0:00:54.403939:\n",
      "Train Epoch: 2 [73536/148073 (50%)]\tMean Loss : 3.840991\t lr 0.0001\t time 0:00:57.485940:\n",
      "Train Epoch: 2 [76736/148073 (52%)]\tMean Loss : 3.845817\t lr 0.0001\t time 0:00:58.052905:\n",
      "Epoch   115: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Train Epoch: 2 [79936/148073 (54%)]\tMean Loss : 3.836463\t lr 1e-05\t time 0:00:56.999267:\n",
      "Train Epoch: 2 [83136/148073 (56%)]\tMean Loss : 3.834935\t lr 1e-05\t time 0:00:56.185206:\n",
      "Train Epoch: 2 [86336/148073 (58%)]\tMean Loss : 3.841550\t lr 1e-05\t time 0:00:59.061612:\n",
      "Train Epoch: 2 [89536/148073 (60%)]\tMean Loss : 3.829769\t lr 1e-05\t time 0:00:55.051468:\n",
      "Train Epoch: 2 [92736/148073 (63%)]\tMean Loss : 3.834970\t lr 1e-05\t time 0:00:57.990881:\n",
      "Train Epoch: 2 [95936/148073 (65%)]\tMean Loss : 3.835243\t lr 1e-05\t time 0:00:56.887747:\n",
      "Train Epoch: 2 [99136/148073 (67%)]\tMean Loss : 3.843190\t lr 1e-05\t time 0:00:51.990674:\n",
      "Train Epoch: 2 [102336/148073 (69%)]\tMean Loss : 3.836843\t lr 1e-05\t time 0:00:53.375835:\n",
      "Train Epoch: 2 [105536/148073 (71%)]\tMean Loss : 3.820899\t lr 1e-05\t time 0:00:58.397336:\n",
      "Train Epoch: 2 [108736/148073 (73%)]\tMean Loss : 3.840183\t lr 1e-05\t time 0:00:51.357342:\n",
      "Train Epoch: 2 [111936/148073 (76%)]\tMean Loss : 3.843557\t lr 1e-05\t time 0:00:53.941178:\n",
      "Train Epoch: 2 [115136/148073 (78%)]\tMean Loss : 3.845625\t lr 1e-05\t time 0:00:55.201909:\n",
      "Train Epoch: 2 [118336/148073 (80%)]\tMean Loss : 3.838592\t lr 1e-05\t time 0:00:56.719752:\n",
      "Train Epoch: 2 [121536/148073 (82%)]\tMean Loss : 3.828870\t lr 1e-05\t time 0:01:01.653224:\n",
      "Train Epoch: 2 [124736/148073 (84%)]\tMean Loss : 3.843126\t lr 1e-05\t time 0:00:53.997566:\n",
      "Train Epoch: 2 [127936/148073 (86%)]\tMean Loss : 3.841020\t lr 1e-05\t time 0:00:56.296879:\n",
      "Train Epoch: 2 [131136/148073 (89%)]\tMean Loss : 3.830778\t lr 1e-05\t time 0:00:55.376424:\n",
      "Train Epoch: 2 [134336/148073 (91%)]\tMean Loss : 3.845167\t lr 1e-05\t time 0:00:54.293888:\n",
      "Train Epoch: 2 [137536/148073 (93%)]\tMean Loss : 3.841832\t lr 1e-05\t time 0:00:58.945655:\n",
      "Train Epoch: 2 [140736/148073 (95%)]\tMean Loss : 3.836832\t lr 1e-05\t time 0:00:56.277576:\n",
      "Epoch   135: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Train Epoch: 2 [143936/148073 (97%)]\tMean Loss : 3.819304\t lr 1.0000000000000002e-06\t time 0:00:59.556577:\n",
      "Train Epoch: 2 [147136/148073 (99%)]\tMean Loss : 3.826917\t lr 1.0000000000000002e-06\t time 0:00:59.746196:\n",
      "Training, Logging: Mean loss of previous 50 batches \n",
      "\n",
      "Train Epoch: 3 [3136/148073 (2%)]\tMean Loss : 3.840889\t lr 1.0000000000000002e-06\t time 0:00:55.817526:\n",
      "Train Epoch: 3 [6336/148073 (4%)]\tMean Loss : 3.838239\t lr 1.0000000000000002e-06\t time 0:00:58.922924:\n",
      "Train Epoch: 3 [9536/148073 (6%)]\tMean Loss : 3.835938\t lr 1.0000000000000002e-06\t time 0:00:59.998634:\n",
      "Train Epoch: 3 [12736/148073 (9%)]\tMean Loss : 3.854497\t lr 1.0000000000000002e-06\t time 0:00:55.030574:\n",
      "Train Epoch: 3 [15936/148073 (11%)]\tMean Loss : 3.838843\t lr 1.0000000000000002e-06\t time 0:01:03.027205:\n",
      "Train Epoch: 3 [19136/148073 (13%)]\tMean Loss : 3.839106\t lr 1.0000000000000002e-06\t time 0:00:56.936276:\n",
      "Train Epoch: 3 [22336/148073 (15%)]\tMean Loss : 3.833508\t lr 1.0000000000000002e-06\t time 0:00:58.307332:\n",
      "Train Epoch: 3 [25536/148073 (17%)]\tMean Loss : 3.839629\t lr 1.0000000000000002e-06\t time 0:01:01.520976:\n",
      "Train Epoch: 3 [28736/148073 (19%)]\tMean Loss : 3.826540\t lr 1.0000000000000002e-06\t time 0:00:57.431021:\n",
      "Train Epoch: 3 [31936/148073 (22%)]\tMean Loss : 3.840141\t lr 1.0000000000000002e-06\t time 0:00:57.263745:\n",
      "Train Epoch: 3 [35136/148073 (24%)]\tMean Loss : 3.826802\t lr 1.0000000000000002e-06\t time 0:00:57.349871:\n",
      "Train Epoch: 3 [38336/148073 (26%)]\tMean Loss : 3.847175\t lr 1.0000000000000002e-06\t time 0:00:55.650406:\n",
      "Train Epoch: 3 [41536/148073 (28%)]\tMean Loss : 3.817797\t lr 1.0000000000000002e-06\t time 0:01:04.289048:\n",
      "Train Epoch: 3 [44736/148073 (30%)]\tMean Loss : 3.848834\t lr 1.0000000000000002e-06\t time 0:00:59.512248:\n",
      "Train Epoch: 3 [47936/148073 (32%)]\tMean Loss : 3.841263\t lr 1.0000000000000002e-06\t time 0:00:54.920834:\n",
      "Train Epoch: 3 [51136/148073 (35%)]\tMean Loss : 3.835144\t lr 1.0000000000000002e-06\t time 0:00:57.853496:\n",
      "Train Epoch: 3 [54336/148073 (37%)]\tMean Loss : 3.843621\t lr 1.0000000000000002e-06\t time 0:00:56.693268:\n",
      "Train Epoch: 3 [57536/148073 (39%)]\tMean Loss : 3.837897\t lr 1.0000000000000002e-06\t time 0:00:51.079522:\n",
      "Train Epoch: 3 [60736/148073 (41%)]\tMean Loss : 3.833145\t lr 1.0000000000000002e-06\t time 0:00:54.142318:\n",
      "Train Epoch: 3 [63936/148073 (43%)]\tMean Loss : 3.828722\t lr 1.0000000000000002e-06\t time 0:00:56.296211:\n",
      "Train Epoch: 3 [67136/148073 (45%)]\tMean Loss : 3.825907\t lr 1.0000000000000002e-06\t time 0:00:55.025933:\n",
      "Train Epoch: 3 [70336/148073 (48%)]\tMean Loss : 3.848036\t lr 1.0000000000000002e-06\t time 0:00:51.232050:\n",
      "Train Epoch: 3 [73536/148073 (50%)]\tMean Loss : 3.834852\t lr 1.0000000000000002e-06\t time 0:00:57.656821:\n",
      "Train Epoch: 3 [76736/148073 (52%)]\tMean Loss : 3.834760\t lr 1.0000000000000002e-06\t time 0:00:57.043682:\n",
      "Epoch   161: reducing learning rate of group 0 to 1.0000e-07.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ffb32dedab48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m#save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-012638ae8a19>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, print_interval, writer, log_interval, scheduler)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/models/las_model/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, target)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0my_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mteacher_force\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/models/las_model/attend_and_spell.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, yt_prev, hidden_prev, encoder_output, c_prev)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mspell_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mh_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_lstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspell_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_prev\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh_0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;31m# TODO: if statement only here to tell the jit to skip emitting this when it is None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmomentum\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# use cumulative moving average\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                     \u001b[0mexponential_average_factor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.0\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_batches_tracked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, verbose=True, cooldown=5, min_lr=0.00001)\n",
    "\n",
    "# hence approximately waiting for print_interval*batch_size*(patience+cooldown) to improve\n",
    "log_interval = 5\n",
    "print_interval = 50\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "for epoch in range(0, epochs): \n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval, scheduler)\n",
    "    \n",
    "    #save model\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    \n",
    "    # Decrease tf_ratio\n",
    "    if (epoch+1)%10 == 0:\n",
    "        model.tf_ratio = model.tf_ratio - 0.5\n",
    "        print(\"\\nTeacher forcing ratio:\", model.tf_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOES DEEPER NETWORK HELP ?\n",
    "YES\n",
    "\n",
    "### DOES AMSGRAD HELP ?\n",
    "\n",
    "### DOES LAYER NORMALIZATION HELP ?\n",
    "YES, WITH SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    out = out.squeeze(0)\n",
    "    for t in out:\n",
    "        lol = t.max(dim=0)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14])\n",
      "torch.Size([1, 14, 82])\n",
      "True sent :  දුකක් දැනෙනවා<eos>\n",
      "\n",
      "Pred sent :  මා නනනකනනනනනනන\n",
      "Loss : 4.253574848175049\n",
      "\n",
      "\n",
      "torch.Size([9])\n",
      "torch.Size([1, 9, 82])\n",
      "True sent :  මේ ශාසනය<eos>\n",
      "\n",
      "Pred sent :  මා කනනනනන\n",
      "Loss : 4.093790054321289\n",
      "\n",
      "\n",
      "torch.Size([35])\n",
      "torch.Size([1, 35, 82])\n",
      "True sent :  හම්බවෙන පිළිවෙලටනේ සයිට් එකේ යන්නේ<eos>\n",
      "\n",
      "Pred sent :  මා  නනනනකනනනනනනන න කනනන නකකන කනනනන \n",
      "Loss : 4.266133785247803\n",
      "\n",
      "\n",
      "torch.Size([45])\n",
      "torch.Size([1, 45, 82])\n",
      "True sent :  මනා සංවරයෙන් යුතු භික්ෂුන් වහන්සේ නමක් ලෙසයි<eos>\n",
      "\n",
      "Pred sent :  මා නකනකනනනනනනකනනනනකනනනනනනනනකනනනනන කනනන කනනනනන\n",
      "Loss : 4.285437107086182\n",
      "\n",
      "\n",
      "torch.Size([30])\n",
      "torch.Size([1, 30, 82])\n",
      "True sent :  කෙනෙකු විසින් මරාදමා ඇති බවයි<eos>\n",
      "\n",
      "Pred sent :  මානනනනනකනනනනනනකනනනන නකනනනකනනනන\n",
      "Loss : 4.340115070343018\n",
      "\n",
      "\n",
      "torch.Size([25])\n",
      "torch.Size([1, 25, 82])\n",
      "True sent :  කොපි කරන එකේ අපි එයාගෙන්<eos>\n",
      "\n",
      "Pred sent :  මාකනනකනනනකකන කනනනකකනනනනනන\n",
      "Loss : 4.238231658935547\n",
      "\n",
      "\n",
      "torch.Size([28])\n",
      "torch.Size([1, 28, 82])\n",
      "True sent :  නමින් හැඳින්වීම අර්ථාන්විතය<eos>\n",
      "\n",
      "Pred sent :  මා නනනකනනනනනනනන කනනනනනනනනනනන\n",
      "Loss : 4.28655481338501\n",
      "\n",
      "\n",
      "torch.Size([20])\n",
      "torch.Size([1, 20, 82])\n",
      "True sent :  එදා යුද්දය වෙනුවෙන්<eos>\n",
      "\n",
      "Pred sent :  මකනනකනනනනනනකනනනනනනනන\n",
      "Loss : 4.30584716796875\n",
      "\n",
      "\n",
      "torch.Size([23])\n",
      "torch.Size([1, 23, 82])\n",
      "True sent :  බාහිර ආටෝපවලට අකැමැතිව<eos>\n",
      "\n",
      "Pred sent :  මා නනනකකනනනනන කනනනනනනනන\n",
      "Loss : 4.35223913192749\n",
      "\n",
      "\n",
      "torch.Size([18])\n",
      "torch.Size([1, 18, 82])\n",
      "True sent :  තැන්පත් කරන දෙන්න<eos>\n",
      "\n",
      "Pred sent :  මානන නනනකනනනකනනනනන\n",
      "Loss : 4.154826641082764\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, data_dir, char_to_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    print(y.shape)\n",
    "    \n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(output.shape)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Knowing the frequency of words\n",
    "\n",
    "def process(s):\n",
    "    return list(s)\n",
    "\n",
    "si_field = Field(\n",
    "    tokenizer_language='si',\n",
    "    lower=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    "    preprocessing=process\n",
    ")\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=os.path.join(data_dir, 'temp.csv'),\n",
    "    format='CSV',\n",
    "    fields=[('index', None),('unnamed', None), ('sent', si_field)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_field.build_vocab(dataset, min_freq=2)\n",
    "print(len(si_field.vocab.stoi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hacking the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.state_dict()['param_groups'][0]['lr']"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
