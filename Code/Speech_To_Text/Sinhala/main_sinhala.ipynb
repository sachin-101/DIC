{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/computermaestro/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import datetime\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "NAME = # helps to differentiate between various training instances\n",
    "#os.chdir(os.path.join(os.getcwd(), 'LAS Model'))\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "from data import SpeechDataset, AudioDataLoader\n",
    "from listener import Listener\n",
    "from attend_and_spell import AttendAndSpell\n",
    "from seq2seq import Seq2Seq\n",
    "from utils import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:1\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:1') if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training examples: 149569\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000f47c22.flac</td>\n",
       "      <td>මහවැලි ගඟට ගොස් ආපසු එන ගමනේදී</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000101700f.flac</td>\n",
       "      <td>උන්වහන්සේ කපාපු</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000107b539.flac</td>\n",
       "      <td>එය එතනින් අවසන් නොවී</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00016825d3.flac</td>\n",
       "      <td>සිතින් අයහපතෙහි හැසිරීම නිසයි</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0002205a57.flac</td>\n",
       "      <td>ඊට අවසරයද හිමිවූ බව ඇය කියන්නීය</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              path                             sent\n",
       "0  0000f47c22.flac   මහවැලි ගඟට ගොස් ආපසු එන ගමනේදී\n",
       "1  000101700f.flac                  උන්වහන්සේ කපාපු\n",
       "2  000107b539.flac             එය එතනින් අවසන් නොවී\n",
       "3  00016825d3.flac    සිතින් අයහපතෙහි හැසිරීම නිසයි\n",
       "4  0002205a57.flac  ඊට අවසරයද හිමිවූ බව ඇය කියන්නීය"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = '../../../Dataset/Sinhala'\n",
    "\n",
    "remove_chars = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', \\\n",
    "                 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', \\\n",
    "                'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w',  'x', 'y', 'z', \\\n",
    "                '“', '”', '\\u200b', '\\u200c', '\\u200d', 'µ', '\\x94', '»', 'ª', '’', '‘']\n",
    "\n",
    "\n",
    "def preprocess(s):\n",
    "    s = s.replace('\\n', '')  # remove '\\n'\n",
    "    return s.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "\n",
    "\n",
    "# reading the main transcript\n",
    "lines = []\n",
    "with open(os.path.join(data_dir, 'utt_spk_text.tsv'), 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "examples = []\n",
    "for l in lines:\n",
    "    append = True\n",
    "    id_, _, sent = l.split('\\t')\n",
    "    sent = preprocess(sent)\n",
    "    for c in sent:\n",
    "        if c in remove_chars:  # removing sentences with eng_chars\n",
    "            append = False\n",
    "            break\n",
    "    if append:\n",
    "        examples.append((id_+'.flac', sent))\n",
    "\n",
    "data_df = pd.DataFrame(examples, columns=['path', 'sent'])\n",
    "data_df.to_csv(os.path.join(data_dir, 'data_df.csv')) # save\n",
    "print(\"Number of Training examples:\", data_df.shape[0])\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see their are some english sentences also in the dataset, so we will go ahead and clean the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134612, 2)\n",
      "(14957, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8800</th>\n",
       "      <td>0f0339e0fd.flac</td>\n",
       "      <td>අතට අරගෙන බැලුවම ආච්චිගෙ බොරු දත් දෙකක්ලු</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72286</th>\n",
       "      <td>7c42c3e476.flac</td>\n",
       "      <td>ස්තූතියි ලංකාදීපයට</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18578</th>\n",
       "      <td>1fe1bd26e0.flac</td>\n",
       "      <td>එම සිදුවීම තුළින්</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87200</th>\n",
       "      <td>958b0b82c0.flac</td>\n",
       "      <td>බුදුරජාණන් වහන්සේට</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89946</th>\n",
       "      <td>9a35c473e4.flac</td>\n",
       "      <td>තීන්දු කර සෝර්බා අමතයි</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  path                                       sent\n",
       "8800   0f0339e0fd.flac  අතට අරගෙන බැලුවම ආච්චිගෙ බොරු දත් දෙකක්ලු\n",
       "72286  7c42c3e476.flac                         ස්තූතියි ලංකාදීපයට\n",
       "18578  1fe1bd26e0.flac                          එම සිදුවීම තුළින්\n",
       "87200  958b0b82c0.flac                         බුදුරජාණන් වහන්සේට\n",
       "89946  9a35c473e4.flac                     තීන්දු කර සෝර්බා අමතයි"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = pd.read_csv(os.path.join(data_dir, 'data_df.csv'), usecols=['path', 'sent'])\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1)\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 82\n",
      "['<pad>', '<unk>', '<sos>', '<eos>', 'අ', 'ත', 'ට', ' ', 'ර', 'ග', 'ෙ', 'න', 'බ', 'ැ', 'ල', 'ු', 'ව', 'ම', 'ආ', 'ච', '්', 'ි', 'ො', 'ද', 'ක', 'ස', 'ූ', 'ය', 'ං', 'ා', 'ී', 'ප', 'එ', 'ළ', 'ජ', 'ණ', 'හ', 'ේ', 'ෝ', 'ඇ', 'ඒ', 'ඊ', 'ඉ', 'උ', 'ථ', 'ඩ', 'ඳ', 'ෑ', 'ධ', 'ශ', 'ෆ', 'ඔ', 'ඹ', 'ඃ', 'භ', 'ෂ', 'ඥ', 'ඟ', 'ඓ', 'ඕ', 'ෛ', 'ඬ', 'ඌ', 'ෞ', 'ඡ', 'ඵ', 'ඝ', 'ෘ', 'ඤ', 'ඈ', 'ඨ', 'ඛ', 'ඞ', 'ඍ', 'ඣ', 'ඖ', 'ඪ', '–', 'ෲ', '෴', 'ෳ', 'ෟ']\n"
     ]
    }
   ],
   "source": [
    "def get_chars(train_df):\n",
    "    chars = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    for idx in range(train_df.shape[0]):\n",
    "        id_, sent = train_df.iloc[idx]\n",
    "        for c in sent:\n",
    "            if c not in chars:\n",
    "                chars.append(c)\n",
    "    return chars\n",
    "    \n",
    "\n",
    "chars = get_chars(train_df)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']\n",
    "unk_token = char_to_token['<unk>']\n",
    "\n",
    "print(\"Number of characters:\", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_dir = os.path.join('tb_summary')\n",
    "train_dataset = SpeechDataset(train_df, data_dir, char_to_token)\n",
    "train_loader = AudioDataLoader(pad_token, train_dataset, batch_size=32, \n",
    "                               shuffle=True, drop_last=True, num_workers=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = False\n",
    "\n",
    "if load:\n",
    "    saved_file = 'Trained Models/Training_2019-12-25 00:09:23.921978/las_model_6'\n",
    "    model.load_state_dict(torch.load(saved_file))\n",
    "    start_epoch = int(saved_file[-1]) + 1\n",
    "    time = os.listdir(tensorboard_dir)[-1]  # use the last one\n",
    "else:\n",
    "    start_epoch = 0\n",
    "    time = str(datetime.datetime.now())\n",
    "\n",
    "name = f'first_amsgrad_{time}'\n",
    "save_dir = os.path.join('trained_models_librispeech', name)\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): piBLSTM(\n",
       "        (lstm): LSTM(2560, 640, batch_first=True, bidirectional=True)\n",
       "        (ln): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (embedding): Embedding(82, 50)\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=3200, out_features=1600, bias=True)\n",
       "      (linear2): Linear(in_features=1600, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(2610, 640)\n",
       "    (post_lstm_cell): LSTMCell(3200, 640)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=640, out_features=82, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(82, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 640  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "layer_norm = True   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 640\n",
    "embed_dim = 50\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(embed_dim, hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "               'embed_dim':embed_dim, 'vocab_size':vocab_size}\n",
    "\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)\n",
    "\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(os.path.join(save_dir, 'las_model_1')))\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models_librispeech/first_amsgrad_2019-12-30 03:41:23.080861\n",
      "\n",
      "Teacher forcing ratio: 1.0\n",
      "Training, Logging: Mean loss of previous 40 batches \n",
      "\n",
      "Train Epoch: 0 [1248/134612 (1%)]\tMean Loss : 4.407001\t time 0:00:15.856450:\n",
      "Train Epoch: 0 [2528/134612 (2%)]\tMean Loss : 4.395084\t time 0:00:13.816954:\n",
      "Train Epoch: 0 [3808/134612 (3%)]\tMean Loss : 4.384286\t time 0:00:13.875093:\n",
      "Train Epoch: 0 [5088/134612 (4%)]\tMean Loss : 4.368769\t time 0:00:12.529653:\n",
      "Train Epoch: 0 [6368/134612 (5%)]\tMean Loss : 4.341573\t time 0:00:13.634184:\n",
      "Train Epoch: 0 [7648/134612 (6%)]\tMean Loss : 4.311553\t time 0:00:13.940840:\n",
      "Train Epoch: 0 [8928/134612 (7%)]\tMean Loss : 4.279588\t time 0:00:13.309770:\n",
      "Train Epoch: 0 [10208/134612 (8%)]\tMean Loss : 4.234485\t time 0:00:14.378923:\n",
      "Train Epoch: 0 [11488/134612 (9%)]\tMean Loss : 4.194911\t time 0:00:13.321922:\n",
      "Train Epoch: 0 [12768/134612 (9%)]\tMean Loss : 4.177648\t time 0:00:12.931940:\n",
      "Train Epoch: 0 [14048/134612 (10%)]\tMean Loss : 4.124570\t time 0:00:13.096373:\n",
      "Train Epoch: 0 [15328/134612 (11%)]\tMean Loss : 4.078859\t time 0:00:13.648201:\n",
      "Train Epoch: 0 [16608/134612 (12%)]\tMean Loss : 4.054895\t time 0:00:12.922145:\n",
      "Train Epoch: 0 [17888/134612 (13%)]\tMean Loss : 4.017761\t time 0:00:13.431314:\n",
      "Train Epoch: 0 [19168/134612 (14%)]\tMean Loss : 4.000510\t time 0:00:13.452683:\n",
      "Train Epoch: 0 [20448/134612 (15%)]\tMean Loss : 4.009968\t time 0:00:13.231707:\n",
      "Train Epoch: 0 [21728/134612 (16%)]\tMean Loss : 3.970935\t time 0:00:13.027261:\n",
      "Train Epoch: 0 [23008/134612 (17%)]\tMean Loss : 3.965672\t time 0:00:13.637064:\n",
      "Train Epoch: 0 [24288/134612 (18%)]\tMean Loss : 3.964035\t time 0:00:12.778101:\n",
      "Train Epoch: 0 [25568/134612 (19%)]\tMean Loss : 3.949681\t time 0:00:13.508622:\n",
      "Train Epoch: 0 [26848/134612 (20%)]\tMean Loss : 3.959437\t time 0:00:13.131372:\n",
      "Train Epoch: 0 [28128/134612 (21%)]\tMean Loss : 3.954222\t time 0:00:13.163616:\n",
      "Train Epoch: 0 [29408/134612 (22%)]\tMean Loss : 3.951087\t time 0:00:13.015017:\n",
      "Train Epoch: 0 [30688/134612 (23%)]\tMean Loss : 3.951187\t time 0:00:13.269690:\n",
      "Train Epoch: 0 [31968/134612 (24%)]\tMean Loss : 3.930946\t time 0:00:13.786506:\n",
      "Train Epoch: 0 [33248/134612 (25%)]\tMean Loss : 3.958435\t time 0:00:13.410313:\n",
      "Train Epoch: 0 [34528/134612 (26%)]\tMean Loss : 3.958192\t time 0:00:12.923718:\n",
      "Train Epoch: 0 [35808/134612 (27%)]\tMean Loss : 3.943325\t time 0:00:13.040208:\n",
      "Train Epoch: 0 [37088/134612 (28%)]\tMean Loss : 3.943450\t time 0:00:13.248911:\n",
      "Train Epoch: 0 [38368/134612 (29%)]\tMean Loss : 3.949596\t time 0:00:12.626908:\n",
      "Train Epoch: 0 [39648/134612 (29%)]\tMean Loss : 3.940750\t time 0:00:13.478609:\n",
      "Train Epoch: 0 [40928/134612 (30%)]\tMean Loss : 3.933148\t time 0:00:13.321197:\n",
      "Train Epoch: 0 [42208/134612 (31%)]\tMean Loss : 3.919681\t time 0:00:13.941011:\n",
      "Train Epoch: 0 [43488/134612 (32%)]\tMean Loss : 3.930073\t time 0:00:13.514651:\n",
      "Train Epoch: 0 [44768/134612 (33%)]\tMean Loss : 3.930206\t time 0:00:13.314670:\n",
      "Train Epoch: 0 [46048/134612 (34%)]\tMean Loss : 3.946563\t time 0:00:13.577959:\n",
      "Train Epoch: 0 [47328/134612 (35%)]\tMean Loss : 3.924008\t time 0:00:14.650949:\n",
      "Train Epoch: 0 [48608/134612 (36%)]\tMean Loss : 3.917955\t time 0:00:14.327891:\n",
      "Train Epoch: 0 [49888/134612 (37%)]\tMean Loss : 3.931492\t time 0:00:13.431658:\n",
      "Train Epoch: 0 [51168/134612 (38%)]\tMean Loss : 3.919172\t time 0:00:13.634127:\n",
      "Train Epoch: 0 [52448/134612 (39%)]\tMean Loss : 3.914130\t time 0:00:13.178248:\n",
      "Train Epoch: 0 [53728/134612 (40%)]\tMean Loss : 3.914777\t time 0:00:13.904974:\n",
      "Train Epoch: 0 [55008/134612 (41%)]\tMean Loss : 3.936937\t time 0:00:12.626666:\n",
      "Train Epoch: 0 [56288/134612 (42%)]\tMean Loss : 3.928036\t time 0:00:13.260882:\n",
      "Train Epoch: 0 [57568/134612 (43%)]\tMean Loss : 3.917955\t time 0:00:13.692745:\n",
      "Train Epoch: 0 [58848/134612 (44%)]\tMean Loss : 3.933799\t time 0:00:13.453939:\n",
      "Train Epoch: 0 [60128/134612 (45%)]\tMean Loss : 3.921959\t time 0:00:13.768089:\n",
      "Train Epoch: 0 [61408/134612 (46%)]\tMean Loss : 3.916243\t time 0:00:13.333463:\n",
      "Train Epoch: 0 [62688/134612 (47%)]\tMean Loss : 3.917343\t time 0:00:13.680450:\n",
      "Train Epoch: 0 [63968/134612 (48%)]\tMean Loss : 3.923241\t time 0:00:13.267208:\n",
      "Train Epoch: 0 [65248/134612 (48%)]\tMean Loss : 3.914455\t time 0:00:12.973523:\n",
      "Train Epoch: 0 [66528/134612 (49%)]\tMean Loss : 3.913273\t time 0:00:13.656887:\n",
      "Train Epoch: 0 [67808/134612 (50%)]\tMean Loss : 3.906875\t time 0:00:13.638340:\n",
      "Train Epoch: 0 [69088/134612 (51%)]\tMean Loss : 3.937631\t time 0:00:13.596503:\n",
      "Train Epoch: 0 [70368/134612 (52%)]\tMean Loss : 3.926245\t time 0:00:13.520597:\n",
      "Train Epoch: 0 [71648/134612 (53%)]\tMean Loss : 3.912348\t time 0:00:13.577192:\n",
      "Train Epoch: 0 [72928/134612 (54%)]\tMean Loss : 3.944339\t time 0:00:13.782214:\n",
      "Train Epoch: 0 [74208/134612 (55%)]\tMean Loss : 3.932709\t time 0:00:13.399393:\n",
      "Train Epoch: 0 [75488/134612 (56%)]\tMean Loss : 3.915789\t time 0:00:14.311358:\n",
      "Train Epoch: 0 [76768/134612 (57%)]\tMean Loss : 3.931400\t time 0:00:13.556139:\n",
      "Train Epoch: 0 [78048/134612 (58%)]\tMean Loss : 3.919006\t time 0:00:14.268596:\n",
      "Train Epoch: 0 [79328/134612 (59%)]\tMean Loss : 3.908457\t time 0:00:13.500552:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-c623184fc2f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nTeacher forcing ratio:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_ratio\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;31m# scheduler.step()                                 # Decrease learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'las_model_{epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/LAS Model/utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, print_interval, writer, log_interval)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \"\"\"\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.2)  # lr = 0.2 used in paper\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), amsgrad=True)\n",
    "\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "\n",
    "log_interval = 5\n",
    "print_interval = 40\n",
    "\n",
    "epochs = 20\n",
    "load = False\n",
    "\n",
    "summary_dir = os.path.join(tensorboard_dir, save_dir)\n",
    "writer = SummaryWriter(summary_dir)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    print(\"\\nTeacher forcing ratio:\", model.tf_ratio)\n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    # scheduler.step()                                 # Decrease learning rate\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}'))\n",
    "    model.tf_ratio = max(model.tf_ratio - 0.025, 0.8)    # Decrease teacher force ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DOES DEEPER NETWORK HELP ?\n",
    "YES\n",
    "\n",
    "### DOES AMSGRAD HELP ?\n",
    "\n",
    "### DOES LAYER NORMALIZATION HELP ?\n",
    "YES, WITH SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    out = out.squeeze(0)\n",
    "    for t in out:\n",
    "        lol = t.max(dim=0)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([21])\n",
      "torch.Size([1, 21, 82])\n",
      "True sent :  අනික රාහු කේතු කියලා<eos>\n",
      "\n",
      "Pred sent :  ්නනනිවන<eos>නනවි<eos>ිනවින<eos>ා<eos>\n",
      "Loss : 4.284475803375244\n",
      "\n",
      "\n",
      "torch.Size([37])\n",
      "torch.Size([1, 37, 82])\n",
      "True sent :  ඒක උනත් වටිනවා මෙහෙ හරක් මඩු වලට වඩා<eos>\n",
      "\n",
      "Pred sent :  ්නිවිනි<eos>වි<eos><eos>ාා<eos>ව<eos>නනනවනනි<eos>වින<eos>විා<eos>වාන<eos>\n",
      "Loss : 4.271315574645996\n",
      "\n",
      "\n",
      "torch.Size([26])\n",
      "torch.Size([1, 26, 82])\n",
      "True sent :  එවිට ලක්ෂ ගණන් නිරිසත්වයෝ<eos>\n",
      "\n",
      "Pred sent :  ්නාන<eos>විි<eos><eos>විනා<eos>විනනනිි<eos>ා<eos><eos>\n",
      "Loss : 4.331305027008057\n",
      "\n",
      "\n",
      "torch.Size([22])\n",
      "torch.Size([1, 22, 82])\n",
      "True sent :  ඒවා කීපයක් සඳහන් කරමි<eos>\n",
      "\n",
      "Pred sent :  ්නා<eos>වින<eos><eos>ි<eos>විනනා<eos>වින<eos><eos>\n",
      "Loss : 4.337202072143555\n",
      "\n",
      "\n",
      "torch.Size([13])\n",
      "torch.Size([1, 13, 82])\n",
      "True sent :  කලුරිය කරන්න<eos>\n",
      "\n",
      "Pred sent :  ්නනනනන<eos>විනා<eos><eos>\n",
      "Loss : 4.31797981262207\n",
      "\n",
      "\n",
      "torch.Size([34])\n",
      "torch.Size([1, 34, 82])\n",
      "True sent :  එම මගුල් පොකුණ අඩියේ දක්නට ලැබුණි<eos>\n",
      "\n",
      "Pred sent :  ්නනවනනනා<eos>විනිනනවනනන<eos><eos>වෙි<eos>ා<eos>විනනනනන\n",
      "Loss : 4.41461706161499\n",
      "\n",
      "\n",
      "torch.Size([38])\n",
      "torch.Size([1, 38, 82])\n",
      "True sent :  එදා ගන්ධාරි කලායතනය අබියස වැසී තිබුණු<eos>\n",
      "\n",
      "Pred sent :  ්නන<eos>වින<eos>න<eos>නනවින<eos><eos>ිා<eos>වනනන<eos>ිවානිනවිනනනනන\n",
      "Loss : 4.3498358726501465\n",
      "\n",
      "\n",
      "torch.Size([21])\n",
      "torch.Size([1, 21, 82])\n",
      "True sent :  අනෙක දිග කතා කියවන්න<eos>\n",
      "\n",
      "Pred sent :  ්නනනිවිනනවිි<eos>වින<eos>ාා<eos><eos>\n",
      "Loss : 4.297987937927246\n",
      "\n",
      "\n",
      "torch.Size([30])\n",
      "torch.Size([1, 30, 82])\n",
      "True sent :  සාක්කුවට නම් හොඳයි එතුමන්ලාගේ<eos>\n",
      "\n",
      "Pred sent :  ්න<eos>ි<eos>ිනා<eos>වින<eos>වනනන<eos>නවිිනනා<eos>ා<eos>න<eos>\n",
      "Loss : 4.366408824920654\n",
      "\n",
      "\n",
      "torch.Size([29])\n",
      "torch.Size([1, 29, 82])\n",
      "True sent :  එක්සත් ජාතීන්ගේ අරමුදල් සඳහා<eos>\n",
      "\n",
      "Pred sent :  ්නි<eos>ිි<eos>වි<eos>ි<eos><eos><eos><eos><eos>වනනනනාා<eos>විනන<eos>\n",
      "Loss : 4.380702972412109\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_sent = 10\n",
    "model.eval()\n",
    "\n",
    "for _ in range(num_sent):\n",
    "    \n",
    "    idx = random.randint(0, train_df.shape[0])\n",
    "    trial_dataset = SpeechDataset(train_df, data_dir, char_to_token)\n",
    "\n",
    "    x, y = trial_dataset.__getitem__(idx)\n",
    "    # plt.imshow(x[0,:,:].detach())\n",
    "\n",
    "    # Model output\n",
    "    print(y.shape)\n",
    "    \n",
    "    target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "    data = x.permute(0, 2, 1).to(DEVICE)\n",
    "    loss, output = model(data, target)\n",
    "    print(output.shape)\n",
    "    print(\"True sent : \", decode_true_sent(y), end='\\n\\n')\n",
    "    print(\"Pred sent : \", decode_pred_sent(output))\n",
    "    print(\"Loss :\", loss.item())    \n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Knowing the frequency of words\n",
    "\n",
    "def process(s):\n",
    "    return list(s)\n",
    "\n",
    "si_field = Field(\n",
    "    tokenizer_language='si',\n",
    "    lower=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    "    preprocessing=process\n",
    ")\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=os.path.join(data_dir, 'temp.csv'),\n",
    "    format='CSV',\n",
    "    fields=[('index', None),('unnamed', None), ('sent', si_field)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7963\n"
     ]
    }
   ],
   "source": [
    "si_field.build_vocab(dataset, min_freq=2)\n",
    "print(len(si_field.vocab.stoi))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
