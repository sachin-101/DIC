{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import string\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "NAME = 'amsgrad_clean_1h' # helps to differentiate between various training instances\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "\n",
    "from models.las_model.data import SpeechDataset, AudioDataLoader\n",
    "from models.las_model.listener import Listener\n",
    "from models.las_model.attend_and_spell import AttendAndSpell\n",
    "from models.las_model.seq2seq import Seq2Seq\n",
    "#from models.las_model.utils import  train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GeForce GTX 1080 Ti\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla P100-PCIE-16GB\n",
      "Tesla V100-PCIE-32GB\n",
      "Tesla V100-PCIE-32GB\n",
      "Tesla V100-PCIE-32GB\n",
      "Tesla V100-PCIE-32GB\n",
      "Tesla V100-PCIE-16GB\n"
     ]
    }
   ],
   "source": [
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE : cuda:4\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:4') #if torch.cuda.is_available() else 'cpu'\n",
    "print('DEVICE :', DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Training examples: 1251\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sin_2241_0329430812.wav</td>\n",
       "      <td>කෝකටත් මං වෙනදා තරම් කාලෙ ගන්නැතිව ඇඳ ගත්තා</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sin_2241_0598895166.wav</td>\n",
       "      <td>ඇන්ජලීනා ජොලී කියන්නේ පසුගිය දිනවල බොහෝ සෙයින්...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sin_2241_0701577369.wav</td>\n",
       "      <td>ආර්ථික චින්තනය හා සාමාජීය දියුණුව ඇති කළ හැකිව...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sin_2241_0715400935.wav</td>\n",
       "      <td>ඉන් අදහස් වන්නේ විචාරාත්මක විනිවිද දැකීමෙන් තො...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sin_2241_0817100025.wav</td>\n",
       "      <td>අප යුද්ධයේ පළමු පියවරේදීම පරාද වී අවසානය</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      path                                               sent\n",
       "0  sin_2241_0329430812.wav        කෝකටත් මං වෙනදා තරම් කාලෙ ගන්නැතිව ඇඳ ගත්තා\n",
       "1  sin_2241_0598895166.wav  ඇන්ජලීනා ජොලී කියන්නේ පසුගිය දිනවල බොහෝ සෙයින්...\n",
       "2  sin_2241_0701577369.wav  ආර්ථික චින්තනය හා සාමාජීය දියුණුව ඇති කළ හැකිව...\n",
       "3  sin_2241_0715400935.wav  ඉන් අදහස් වන්නේ විචාරාත්මක විනිවිද දැකීමෙන් තො...\n",
       "4  sin_2241_0817100025.wav           අප යුද්ධයේ පළමු පියවරේදීම පරාද වී අවසානය"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_dir = '../../../Dataset/sinhala_clean'\n",
    "data_dir = os.path.join(root_dir, 'data')\n",
    "\n",
    "\n",
    "# reading the main transcript\n",
    "lines = []\n",
    "with open(os.path.join(root_dir, 'si_lk.lines.txt'), 'r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "examples = []\n",
    "for l in lines:\n",
    "    id_, sent, _ = l.split('\"')\n",
    "    id_ = id_.replace(\"(\", '').strip()\n",
    "    sent = sent.strip()\n",
    "    examples.append((id_+'.wav',sent))\n",
    "\n",
    "\n",
    "data_df = pd.DataFrame(examples, columns=['path', 'sent'])\n",
    "data_df.to_csv(os.path.join(root_dir, 'data_df.csv')) # save\n",
    "print(\"Number of Training examples:\", data_df.shape[0])\n",
    "data_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have tried removing all the unnecessary characters from the dataset. The others will be replaced by unknown token, while training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num training example: (1125, 2)\n",
      "Num validation example (126, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>sin_6314_0000039087.wav</td>\n",
       "      <td>රජතුමාට වෙනදා මෙන් කරුණාවෙන් සෙත් පැතුවා</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>418</th>\n",
       "      <td>sin_3688_7927489278.wav</td>\n",
       "      <td>කෙනෙක් බඩගෝස්තරය නැති වෙයි කියලා බයවෙලා</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>sin_2282_2140134972.wav</td>\n",
       "      <td>හිතාගන්න පුළුවන්නේ ෆිල්ම් එක කොහොමට ඇතිද කියලා</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1101</th>\n",
       "      <td>sin_7183_6716425545.wav</td>\n",
       "      <td>ඇත් ගව මුව වඳුරු නාග ආදී සත්ත්ව කුලවල ඉපිද ඇත</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>sin_4191_8578172520.wav</td>\n",
       "      <td>මේකෙත් හිටියෙ හුරුපුරුදු සුපිරි නළුවෙක් තමයි</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         path                                            sent\n",
       "804   sin_6314_0000039087.wav        රජතුමාට වෙනදා මෙන් කරුණාවෙන් සෙත් පැතුවා\n",
       "418   sin_3688_7927489278.wav         කෙනෙක් බඩගෝස්තරය නැති වෙයි කියලා බයවෙලා\n",
       "140   sin_2282_2140134972.wav  හිතාගන්න පුළුවන්නේ ෆිල්ම් එක කොහොමට ඇතිද කියලා\n",
       "1101  sin_7183_6716425545.wav   ඇත් ගව මුව වඳුරු නාග ආදී සත්ත්ව කුලවල ඉපිද ඇත\n",
       "567   sin_4191_8578172520.wav    මේකෙත් හිටියෙ හුරුපුරුදු සුපිරි නළුවෙක් තමයි"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_df = pd.read_csv(os.path.join(root_dir, 'data_df.csv'), usecols=['path', 'sent'])\n",
    "train_df, val_df = train_test_split(data_df, test_size=0.1)\n",
    "print(\"Num training example:\", train_df.shape)\n",
    "print(\"Num validation example\", val_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 70\n",
      "['<pad>', '<unk>', '<sos>', '<eos>', 'ර', 'ජ', 'ත', 'ු', 'ම', 'ා', 'ට', ' ', 'ව', 'ෙ', 'න', 'ද', '්', 'ක', 'ණ', 'ස', 'ප', 'ැ', 'බ', 'ඩ', 'ග', 'ෝ', 'ය', 'ි', 'ල', 'හ', 'ළ', 'ේ', 'ෆ', 'එ', 'ො', 'ඇ', 'ඳ', 'ආ', 'ී', 'ඉ', 'අ', 'ඔ', 'ධ', 'ථ', 'ඒ', 'භ', 'ූ', 'ච', 'උ', 'ඊ', 'ං', 'ඬ', 'ශ', 'ෑ', 'ඈ', 'ෂ', 'ඤ', 'ඪ', 'ඹ', 'ඟ', 'ඕ', 'ඝ', 'ෞ', 'ඓ', 'ඨ', 'ඛ', 'ඵ', 'ඡ', 'ඌ', 'ෛ']\n"
     ]
    }
   ],
   "source": [
    "def get_chars(train_df):\n",
    "    chars = ['<pad>', '<unk>', '<sos>', '<eos>']\n",
    "    for idx in range(train_df.shape[0]):\n",
    "        id_, sent = train_df.iloc[idx]\n",
    "        for c in sent:\n",
    "            if c not in chars:\n",
    "                chars.append(c)\n",
    "    return chars\n",
    "    \n",
    "\n",
    "chars = get_chars(train_df)\n",
    "char_to_token = {c:i for i,c in enumerate(chars)} \n",
    "token_to_char = {i:c for c,i in char_to_token.items()}\n",
    "sos_token = char_to_token['<sos>']\n",
    "eos_token = char_to_token['<eos>']\n",
    "pad_token = char_to_token['<pad>']\n",
    "unk_token = char_to_token['<unk>']\n",
    "\n",
    "print(\"Number of characters:\", len(chars))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Listener(\n",
       "    (layers): ModuleList(\n",
       "      (0): piBLSTM(\n",
       "        (lstm): LSTM(128, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (1): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (2): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (3): piBLSTM(\n",
       "        (lstm): LSTM(2048, 512, batch_first=True, bidirectional=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): AttendAndSpell(\n",
       "    (attention_layer): Attention(\n",
       "      (linear1): Linear(in_features=2560, out_features=1280, bias=True)\n",
       "      (linear2): Linear(in_features=1280, out_features=1, bias=True)\n",
       "    )\n",
       "    (pre_lstm_cell): LSTMCell(2118, 512)\n",
       "    (post_lstm_cell): LSTMCell(2560, 512)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=512, out_features=70, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): BatchNorm1d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (3): Softmax(dim=1)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_size = 128    # num rows in instagram\n",
    "hidden_dim = 512  # 256*2 nodes in each LSTM\n",
    "num_layers = 4\n",
    "dropout = 0.1\n",
    "layer_norm = False   \n",
    "encoder = Listener(input_size, hidden_dim, num_layers, dropout=dropout, layer_norm=layer_norm)\n",
    "\n",
    "hid_sz = 512\n",
    "vocab_size = len(chars)\n",
    "decoder = AttendAndSpell(hid_sz, encoder.output_size, vocab_size)\n",
    "\n",
    "hyperparams = {'input_size':input_size, 'hidden_dim':hidden_dim, \n",
    "               'num_layers':num_layers,'dropout':dropout, \n",
    "               'layer_norm':layer_norm, 'hid_sz':hid_sz, \n",
    "                'vocab_size':vocab_size}\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, tf_ratio = 1.0, device=DEVICE).to(DEVICE)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "time = str(datetime.datetime.now())\n",
    "save_dir = os.path.join('trained_models', f'{NAME}_{time}')\n",
    "try:    \n",
    "    os.mkdir(save_dir);\n",
    "except FileExistsError:\n",
    "    pass\n",
    "\n",
    "# Saving hyperparmas\n",
    "with open(os.path.join(save_dir, 'info.pickle'), 'wb') as f:\n",
    "    pickle.dump(hyperparams, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, \n",
    "          print_interval, writer=None, log_interval=-1, scheduler=None):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = []\n",
    "    date1 = datetime.datetime.now()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        loss, _ = model(data, target)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss.append(loss.detach().item())    # update running loss\n",
    "        \n",
    "        # Writing to tensorboard\n",
    "        if (batch_idx+1) % log_interval == 0:\n",
    "            if writer:\n",
    "                global_step = epoch * len(train_loader) + batch_idx\n",
    "                writer.add_scalar('Loss', np.mean(running_loss[-log_interval:]), global_step)\n",
    "                \n",
    "    # After epoch ends           \n",
    "    date2 = datetime.datetime.now()\n",
    "    print('Epoch: {}\\tMean Loss : {:.6f}\\t lr {}\\t time {}:'.format(\n",
    "        epoch, np.mean(running_loss[-print_interval:]), \n",
    "        optimizer.state_dict()['param_groups'][0]['lr'],\n",
    "        date2 - date1))\n",
    "    \n",
    "\n",
    "    \n",
    "def decode_pred_sent(out):\n",
    "    pred_sent = []\n",
    "    out = out.squeeze(0)\n",
    "    for t in out:\n",
    "        lol = t.max(dim=0)[1].item()\n",
    "        pred_sent.append(token_to_char[lol])\n",
    "    return ''.join(pred_sent)\n",
    "\n",
    "\n",
    "def decode_true_sent(y):\n",
    "    sent = []\n",
    "    for t in y:\n",
    "        sent.append(token_to_char[t.item()])\n",
    "    return ''.join(sent)\n",
    "\n",
    "def validate_personal(model, num_sent, dataset, show=False):\n",
    "    model.eval()\n",
    "    for _ in range(num_sent):\n",
    "        idx = random.randint(0, dataset.__len__())\n",
    "\n",
    "        x, y = dataset.__getitem__(idx)\n",
    "        plt.imshow(x[0,:,:].detach().log2())\n",
    "        if show:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "        target = y.unsqueeze(dim=0).to(DEVICE)\n",
    "        data = x.permute(0, 2, 1).to(DEVICE)\n",
    "        loss, output = model(data, target)\n",
    "        print(\"\\n\")\n",
    "        print(\"True sent : \", decode_true_sent(y))\n",
    "        print(\"Pred sent : \", decode_pred_sent(output))\n",
    "        print(\"Loss :\", loss.item())    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_loader(model):\n",
    "#     train_dataset = SpeechDataset(train_df, data_dir, char_to_token, n_fft=2048, hop_length=512)\n",
    "    \n",
    "#     if epoch < 3:\n",
    "#         train_loader = train_loader = AudioDataLoader(pad_token, train_dataset, \n",
    "#                                                       batch_size=64, num_workers=8, \n",
    "#                                                       drop_last=True, shuffle=True)\n",
    "#     elif epoch >= 3 and epoch < 5:\n",
    "#         train_loader = train_loader = AudioDataLoader(pad_token, train_dataset, \n",
    "#                                                       batch_size=32, num_workers=8, \n",
    "#                                                       drop_last=True, shuffle=True)\n",
    "#     elif epoch >= 5:\n",
    "#         train_loader = train_loader = AudioDataLoader(pad_token, train_dataset, \n",
    "#                                                       batch_size=8, num_workers=8, \n",
    "#                                                       drop_last=True, shuffle=True)\n",
    "#     return train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_dir trained_models/amsgrad_clean_1h_2019-12-31 07:23:15.666224\n",
      "Epoch: 1\tMean Loss : 4.245530\t lr 0.001\t time 0:00:29.917670:\n",
      "Epoch: 2\tMean Loss : 4.247317\t lr 0.001\t time 0:00:29.485088:\n",
      "Epoch: 3\tMean Loss : 4.246732\t lr 0.001\t time 0:00:30.648765:\n",
      "Epoch: 4\tMean Loss : 4.245434\t lr 0.001\t time 0:00:28.715245:\n",
      "Epoch: 5\tMean Loss : 4.242517\t lr 0.001\t time 0:00:30.207478:\n",
      "Epoch: 6\tMean Loss : 4.234566\t lr 0.001\t time 0:00:31.465014:\n",
      "Epoch: 7\tMean Loss : 4.225525\t lr 0.001\t time 0:00:29.935297:\n",
      "Epoch: 8\tMean Loss : 4.219678\t lr 0.001\t time 0:00:31.768671:\n",
      "Epoch: 9\tMean Loss : 4.212035\t lr 0.001\t time 0:00:30.132690:\n",
      "Epoch: 10\tMean Loss : 4.205882\t lr 0.001\t time 0:00:32.580395:\n",
      "Epoch: 11\tMean Loss : 4.202010\t lr 0.001\t time 0:00:32.153438:\n",
      "Epoch: 12\tMean Loss : 4.197245\t lr 0.001\t time 0:00:30.768699:\n",
      "Epoch: 13\tMean Loss : 4.196337\t lr 0.001\t time 0:00:30.235401:\n",
      "Epoch: 14\tMean Loss : 4.192161\t lr 0.001\t time 0:00:31.435306:\n",
      "Epoch: 15\tMean Loss : 4.189893\t lr 0.001\t time 0:00:30.773202:\n",
      "Epoch: 16\tMean Loss : 4.190926\t lr 0.001\t time 0:00:30.121649:\n",
      "Epoch: 17\tMean Loss : 4.186704\t lr 0.001\t time 0:00:31.112546:\n",
      "Epoch: 18\tMean Loss : 4.182565\t lr 0.001\t time 0:00:31.743812:\n",
      "Epoch: 19\tMean Loss : 4.180846\t lr 0.001\t time 0:00:31.432568:\n",
      "Epoch: 20\tMean Loss : 4.178725\t lr 0.001\t time 0:00:31.920401:\n",
      "Epoch: 21\tMean Loss : 4.176731\t lr 0.001\t time 0:00:31.625322:\n",
      "Epoch: 22\tMean Loss : 4.174307\t lr 0.001\t time 0:00:31.849917:\n",
      "Epoch: 23\tMean Loss : 4.170416\t lr 0.001\t time 0:00:31.959406:\n",
      "Epoch: 24\tMean Loss : 4.169279\t lr 0.001\t time 0:00:31.684355:\n",
      "Epoch: 25\tMean Loss : 4.165514\t lr 0.001\t time 0:00:31.834745:\n",
      "Epoch: 26\tMean Loss : 4.161808\t lr 0.001\t time 0:00:30.417171:\n",
      "Epoch: 27\tMean Loss : 4.160821\t lr 0.001\t time 0:00:31.670348:\n",
      "Epoch: 28\tMean Loss : 4.157173\t lr 0.001\t time 0:00:32.422099:\n",
      "Epoch: 29\tMean Loss : 4.154710\t lr 0.001\t time 0:00:32.892406:\n",
      "Epoch: 30\tMean Loss : 4.151831\t lr 0.001\t time 0:00:30.390065:\n",
      "\n",
      "\n",
      "True sent :  ආයෙත් දොළහෙ නිවුස් බැලුවෙත් අපි කට්ටියම තනියම<eos>\n",
      "Pred sent :   පෙෙපපපපපපපපපෙයෙදදදපදදදදෙද්ධපතියකියධේයේ්පි්යි්\n",
      "Loss : 4.123450756072998\n",
      "Epoch: 31\tMean Loss : 4.151508\t lr 0.001\t time 0:00:32.349136:\n",
      "Epoch: 32\tMean Loss : 4.145357\t lr 0.001\t time 0:00:31.336704:\n",
      "Epoch: 33\tMean Loss : 4.144861\t lr 0.001\t time 0:00:31.897669:\n",
      "Epoch: 34\tMean Loss : 4.140512\t lr 0.001\t time 0:00:31.246201:\n",
      "Epoch: 35\tMean Loss : 4.140086\t lr 0.001\t time 0:00:32.110550:\n",
      "Epoch: 36\tMean Loss : 4.133395\t lr 0.001\t time 0:00:30.891826:\n",
      "Epoch: 37\tMean Loss : 4.133542\t lr 0.001\t time 0:00:30.331064:\n",
      "Epoch: 38\tMean Loss : 4.128341\t lr 0.001\t time 0:00:29.969536:\n",
      "Epoch: 39\tMean Loss : 4.124138\t lr 0.001\t time 0:00:31.228972:\n",
      "Epoch: 40\tMean Loss : 4.123943\t lr 0.001\t time 0:00:32.369529:\n",
      "\n",
      "\n",
      "True sent :  එක්කෙනෙක් ඉතින් මම ලියපු දිනිති කතාවෙ දිනිති<eos>\n",
      "Pred sent :  <pad><pad>්ෙෙැපපපපපෙෙෙෙෙපෙපපපපපපපපෙපපපපයපෙපතෙකකෙයකයිය\n",
      "Loss : 4.193723201751709\n",
      "Epoch: 41\tMean Loss : 4.117235\t lr 0.001\t time 0:00:30.072449:\n",
      "Epoch: 42\tMean Loss : 4.116990\t lr 0.001\t time 0:00:32.657749:\n",
      "Epoch: 43\tMean Loss : 4.112477\t lr 0.001\t time 0:00:30.844817:\n",
      "Epoch: 44\tMean Loss : 4.107751\t lr 0.001\t time 0:00:32.946216:\n",
      "Epoch: 45\tMean Loss : 4.104655\t lr 0.001\t time 0:00:32.167982:\n",
      "Epoch: 46\tMean Loss : 4.099808\t lr 0.001\t time 0:00:31.587552:\n",
      "Epoch: 47\tMean Loss : 4.095237\t lr 0.001\t time 0:00:29.298139:\n",
      "Epoch: 48\tMean Loss : 4.091296\t lr 0.001\t time 0:00:30.389145:\n",
      "Epoch: 49\tMean Loss : 4.090032\t lr 0.001\t time 0:00:32.258662:\n",
      "Epoch: 50\tMean Loss : 4.081033\t lr 0.001\t time 0:00:30.967989:\n",
      "\n",
      "\n",
      "True sent :  අනෙක් එවැනි ම රටක් වනුයේ බොලීවියාවයි<eos>\n",
      "Pred sent :  <pad>තෙෙපපපපපෙෙෙෙෙෙෙෙේේේේේේේේපේපපපේෙේපපපප\n",
      "Loss : 4.224092483520508\n",
      "Epoch: 51\tMean Loss : 4.077569\t lr 0.001\t time 0:00:32.362061:\n",
      "Epoch: 52\tMean Loss : 4.073260\t lr 0.001\t time 0:00:32.024252:\n",
      "Epoch: 53\tMean Loss : 4.067319\t lr 0.001\t time 0:00:30.653319:\n",
      "Epoch: 54\tMean Loss : 4.061338\t lr 0.001\t time 0:00:31.178674:\n",
      "Epoch: 55\tMean Loss : 4.053978\t lr 0.001\t time 0:00:30.584084:\n",
      "Epoch: 56\tMean Loss : 4.053456\t lr 0.001\t time 0:00:31.710862:\n",
      "Epoch: 57\tMean Loss : 4.048444\t lr 0.001\t time 0:00:30.126328:\n",
      "Epoch: 58\tMean Loss : 4.040071\t lr 0.001\t time 0:00:32.069179:\n",
      "Epoch: 59\tMean Loss : 4.033702\t lr 0.001\t time 0:00:30.843662:\n",
      "Epoch: 60\tMean Loss : 4.028966\t lr 0.001\t time 0:00:31.733516:\n",
      "\n",
      "\n",
      "True sent :  නතාෂා නැන්දා අපිට ලණුවකුත් කොක්කකුත් දුන්නා<eos>\n",
      "Pred sent :  <pad><pad>ෙෙෙපපපපපෙෙෙෙෙෙෙපපේේපේපපපපපපපපපපපණ්පපෙණ්කේ<eos>\n",
      "Loss : 4.212106704711914\n",
      "Epoch: 61\tMean Loss : 4.025136\t lr 0.001\t time 0:00:32.224851:\n",
      "Epoch: 62\tMean Loss : 4.018209\t lr 0.001\t time 0:00:33.657239:\n",
      "Epoch: 63\tMean Loss : 4.012412\t lr 0.001\t time 0:00:32.000194:\n",
      "Epoch: 64\tMean Loss : 4.002770\t lr 0.001\t time 0:00:32.161389:\n",
      "Epoch: 65\tMean Loss : 3.995474\t lr 0.001\t time 0:00:31.645103:\n",
      "Epoch: 66\tMean Loss : 3.999051\t lr 0.001\t time 0:00:31.238617:\n",
      "Epoch: 67\tMean Loss : 3.985949\t lr 0.001\t time 0:00:30.604336:\n",
      "Epoch: 68\tMean Loss : 3.975879\t lr 0.001\t time 0:00:31.725429:\n",
      "Epoch: 69\tMean Loss : 3.970124\t lr 0.001\t time 0:00:31.577906:\n",
      "Epoch: 70\tMean Loss : 3.961714\t lr 0.001\t time 0:00:30.946422:\n",
      "\n",
      "\n",
      "True sent :  මේකට විසඳුමක් දෙන්න ලංකාවට භාරදෙන්නයි තිබුනෙ<eos>\n",
      "Pred sent :  <pad>ුෙෙෙපපපපෙෙෙෙෙෙෙපපපපපපපපපපපපපපපපපපපපපපපියුණ්ක\n",
      "Loss : 4.230713844299316\n",
      "Epoch: 71\tMean Loss : 3.961366\t lr 0.001\t time 0:00:30.955798:\n",
      "Epoch: 72\tMean Loss : 3.952953\t lr 0.001\t time 0:00:30.756026:\n",
      "Epoch: 73\tMean Loss : 3.944141\t lr 0.001\t time 0:00:30.883544:\n",
      "Epoch: 74\tMean Loss : 3.937766\t lr 0.001\t time 0:00:30.466165:\n",
      "Epoch: 75\tMean Loss : 3.989913\t lr 0.001\t time 0:00:32.163053:\n",
      "Epoch: 76\tMean Loss : 3.998644\t lr 0.001\t time 0:00:32.511742:\n",
      "Epoch: 77\tMean Loss : 3.959006\t lr 0.001\t time 0:00:29.690661:\n",
      "Epoch: 78\tMean Loss : 3.943572\t lr 0.001\t time 0:00:31.167196:\n",
      "Epoch: 79\tMean Loss : 3.929114\t lr 0.001\t time 0:00:32.049794:\n",
      "Epoch: 80\tMean Loss : 3.924418\t lr 0.001\t time 0:00:32.235021:\n",
      "\n",
      "\n",
      "True sent :  හැබැයි නිලධාරීවාදයෙ නං සෑහෙන බලපෑමක් තියනවා<eos>\n",
      "Pred sent :  කකකඩෙතයයයසසසසයයයයයයසසසසසසසසසසපපපපපප්පපපයප්ප<eos>\n",
      "Loss : 4.138116359710693\n",
      "Epoch: 81\tMean Loss : 3.913475\t lr 0.001\t time 0:00:32.020135:\n",
      "Epoch: 82\tMean Loss : 3.905737\t lr 0.001\t time 0:00:31.309222:\n",
      "Epoch: 83\tMean Loss : 3.896005\t lr 0.001\t time 0:00:31.564710:\n",
      "Epoch: 84\tMean Loss : 3.892242\t lr 0.001\t time 0:00:30.315477:\n",
      "Epoch: 85\tMean Loss : 3.884042\t lr 0.001\t time 0:00:32.715570:\n",
      "Epoch: 86\tMean Loss : 3.880319\t lr 0.001\t time 0:00:30.208562:\n",
      "Epoch: 87\tMean Loss : 3.874227\t lr 0.001\t time 0:00:29.708624:\n",
      "Epoch: 88\tMean Loss : 3.862851\t lr 0.001\t time 0:00:31.681785:\n",
      "Epoch: 89\tMean Loss : 3.859012\t lr 0.001\t time 0:00:32.098192:\n",
      "Epoch: 90\tMean Loss : 3.857586\t lr 0.001\t time 0:00:31.375977:\n",
      "\n",
      "\n",
      "True sent :  රේමයයි වෛරයයි අතරේ වෙනස කෙස් ගසකට සමයි<eos>\n",
      "Pred sent :  කකකඩෙතයයයසසසසසයයයයයසසසසසසසසසසසසසයයසයයයස\n",
      "Loss : 4.1652750968933105\n",
      "Epoch: 91\tMean Loss : 3.851586\t lr 0.001\t time 0:00:30.223399:\n",
      "Epoch: 92\tMean Loss : 3.845324\t lr 0.001\t time 0:00:31.609362:\n",
      "Epoch: 93\tMean Loss : 3.836252\t lr 0.001\t time 0:00:31.858511:\n",
      "Epoch: 94\tMean Loss : 3.832204\t lr 0.001\t time 0:00:30.364515:\n",
      "Epoch: 95\tMean Loss : 3.821854\t lr 0.001\t time 0:00:31.723907:\n",
      "Epoch: 96\tMean Loss : 3.822993\t lr 0.001\t time 0:00:31.143367:\n",
      "Epoch: 97\tMean Loss : 3.817055\t lr 0.001\t time 0:00:32.316561:\n",
      "Epoch: 98\tMean Loss : 3.809414\t lr 0.001\t time 0:00:31.303159:\n",
      "Epoch: 99\tMean Loss : 3.805731\t lr 0.001\t time 0:00:31.167602:\n",
      "Epoch: 100\tMean Loss : 3.803173\t lr 0.001\t time 0:00:31.155219:\n",
      "\n",
      "\n",
      "True sent :  මෙය දුටු ජම්බුක ආජිවකයාගේ මාන්නය දුරුවුණා<eos>\n",
      "Pred sent :  කකකඩතතයයයෙසයයයයයයයයයයසේෙේයසේයයයේේයෙයුයයයය<eos>\n",
      "Loss : 4.204971790313721\n",
      "Epoch: 101\tMean Loss : 3.802202\t lr 0.001\t time 0:00:31.844876:\n",
      "Epoch: 102\tMean Loss : 3.791065\t lr 0.001\t time 0:00:31.224841:\n",
      "Epoch: 103\tMean Loss : 3.793157\t lr 0.001\t time 0:00:31.341050:\n",
      "Epoch: 104\tMean Loss : 3.789899\t lr 0.001\t time 0:00:31.339046:\n",
      "Epoch: 105\tMean Loss : 3.779701\t lr 0.001\t time 0:00:30.739328:\n",
      "Epoch: 106\tMean Loss : 3.779781\t lr 0.001\t time 0:00:31.575958:\n",
      "Epoch: 107\tMean Loss : 3.770593\t lr 0.001\t time 0:00:31.710723:\n",
      "Epoch: 108\tMean Loss : 3.769051\t lr 0.001\t time 0:00:30.325553:\n",
      "Epoch: 109\tMean Loss : 3.761182\t lr 0.001\t time 0:00:32.207985:\n",
      "Epoch: 110\tMean Loss : 3.760878\t lr 0.001\t time 0:00:31.839484:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "True sent :  එතනින් පස්සෙ ජෝන්ගෙ ජීවිතේ වෙනස්ම පැත්තකට පෙරලෙනවා<eos>\n",
      "Pred sent :  ොකකතතතයපෙෙසසසසසසයයේේේෙසෙසේේසෙේේේේේපෙතපතේයයපදතද්ස්ත<eos>\n",
      "Loss : 4.095759391784668\n",
      "Epoch: 111\tMean Loss : 3.759388\t lr 0.001\t time 0:00:32.063506:\n",
      "Epoch: 112\tMean Loss : 3.757245\t lr 0.001\t time 0:00:31.770395:\n",
      "Epoch: 113\tMean Loss : 3.753969\t lr 0.001\t time 0:00:32.039945:\n",
      "Epoch: 114\tMean Loss : 3.746250\t lr 0.001\t time 0:00:31.744447:\n",
      "Epoch: 115\tMean Loss : 3.741589\t lr 0.001\t time 0:00:30.973200:\n",
      "Epoch: 116\tMean Loss : 3.748674\t lr 0.001\t time 0:00:29.892615:\n",
      "Epoch: 117\tMean Loss : 3.731640\t lr 0.001\t time 0:00:32.308737:\n",
      "Epoch: 118\tMean Loss : 3.740300\t lr 0.001\t time 0:00:30.725765:\n",
      "Epoch: 119\tMean Loss : 3.733225\t lr 0.001\t time 0:00:32.315861:\n",
      "Epoch: 120\tMean Loss : 3.732693\t lr 0.001\t time 0:00:31.060725:\n",
      "\n",
      "\n",
      "True sent :  මියගිය පුද්ගලයාට කරන සැබෑ ගෞරවයක් ද නොවේ<eos>\n",
      "Pred sent :  කකකටතතෙයෙෙසසසයෙසයයේේසසේසැෙසැසයයප්ඩදෙසැඩේ<eos>\n",
      "Loss : 4.11738395690918\n",
      "Epoch: 121\tMean Loss : 3.733336\t lr 0.001\t time 0:00:29.787718:\n",
      "Epoch: 122\tMean Loss : 3.719820\t lr 0.001\t time 0:00:32.155356:\n",
      "Epoch: 123\tMean Loss : 3.718913\t lr 0.001\t time 0:00:32.227062:\n",
      "Epoch: 124\tMean Loss : 3.718876\t lr 0.001\t time 0:00:31.279431:\n",
      "Epoch: 125\tMean Loss : 3.713166\t lr 0.001\t time 0:00:32.000304:\n",
      "Epoch: 126\tMean Loss : 3.713624\t lr 0.001\t time 0:00:31.413238:\n",
      "Epoch: 127\tMean Loss : 3.708306\t lr 0.001\t time 0:00:32.400288:\n",
      "Epoch: 128\tMean Loss : 3.705310\t lr 0.001\t time 0:00:28.842573:\n",
      "Epoch: 129\tMean Loss : 3.706781\t lr 0.001\t time 0:00:30.232664:\n",
      "Epoch: 130\tMean Loss : 3.698423\t lr 0.001\t time 0:00:30.589877:\n",
      "\n",
      "\n",
      "True sent :  මෙහි තිබෙන වඩාත්ම පෞරාණික වැදගත්කම වනුයේ දහයියා පිළිමයි<eos>\n",
      "Pred sent :  කකටඩතතතෙෙෙසසසසතසසේේේේේෙේෙෙේේේේෙේෙේෙසෙේේේේපෙසි<eos>ේ<eos>පිතිසසි<eos>\n",
      "Loss : 4.095605850219727\n",
      "Epoch: 131\tMean Loss : 3.704707\t lr 0.001\t time 0:00:31.465167:\n",
      "Epoch: 132\tMean Loss : 3.702886\t lr 0.001\t time 0:00:30.044390:\n",
      "Epoch: 133\tMean Loss : 3.692575\t lr 0.001\t time 0:00:31.239420:\n",
      "Epoch: 134\tMean Loss : 3.692140\t lr 0.001\t time 0:00:31.839471:\n",
      "Epoch: 135\tMean Loss : 3.689754\t lr 0.001\t time 0:00:30.851430:\n",
      "Epoch: 136\tMean Loss : 3.679236\t lr 0.001\t time 0:00:31.717599:\n",
      "Epoch: 137\tMean Loss : 3.684137\t lr 0.001\t time 0:00:31.091906:\n",
      "Epoch: 138\tMean Loss : 3.689957\t lr 0.001\t time 0:00:30.600745:\n",
      "Epoch: 139\tMean Loss : 3.682957\t lr 0.001\t time 0:00:30.447810:\n",
      "Epoch: 140\tMean Loss : 3.675828\t lr 0.001\t time 0:00:32.021601:\n",
      "\n",
      "\n",
      "True sent :  සිංහල වේවා දෙමල වේවා කවුරු උනත් මිනිස්සු<eos>\n",
      "Pred sent :  ොකකටතතයෙෙෙසසෙසසසයෙෙේසසෙසෙෙසසෙෙසසසෙසිස්සසද\n",
      "Loss : 4.195925712585449\n",
      "Epoch: 141\tMean Loss : 3.678336\t lr 0.001\t time 0:00:31.000497:\n",
      "Epoch: 142\tMean Loss : 3.677217\t lr 0.001\t time 0:00:32.854347:\n",
      "Epoch: 143\tMean Loss : 3.669583\t lr 0.001\t time 0:00:30.911544:\n",
      "Epoch: 144\tMean Loss : 3.679242\t lr 0.001\t time 0:00:30.252545:\n",
      "Epoch: 145\tMean Loss : 3.681382\t lr 0.001\t time 0:00:31.044371:\n",
      "Epoch: 146\tMean Loss : 3.671019\t lr 0.001\t time 0:00:31.076843:\n",
      "Epoch: 147\tMean Loss : 3.663927\t lr 0.001\t time 0:00:31.550884:\n",
      "Epoch: 148\tMean Loss : 3.670476\t lr 0.001\t time 0:00:30.810695:\n",
      "Epoch: 149\tMean Loss : 3.668728\t lr 0.001\t time 0:00:30.888841:\n",
      "Epoch: 150\tMean Loss : 3.658808\t lr 0.001\t time 0:00:31.652462:\n",
      "\n",
      "\n",
      "True sent :  යන්න කලින් කමෙන්ට් එකක් එහෙම දාලා යන්නත් අමතක කරන්න එපා<eos>\n",
      "Pred sent :  ොකටඩෙතයෙෙෙසසසෙෙසෙයෙෙේෙෙෙෙෙෙෙේදෙෙෙෙෙේ්ඩේ්ඩයයය්්දිු්දේහහත<eos>\n",
      "Loss : 4.155070781707764\n",
      "Epoch: 151\tMean Loss : 3.665835\t lr 0.001\t time 0:00:31.337396:\n",
      "Epoch: 152\tMean Loss : 3.657989\t lr 0.001\t time 0:00:31.524541:\n",
      "Epoch: 153\tMean Loss : 3.659292\t lr 0.001\t time 0:00:32.059402:\n",
      "Epoch: 154\tMean Loss : 3.651122\t lr 0.001\t time 0:00:30.969832:\n",
      "Epoch: 155\tMean Loss : 3.651524\t lr 0.001\t time 0:00:30.502267:\n",
      "Epoch: 156\tMean Loss : 3.657635\t lr 0.001\t time 0:00:30.498300:\n",
      "Epoch: 157\tMean Loss : 3.641384\t lr 0.001\t time 0:00:30.924872:\n",
      "Epoch: 158\tMean Loss : 3.650780\t lr 0.001\t time 0:00:31.701894:\n",
      "Epoch: 159\tMean Loss : 3.644220\t lr 0.001\t time 0:00:31.265629:\n",
      "Epoch: 160\tMean Loss : 3.647685\t lr 0.001\t time 0:00:29.748122:\n",
      "\n",
      "\n",
      "True sent :  උන් එක පාරටම නිවන් නොදකින බවනම් මට විශ්වාසයි<eos>\n",
      "Pred sent :  ොකටඩතතෙෙෙෙසසයෙෙෙෙෙෙෙෙෙෙෙෙෙසෙෙෙඇඇඇටඩසිඩ්ඩසසයි<eos>\n",
      "Loss : 4.118475914001465\n",
      "Epoch: 161\tMean Loss : 3.640052\t lr 0.001\t time 0:00:29.810777:\n",
      "Epoch: 162\tMean Loss : 3.645490\t lr 0.001\t time 0:00:30.993513:\n",
      "Epoch: 163\tMean Loss : 3.643104\t lr 0.001\t time 0:00:29.321651:\n",
      "Epoch: 164\tMean Loss : 3.639982\t lr 0.001\t time 0:00:30.742442:\n",
      "Epoch: 165\tMean Loss : 3.640568\t lr 0.001\t time 0:00:30.930747:\n",
      "Epoch: 166\tMean Loss : 3.642123\t lr 0.001\t time 0:00:29.529822:\n",
      "Epoch: 167\tMean Loss : 3.634883\t lr 0.001\t time 0:00:31.925313:\n",
      "Epoch: 168\tMean Loss : 3.646745\t lr 0.001\t time 0:00:29.472465:\n",
      "Epoch: 169\tMean Loss : 3.634049\t lr 0.001\t time 0:00:31.195968:\n",
      "Epoch: 170\tMean Loss : 3.638554\t lr 0.001\t time 0:00:31.544712:\n",
      "\n",
      "\n",
      "True sent :  මොරගොල්ලේ සිරි ඤාණෝභාස තිස්ස නාහිමිගේ අර්ථ විවරණය<eos>\n",
      "Pred sent :  ොොකඩතතෙළෙෙසසසෙෙෙෙෙෙෙෙසෙෙෙසෙෙේේැේෙෙෙළෙළළේථෙදිසස්ය<eos>\n",
      "Loss : 4.090618133544922\n",
      "Epoch: 171\tMean Loss : 3.638277\t lr 0.001\t time 0:00:31.317551:\n",
      "Epoch: 172\tMean Loss : 3.630732\t lr 0.001\t time 0:00:30.085627:\n",
      "Epoch: 173\tMean Loss : 3.627416\t lr 0.001\t time 0:00:30.613244:\n",
      "Epoch: 174\tMean Loss : 3.637928\t lr 0.001\t time 0:00:30.137392:\n",
      "Epoch: 175\tMean Loss : 3.632324\t lr 0.001\t time 0:00:31.398918:\n",
      "Epoch: 176\tMean Loss : 3.629826\t lr 0.001\t time 0:00:31.523513:\n",
      "Epoch: 177\tMean Loss : 3.631581\t lr 0.001\t time 0:00:30.781718:\n",
      "Epoch: 178\tMean Loss : 3.629866\t lr 0.001\t time 0:00:29.254279:\n",
      "Epoch: 179\tMean Loss : 3.623971\t lr 0.001\t time 0:00:30.365617:\n",
      "Epoch: 180\tMean Loss : 3.625840\t lr 0.001\t time 0:00:30.350518:\n",
      "\n",
      "\n",
      "True sent :  මීට අමතරව රුධිරයෙහි ද මෙම රසායනිකය අන්තර්ගත වී තිබේ<eos>\n",
      "Pred sent :  ොොටඩඩතතළෙෙෙසසෙෙෙෙෙෙෙදෙෙෙෙැෙෙෙයෙෙෙෙෙහෙෙතේෙේසීමසිසේ<eos>\n",
      "Loss : 4.035752296447754\n",
      "Epoch: 181\tMean Loss : 3.631090\t lr 0.001\t time 0:00:30.574293:\n",
      "Epoch: 182\tMean Loss : 3.624415\t lr 0.001\t time 0:00:29.279278:\n",
      "Epoch: 183\tMean Loss : 3.630163\t lr 0.001\t time 0:00:30.517237:\n",
      "Epoch: 184\tMean Loss : 3.615733\t lr 0.001\t time 0:00:29.676295:\n",
      "Epoch: 185\tMean Loss : 3.622441\t lr 0.001\t time 0:00:30.784059:\n",
      "Epoch: 186\tMean Loss : 3.623123\t lr 0.001\t time 0:00:31.570844:\n",
      "Epoch: 187\tMean Loss : 3.624009\t lr 0.001\t time 0:00:30.525896:\n",
      "Epoch: 188\tMean Loss : 3.612313\t lr 0.001\t time 0:00:31.057130:\n",
      "Epoch: 189\tMean Loss : 3.620311\t lr 0.001\t time 0:00:30.673385:\n",
      "Epoch: 190\tMean Loss : 3.618568\t lr 0.001\t time 0:00:30.865204:\n",
      "\n",
      "\n",
      "True sent :  හැබැයි කොහොම හරි සන්නිවේදනය තේරුම් ගන්න පුළුවන් උනා<eos>\n",
      "Pred sent :  ොකකඩඩතළළළෙසසසසෙසසසසසසේෙසෙෙයසසේෙසෙෙෙෙේසෙපුළුසස්<eos>සඩ්<eos>\n",
      "Loss : 4.080318450927734\n",
      "Epoch: 191\tMean Loss : 3.614131\t lr 0.001\t time 0:00:31.232184:\n",
      "Epoch: 192\tMean Loss : 3.619816\t lr 0.001\t time 0:00:30.981082:\n",
      "Epoch: 193\tMean Loss : 3.614395\t lr 0.001\t time 0:00:30.587759:\n",
      "Epoch: 194\tMean Loss : 3.620761\t lr 0.001\t time 0:00:31.693984:\n",
      "Epoch: 195\tMean Loss : 3.615345\t lr 0.001\t time 0:00:30.638541:\n",
      "Epoch: 196\tMean Loss : 3.611159\t lr 0.001\t time 0:00:31.104439:\n",
      "Epoch: 197\tMean Loss : 3.625313\t lr 0.001\t time 0:00:30.522211:\n",
      "Epoch: 198\tMean Loss : 3.600861\t lr 0.001\t time 0:00:29.967464:\n",
      "Epoch: 199\tMean Loss : 3.611801\t lr 0.001\t time 0:00:30.846996:\n",
      "Epoch: 200\tMean Loss : 3.613719\t lr 0.001\t time 0:00:30.110457:\n",
      "\n",
      "\n",
      "True sent :  හසන්තත් එහෙනම් නියම එකකින් වැඩ අල්ලලා තියෙන්නේ<eos>\n",
      "Pred sent :  ොකටඩඩෙළළෙළෙෙෙෙෙෙෙයෙෙෙෙෙෙෙෙෙඇෙඇෙෙේපෙෙෙදියෙේදේ<eos>\n",
      "Loss : 4.070553779602051\n",
      "Epoch: 201\tMean Loss : 3.606609\t lr 0.001\t time 0:00:31.988254:\n",
      "Epoch: 202\tMean Loss : 3.607035\t lr 0.001\t time 0:00:30.623114:\n",
      "Epoch: 203\tMean Loss : 3.607076\t lr 0.001\t time 0:00:30.719001:\n",
      "Epoch: 204\tMean Loss : 3.606731\t lr 0.001\t time 0:00:31.508083:\n",
      "Epoch: 205\tMean Loss : 3.611297\t lr 0.001\t time 0:00:30.210685:\n",
      "Epoch: 206\tMean Loss : 3.608642\t lr 0.001\t time 0:00:29.929611:\n",
      "Epoch: 207\tMean Loss : 3.608712\t lr 0.001\t time 0:00:30.540078:\n",
      "Epoch: 208\tMean Loss : 3.601953\t lr 0.001\t time 0:00:30.969627:\n",
      "Epoch: 209\tMean Loss : 3.601358\t lr 0.001\t time 0:00:30.483628:\n",
      "Epoch: 210\tMean Loss : 3.599696\t lr 0.001\t time 0:00:32.586790:\n",
      "\n",
      "\n",
      "True sent :  නැගෙනහිර දෙසට ගලා බසින නයිජර් ගංගාව ආරම්භ වන්නේ ගිනියාවෙනි<eos>\n",
      "Pred sent :  කකටඩඩතළළළදෙසෙෙෙෙෙසසේෙෙෙෙෙෙෙේෙෙෙෙමෙෙසදම්සදපී්සේ<eos>ඇියිය<eos><eos>ෙල්<eos>\n",
      "Loss : 4.057292938232422\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 211\tMean Loss : 3.611342\t lr 0.001\t time 0:00:31.280132:\n",
      "Epoch: 212\tMean Loss : 3.602684\t lr 0.001\t time 0:00:33.048117:\n",
      "Epoch: 213\tMean Loss : 3.610093\t lr 0.001\t time 0:00:31.439191:\n",
      "Epoch: 214\tMean Loss : 3.602588\t lr 0.001\t time 0:00:31.749717:\n",
      "Epoch: 215\tMean Loss : 3.603895\t lr 0.001\t time 0:00:31.214540:\n",
      "Epoch: 216\tMean Loss : 3.596107\t lr 0.001\t time 0:00:31.180904:\n",
      "Epoch: 217\tMean Loss : 3.600955\t lr 0.001\t time 0:00:31.755955:\n",
      "Epoch: 218\tMean Loss : 3.598211\t lr 0.001\t time 0:00:31.171057:\n",
      "Epoch: 219\tMean Loss : 3.594671\t lr 0.001\t time 0:00:31.552420:\n",
      "Epoch: 220\tMean Loss : 3.600740\t lr 0.001\t time 0:00:32.818136:\n",
      "\n",
      "\n",
      "True sent :  නත්තල් කාලෙදී අමුතු අතුරුදහන්වීම් කිහිපයක් සිද්ධ වෙනවා<eos>\n",
      "Pred sent :  කකකතඩඑඑළළධෙසෙෙමළුතඑළළෙෙෙෙෙෙේඇීම්ළඇළසිපයස්සසිදුධහළෙල්ඇ<eos>\n",
      "Loss : 3.964757204055786\n",
      "Epoch: 221\tMean Loss : 3.595829\t lr 0.001\t time 0:00:31.767928:\n",
      "Epoch: 222\tMean Loss : 3.610573\t lr 0.001\t time 0:00:30.939254:\n",
      "Epoch: 223\tMean Loss : 3.603961\t lr 0.001\t time 0:00:31.073449:\n",
      "Epoch: 224\tMean Loss : 3.605200\t lr 0.001\t time 0:00:31.753994:\n",
      "Epoch: 225\tMean Loss : 3.593613\t lr 0.001\t time 0:00:31.696117:\n",
      "Epoch: 226\tMean Loss : 3.594764\t lr 0.001\t time 0:00:31.621162:\n",
      "Epoch: 227\tMean Loss : 3.589177\t lr 0.001\t time 0:00:32.375500:\n",
      "Epoch: 228\tMean Loss : 3.589919\t lr 0.001\t time 0:00:33.380036:\n",
      "Epoch: 229\tMean Loss : 3.598585\t lr 0.001\t time 0:00:32.049764:\n",
      "Epoch: 230\tMean Loss : 3.592632\t lr 0.001\t time 0:00:32.134535:\n",
      "\n",
      "\n",
      "True sent :  මෙහිදී රජුගේ කාර්යභාරය සලකා බැලීම වැදගත් ය<eos>\n",
      "Pred sent :  ොකකඩඩතඑළළධසසසසෙෙෙෙෙෙසසෙසසෙෙසසැසීමෙයැදෙත්ථස<eos>\n",
      "Loss : 4.07012939453125\n",
      "Epoch: 231\tMean Loss : 3.592825\t lr 0.001\t time 0:00:32.527184:\n",
      "Epoch: 232\tMean Loss : 3.592808\t lr 0.001\t time 0:00:31.633633:\n",
      "Epoch: 233\tMean Loss : 3.601539\t lr 0.001\t time 0:00:30.931459:\n",
      "Epoch: 234\tMean Loss : 3.589191\t lr 0.001\t time 0:00:31.845626:\n",
      "Epoch: 235\tMean Loss : 3.593231\t lr 0.001\t time 0:00:31.341362:\n",
      "Epoch: 236\tMean Loss : 3.587717\t lr 0.001\t time 0:00:32.109513:\n",
      "Epoch: 237\tMean Loss : 3.590800\t lr 0.001\t time 0:00:30.651564:\n",
      "Epoch: 238\tMean Loss : 3.585029\t lr 0.001\t time 0:00:32.314901:\n",
      "Epoch: 239\tMean Loss : 3.590574\t lr 0.001\t time 0:00:30.618950:\n",
      "Epoch: 240\tMean Loss : 3.586732\t lr 0.001\t time 0:00:31.833360:\n",
      "\n",
      "\n",
      "True sent :  මේ පවුලෙ සාමාඡිකයින් එකිනෙකා අභිරහස් විදියට මරා දමනව<eos>\n",
      "Pred sent :  කකකඩඩතළළළධධසසෙෙෙයපප්සසසෙෙෙෙටසසපිපධස්සසෙයියටයපෙසසදමස්ඇ\n",
      "Loss : 4.055160045623779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-87437c2a60d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# train_loader = get_loader(epoch)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# Decrease tf_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-77458e544918>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, device, train_loader, optimizer, epoch, print_interval, writer, log_interval, scheduler)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/models/las_model/seq2seq.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, data, target)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# forward propagte through encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mencoder_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# initialising loss for batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/models/las_model/listener.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/shastra_ASR/Code/Speech_To_Text/models/las_model/listener.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    562\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_packed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_tensor\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0munsorted_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munsorted_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward_impl\u001b[0;34m(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n\u001b[0;32m--> 526\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# optimizer = optim.SGD(model.parameters(), lr=0.001)  # lr = 0.2 used in paper\n",
    "# optimizer = optim.Adadelta(model.parameters())\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), amsgrad=True)\n",
    "#scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "log_interval = 5\n",
    "print_interval = 50\n",
    "\n",
    "epochs = 400\n",
    "load = False\n",
    "\n",
    "train_dataset = SpeechDataset(train_df, data_dir, char_to_token, n_fft=2048, hop_length=512)\n",
    "train_loader = train_loader = AudioDataLoader(pad_token, train_dataset, \n",
    "                                              batch_size=64, num_workers=8, \n",
    "                                              drop_last=True, shuffle=True)\n",
    "\n",
    "writer = SummaryWriter(save_dir)\n",
    "print('save_dir', save_dir)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs):\n",
    "    # train_loader = get_loader(epoch)\n",
    "    \n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    \n",
    "    # Decrease tf_ratio\n",
    "#     if epoch % 20 == 0:\n",
    "#         model.tf_ratio = model.tf_ratio - 0.05\n",
    "#         validate_personal(model, 1, train_dataset)\n",
    "#         print(\"tf_ratio\", model.tf_ratio)\n",
    "\n",
    "    if epoch % 10 == 0 and epoch>=30:\n",
    "        validate_personal(model, 1, train_dataset)\n",
    "    \n",
    "    #scheduler.step()  # update scheduler\n",
    "    \n",
    "    # save model\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}')) #save the model\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_dir, f'optim_{epoch}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 241\tMean Loss : 3.589837\t lr 0.0001\t time 0:00:30.399288:\n",
      "Epoch: 242\tMean Loss : 3.587140\t lr 0.0001\t time 0:00:31.358197:\n",
      "Epoch: 243\tMean Loss : 3.574845\t lr 0.0001\t time 0:00:31.290870:\n",
      "Epoch: 244\tMean Loss : 3.593953\t lr 0.0001\t time 0:00:29.897299:\n",
      "Epoch: 245\tMean Loss : 3.586245\t lr 0.0001\t time 0:00:30.142773:\n",
      "Epoch: 246\tMean Loss : 3.595864\t lr 0.0001\t time 0:00:29.982665:\n",
      "Epoch: 247\tMean Loss : 3.588057\t lr 0.0001\t time 0:00:31.735062:\n",
      "Epoch: 248\tMean Loss : 3.592814\t lr 0.0001\t time 0:00:30.335721:\n",
      "Epoch: 249\tMean Loss : 3.591168\t lr 0.0001\t time 0:00:31.002588:\n",
      "Epoch: 250\tMean Loss : 3.592557\t lr 0.0001\t time 0:00:30.662296:\n",
      "\n",
      "\n",
      "True sent :  අප යුද්ධයේ පළමු පියවරේදීම පරාද වී අවසානය<eos>\n",
      "Pred sent :  ොකකඩඩතළළළළධපෙමෙළෙෙයෙෙෙෙීෙෙපෙපසෙසීපසපසථථයද\n",
      "Loss : 4.078856468200684\n",
      "Epoch: 251\tMean Loss : 3.584551\t lr 0.0001\t time 0:00:32.219878:\n",
      "Epoch: 252\tMean Loss : 3.590463\t lr 0.0001\t time 0:00:31.509418:\n",
      "Epoch: 253\tMean Loss : 3.579198\t lr 0.0001\t time 0:00:30.926085:\n",
      "Epoch: 254\tMean Loss : 3.582514\t lr 0.0001\t time 0:00:31.668318:\n",
      "Epoch: 255\tMean Loss : 3.587679\t lr 0.0001\t time 0:00:30.796459:\n",
      "Epoch: 256\tMean Loss : 3.579301\t lr 0.0001\t time 0:00:30.978455:\n",
      "Epoch: 257\tMean Loss : 3.593071\t lr 0.0001\t time 0:00:31.193066:\n",
      "Epoch: 258\tMean Loss : 3.591383\t lr 0.0001\t time 0:00:31.428806:\n",
      "Epoch: 259\tMean Loss : 3.590146\t lr 0.0001\t time 0:00:31.516564:\n",
      "Epoch: 260\tMean Loss : 3.581961\t lr 0.0001\t time 0:00:30.735782:\n",
      "\n",
      "\n",
      "True sent :  මෙතැන මරණ දැන්වීම් දාන එක ගැන නෙවෙයි කතාව<eos>\n",
      "Pred sent :  ොකකඩඩඑළළළළසෙසෙෙීළළළළළළෙඑළෙෙැෙෙෙෙපෙයිපථථපපප\n",
      "Loss : 4.101139545440674\n",
      "Epoch: 261\tMean Loss : 3.591902\t lr 0.0001\t time 0:00:32.266022:\n",
      "Epoch: 262\tMean Loss : 3.578814\t lr 0.0001\t time 0:00:31.560800:\n",
      "Epoch: 263\tMean Loss : 3.585891\t lr 0.0001\t time 0:00:32.113540:\n",
      "Epoch: 264\tMean Loss : 3.586056\t lr 0.0001\t time 0:00:31.419053:\n",
      "Epoch: 265\tMean Loss : 3.587822\t lr 0.0001\t time 0:00:32.230665:\n",
      "Epoch: 266\tMean Loss : 3.592501\t lr 0.0001\t time 0:00:30.435936:\n",
      "Epoch: 267\tMean Loss : 3.582228\t lr 0.0001\t time 0:00:33.034646:\n",
      "Epoch: 268\tMean Loss : 3.581533\t lr 0.0001\t time 0:00:31.700498:\n",
      "Epoch: 269\tMean Loss : 3.583228\t lr 0.0001\t time 0:00:31.972992:\n",
      "Epoch: 270\tMean Loss : 3.595836\t lr 0.0001\t time 0:00:31.847373:\n",
      "\n",
      "\n",
      "True sent :  මෝඩිගේ මාපියෝ බටහිර ගුජරාටයේ අඩු කුලයකට අයත් වූවෝය<eos>\n",
      "Pred sent :  ොකකඩඩතළළළළසයෙසසසළසටසළසසසෙෙයේෙඑඩුපඇුලයේ්ථපයි්සපූහේය<eos>\n",
      "Loss : 4.02892541885376\n",
      "Epoch: 271\tMean Loss : 3.588190\t lr 0.0001\t time 0:00:31.371298:\n",
      "Epoch: 272\tMean Loss : 3.586737\t lr 0.0001\t time 0:00:31.030654:\n",
      "Epoch: 273\tMean Loss : 3.583580\t lr 0.0001\t time 0:00:32.179796:\n",
      "Epoch: 274\tMean Loss : 3.594787\t lr 0.0001\t time 0:00:30.280572:\n",
      "Epoch: 275\tMean Loss : 3.588423\t lr 0.0001\t time 0:00:32.960104:\n",
      "Epoch: 276\tMean Loss : 3.598738\t lr 0.0001\t time 0:00:33.073692:\n",
      "Epoch: 277\tMean Loss : 3.583901\t lr 0.0001\t time 0:00:30.719235:\n",
      "Epoch: 278\tMean Loss : 3.588600\t lr 0.0001\t time 0:00:32.920548:\n",
      "Epoch: 279\tMean Loss : 3.583862\t lr 0.0001\t time 0:00:33.675259:\n",
      "Epoch: 280\tMean Loss : 3.590586\t lr 0.0001\t time 0:00:29.276434:\n",
      "\n",
      "\n",
      "True sent :  වාසල සොහොයුරා මාව ගොඩක් උනන්දු කරා මේ වැඩේට<eos>\n",
      "Pred sent :  ොකකඩඩතළළළළසසෙඑසසඑෙඑසසෙෙසෙසේසුසසටපපමේථසැඩිට<eos>\n",
      "Loss : 4.089702606201172\n",
      "Epoch: 281\tMean Loss : 3.595395\t lr 0.0001\t time 0:00:31.410365:\n",
      "Epoch: 282\tMean Loss : 3.590244\t lr 0.0001\t time 0:00:31.651861:\n",
      "Epoch: 283\tMean Loss : 3.580102\t lr 0.0001\t time 0:00:32.262711:\n"
     ]
    }
   ],
   "source": [
    "### continuing with reduced lr\n",
    "for param_group in optimizer.param_groups:\n",
    "    param_group['lr'] = param_group['lr']*0.1\n",
    "\n",
    "for epoch in range(241, epochs):\n",
    "    # train_loader = get_loader(epoch)\n",
    "    \n",
    "    train(model, DEVICE, train_loader, optimizer, epoch, print_interval, writer, log_interval)\n",
    "    \n",
    "    # Decrease tf_ratio\n",
    "#     if epoch % 20 == 0:\n",
    "#         model.tf_ratio = model.tf_ratio - 0.05\n",
    "#         validate_personal(model, 1, train_dataset)\n",
    "#         print(\"tf_ratio\", model.tf_ratio)\n",
    "\n",
    "    if epoch % 10 == 0 and epoch>=30:\n",
    "        validate_personal(model, 1, train_dataset)\n",
    "    \n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f'las_model_{epoch}')) #save the model\n",
    "    torch.save(optimizer.state_dict(), os.path.join(save_dir, f'optim_{epoch}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_personal(model, 10, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying with Torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Knowing the frequency of words\n",
    "\n",
    "def process(s):\n",
    "    return list(s)\n",
    "\n",
    "si_field = Field(\n",
    "    tokenizer_language='si',\n",
    "    lower=True, \n",
    "    init_token='<sos>', \n",
    "    eos_token='<eos>',\n",
    "    batch_first=True,\n",
    "    preprocessing=process\n",
    ")\n",
    "\n",
    "dataset = TabularDataset(\n",
    "    path=os.path.join(data_dir, 'temp.csv'),\n",
    "    format='CSV',\n",
    "    fields=[('index', None),('unnamed', None), ('sent', si_field)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "si_field.build_vocab(dataset, min_freq=2)\n",
    "print(len(si_field.vocab.stoi))"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
